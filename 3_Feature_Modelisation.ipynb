{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import des modules\n",
   "id": "e99a8f1c4f4bd3b6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-30T09:49:44.823544Z",
     "start_time": "2025-10-30T09:49:44.817060Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer, OrdinalEncoder,MinMaxScaler\n",
    "\n",
    "#Selection\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, KFold, StratifiedKFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import shap\n",
    "\n",
    "#Mod√®les\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "#Metriques\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, precision_recall_curve, average_precision_score, roc_curve, auc"
   ],
   "outputs": [],
   "execution_count": 334
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:49:44.840869Z",
     "start_time": "2025-10-30T09:49:44.828557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fc = pd.read_csv('fc_after_feature_engineering.csv')\n",
    "print(fc.info())"
   ],
   "id": "ae9d3797053beb5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1470 entries, 0 to 1469\n",
      "Data columns (total 48 columns):\n",
      " #   Column                                     Non-Null Count  Dtype  \n",
      "---  ------                                     --------------  -----  \n",
      " 0   a_quitte_l_entreprise                      1470 non-null   int64  \n",
      " 1   age                                        1470 non-null   int64  \n",
      " 2   annees_dans_l_entreprise                   1470 non-null   int64  \n",
      " 3   annees_dans_le_poste_actuel                1470 non-null   int64  \n",
      " 4   annees_depuis_la_derniere_promotion        1470 non-null   int64  \n",
      " 5   annees_experience_totale                   1470 non-null   int64  \n",
      " 6   annes_sous_responsable_actuel              1470 non-null   int64  \n",
      " 7   augmentation_salaire_precedente            1470 non-null   int64  \n",
      " 8   departement_Commercial                     1470 non-null   int64  \n",
      " 9   departement_Consulting                     1470 non-null   int64  \n",
      " 10  departement_Ressources Humaines            1470 non-null   int64  \n",
      " 11  distance_domicile_travail                  1470 non-null   int64  \n",
      " 12  domaine_etude_Autre                        1470 non-null   int64  \n",
      " 13  domaine_etude_Entrepreunariat              1470 non-null   int64  \n",
      " 14  domaine_etude_Infra & Cloud                1470 non-null   int64  \n",
      " 15  domaine_etude_Marketing                    1470 non-null   int64  \n",
      " 16  domaine_etude_Ressources Humaines          1470 non-null   int64  \n",
      " 17  domaine_etude_Transformation Digitale      1470 non-null   int64  \n",
      " 18  frequence_deplacement                      1470 non-null   float64\n",
      " 19  genre                                      1470 non-null   int64  \n",
      " 20  heure_supplementaires                      1470 non-null   int64  \n",
      " 21  id_employee                                1470 non-null   int64  \n",
      " 22  nb_formations_suivies                      1470 non-null   int64  \n",
      " 23  niveau_education                           1470 non-null   int64  \n",
      " 24  niveau_hierarchique_poste                  1470 non-null   int64  \n",
      " 25  nombre_experiences_precedentes             1470 non-null   int64  \n",
      " 26  nombre_participation_pee                   1470 non-null   int64  \n",
      " 27  note_evaluation_precedente                 1470 non-null   int64  \n",
      " 28  poste_Assistant de Direction               1470 non-null   int64  \n",
      " 29  poste_Cadre Commercial                     1470 non-null   int64  \n",
      " 30  poste_Consultant                           1470 non-null   int64  \n",
      " 31  poste_Directeur Technique                  1470 non-null   int64  \n",
      " 32  poste_Manager                              1470 non-null   int64  \n",
      " 33  poste_Representant Commercial              1470 non-null   int64  \n",
      " 34  poste_Ressources Humaines                  1470 non-null   int64  \n",
      " 35  poste_Senior Manager                       1470 non-null   int64  \n",
      " 36  poste_Tech Lead                            1470 non-null   int64  \n",
      " 37  ratio_experience_interne                   1470 non-null   float64\n",
      " 38  revenu_mensuel                             1470 non-null   int64  \n",
      " 39  satisfaction_employee_environnement        1470 non-null   int64  \n",
      " 40  satisfaction_employee_equilibre_pro_perso  1470 non-null   int64  \n",
      " 41  satisfaction_employee_equipe               1470 non-null   int64  \n",
      " 42  satisfaction_employee_nature_travail       1470 non-null   int64  \n",
      " 43  satisfaction_globale                       1470 non-null   float64\n",
      " 44  score_salaire                              1470 non-null   float64\n",
      " 45  statut_marital_Celibataire                 1470 non-null   int64  \n",
      " 46  statut_marital_Divorce(e)                  1470 non-null   int64  \n",
      " 47  statut_marital_Marie(e)                    1470 non-null   int64  \n",
      "dtypes: float64(4), int64(44)\n",
      "memory usage: 551.4 KB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 335
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# D√©claration des Dataframes X et y\n",
    "- Un DataFrame contenant les features => X\n",
    "- Un Pandas Series contenant la colonne cible => y"
   ],
   "id": "492e0eedd144db3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:49:44.847217Z",
     "start_time": "2025-10-30T09:49:44.844280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Un homme , C√©libataire , Entre 30 et 40 ans , Consultant , Entre 1 et 7 ann√©es dans l‚Äôentreprise, Un revenu compris entre 2500‚Ç¨ et 6000‚Ç¨, Domaine d‚Äôetude Infra & Cloud, Qui a moins de 5 ann√©es sous son responsable actuel, Qui a une distance domicile travail entre 3 et 17km\n",
    "\n",
    "# Test 1\n",
    "#columns_base = ['genre', 'statut_marital', 'age', 'annees_dans_l_entreprise', 'revenu_mensuel', 'distance_domicile_travail', 'satisfaction_globale']\n",
    "# Test 2\n",
    "columns_base = ['genre', 'revenu_mensuel', 'age',  'annees_dans_l_entreprise', 'distance_domicile_travail', 'score_salaire', 'satisfaction_globale']\n",
    "\n",
    "columns_domaine_etude = [col for col in fc.columns if col.startswith('domaine_etude')]\n",
    "columns_poste = [col for col in fc.columns if col.startswith('poste')]\n",
    "columns_statut_marital = [col for col in fc.columns if col.startswith('statut_marital')]\n",
    "columns_satisfaction = [col for col in fc.columns if col.startswith('satisfaction')]\n",
    "\n",
    "all_columns = columns_base + columns_statut_marital + columns_domaine_etude + columns_poste + columns_satisfaction\n",
    "\n",
    "X = fc[all_columns]\n",
    "y = fc['a_quitte_l_entreprise']"
   ],
   "id": "9f209612188d56f5",
   "outputs": [],
   "execution_count": 336
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fonction de S√©paration Train Test\n",
    "- Des m√©triques d‚Äô√©valuation calcul√©es pour chaque mod√®le, sur le jeu d‚Äôapprentissage et le jeu de test."
   ],
   "id": "6adf016352b1d3b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:49:44.857825Z",
     "start_time": "2025-10-30T09:49:44.851213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_test_evaluation(model, X_train, X_test, y_train, y_test, model_name, n_shap_samples=5):\n",
    "    \"\"\"\n",
    "    Entra√Æne un mod√®le et retourne les m√©triques train/test avec d√©tection d'overfitting.\n",
    "\n",
    "    Args:\n",
    "        model: Le mod√®le √† entra√Æner\n",
    "        X_train, X_test: Features d'entra√Ænement et de test\n",
    "        y_train, y_test: Labels d'entra√Ænement et de test\n",
    "        model_name: Nom du mod√®le (string)\n",
    "        n_shap_samples: Nombre d'individus √† analyser avec SHAP (d√©faut: 5)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (results, report_test, cm, curves, df_importances, importances_metadata)\n",
    "    \"\"\"\n",
    "\n",
    "    # Poids\n",
    "    weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "    # Cr√©ation d'un pipeline avec standardisation puis mod√®le\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),  # Normalisation des features\n",
    "        ('model', model)               # Mod√®le de r√©gression\n",
    "    ])\n",
    "\n",
    "    # Entra√Ænement avec les poids pass√©s au mod√®le via le pr√©fixe 'model__'\n",
    "    pipeline.fit(X_train, y_train, model__sample_weight=weights)\n",
    "\n",
    "    # Pr√©dictions sur TRAIN\n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "    report_train = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "\n",
    "    # Pr√©dictions sur TEST\n",
    "    y_pred_test = pipeline.predict(X_test)\n",
    "    report_test = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "\n",
    "    # Matrice de confusion (test)\n",
    "    cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "    # Calcul des √©carts (train - test) pour d√©tecter l'overfitting\n",
    "    accuracy_gap = report_train['accuracy'] - report_test['accuracy']\n",
    "    f1_gap = report_train['macro avg']['f1-score'] - report_test['macro avg']['f1-score']\n",
    "\n",
    "    # Indicateur d'overfitting (seuil √† 12% d'√©cart)\n",
    "    overfitting_flag = 'OUI' if (accuracy_gap > 0.12 or f1_gap > 0.12) else 'NON'\n",
    "\n",
    "    # Extraction des m√©triques principales\n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'method': 'train_test',\n",
    "        # M√©triques TRAIN\n",
    "        'train_accuracy': round(report_train['accuracy'], 2),\n",
    "        'train_f1_macro': round(report_train['macro avg']['f1-score'], 2),\n",
    "        'train_precision_macro': round(report_train['macro avg']['precision'], 2),\n",
    "        'train_recall_macro': round(report_train['macro avg']['recall'], 2),\n",
    "        # M√©triques TEST\n",
    "        'test_accuracy': round(report_test['accuracy'], 2),\n",
    "        'test_f1_macro': round(report_test['macro avg']['f1-score'], 2),\n",
    "        'test_precision_macro': round(report_test['macro avg']['precision'], 2),\n",
    "        'test_recall_macro': round(report_test['macro avg']['recall'], 2),\n",
    "        # √âcarts et overfitting\n",
    "        'accuracy_gap': round(accuracy_gap, 2),\n",
    "        'f1_gap': round(f1_gap, 2),\n",
    "        'overfitting': overfitting_flag,\n",
    "        'confusion_matrix': str(cm.tolist()),\n",
    "    }\n",
    "\n",
    "    # R√©cup√©rer le mod√®le entra√Æn√© du pipeline\n",
    "    trained_model = pipeline.named_steps['model']\n",
    "\n",
    "    # Transformer X_test avec le scaler avant de l'utiliser avec le mod√®le\n",
    "    X_test_scaled = pipeline.named_steps['scaler'].transform(X_test)\n",
    "\n",
    "    # V√©rifier si le mod√®le supporte predict_proba\n",
    "    try:\n",
    "        y_pred_proba = trained_model.predict_proba(X_test_scaled)\n",
    "    except AttributeError:\n",
    "        y_pred_proba = None\n",
    "\n",
    "    # R√©cup√©ration des donn√©es pour les courbes ROC et PR\n",
    "    curves = {\n",
    "        'model': model_name,\n",
    "        'y_true': y_test.copy() if hasattr(y_test, 'copy') else list(y_test),\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'y_pred': y_pred_test\n",
    "    }\n",
    "\n",
    "    # Permutation importance sur les donn√©es scal√©es\n",
    "    pi = permutation_importance(trained_model, X_test_scaled, y_test, n_repeats=10, random_state=82)\n",
    "\n",
    "    # SHAP et feature importances (uniquement pour mod√®les tree-based)\n",
    "    tree_based_models = (\n",
    "        XGBClassifier\n",
    "    )\n",
    "\n",
    "    if isinstance(trained_model, tree_based_models):\n",
    "        explainer = shap.TreeExplainer(trained_model)\n",
    "        shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "        # R√©cup√©rer les noms de features\n",
    "        feature_names_in = trained_model.feature_names_in_ if hasattr(trained_model, 'feature_names_in_') else X_test.columns.tolist()\n",
    "        feature_importances = trained_model.feature_importances_\n",
    "    else:\n",
    "        shap_values = None\n",
    "        feature_names_in = X_test.columns.tolist()\n",
    "        feature_importances = None\n",
    "        print(f\"‚ö† Mod√®le ignor√© (non compatible avec TreeExplainer): {type(trained_model).__name__}\")\n",
    "\n",
    "    # Construction du DataFrame d'importances\n",
    "    df_importances = pd.DataFrame({\n",
    "        'model': model_name,\n",
    "        'feature': X_test.columns,\n",
    "        'importance_mean': pi.importances_mean,\n",
    "        'importance_std': pi.importances_std\n",
    "    })\n",
    "\n",
    "    # Ajouter les feature importances si disponibles\n",
    "    if feature_importances is not None:\n",
    "        df_importances['feature_importances'] = feature_importances\n",
    "\n",
    "    # Stocker les autres informations s√©par√©ment\n",
    "    importances_metadata = {\n",
    "        'model': model_name,\n",
    "        'X_test': X_test,  # Garder les donn√©es non-scal√©es pour r√©f√©rence\n",
    "        'X_test_scaled': X_test_scaled,  # Ajouter les donn√©es scal√©es\n",
    "        'feature_names_in': feature_names_in,\n",
    "        'shap_values': shap_values\n",
    "    }\n",
    "\n",
    "    return results, report_test, cm, curves, df_importances, importances_metadata"
   ],
   "id": "ab9ae595cb3d2815",
   "outputs": [],
   "execution_count": 337
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fonction de Validation Crois√©e\n",
    "- cross_validate"
   ],
   "id": "6f174397daf9030d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:49:44.863561Z",
     "start_time": "2025-10-30T09:49:44.860345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cross_validation_evaluation(model, X, y, model_name, cv=5):\n",
    "    \"\"\"\n",
    "    Effectue une validation crois√©e et retourne les m√©triques avec d√©tection d'overfitting.\n",
    "\n",
    "    Args:\n",
    "        model: Le mod√®le √† √©valuer\n",
    "        X: Features compl√®tes\n",
    "        y: Labels complets\n",
    "        model_name: Nom du mod√®le (string)\n",
    "        cv: Nombre de folds (d√©faut: 5)\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionnaire contenant les r√©sultats moyens avec scores train et test\n",
    "    \"\"\"\n",
    "\n",
    "    # Cr√©ation du pipeline avec scaling\n",
    "    pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', model)\n",
    "    ])\n",
    "\n",
    "    # D√©finition des m√©triques √† calculer\n",
    "    scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "\n",
    "    # Validation crois√©e avec return_train_score=True pour d√©tecter overfitting\n",
    "    cv_results = cross_validate(pipeline, X, y, cv=cv, scoring=scoring, return_train_score=True, error_score='raise')\n",
    "\n",
    "    # Calcul des √©carts moyens (train - test)\n",
    "    accuracy_gap_cv = cv_results['train_accuracy'].mean() - cv_results['test_accuracy'].mean()\n",
    "    f1_gap_cv = cv_results['train_f1_macro'].mean() - cv_results['test_f1_macro'].mean()\n",
    "\n",
    "    # Indicateur d'overfitting\n",
    "    overfitting_flag_cv = 'OUI' if (accuracy_gap_cv >= 0.12 or f1_gap_cv >= 0.12) else 'NON'\n",
    "\n",
    "    # Calcul des moyennes et √©carts-types\n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'method': 'cross_validation',\n",
    "        # M√©triques TRAIN\n",
    "        'train_accuracy': round(cv_results['train_accuracy'].mean(), 2),\n",
    "        'train_accuracy_std': round(cv_results['train_accuracy'].std(), 2),\n",
    "        'train_f1_macro': round(cv_results['train_f1_macro'].mean(), 2),\n",
    "        'train_f1_macro_std': round(cv_results['train_f1_macro'].std(), 2),\n",
    "        'train_precision_macro': round(cv_results['train_precision_macro'].mean(), 2),\n",
    "        'train_recall_macro': round(cv_results['train_recall_macro'].mean(), 2),\n",
    "        # M√©triques TEST\n",
    "        'test_accuracy': round(cv_results['test_accuracy'].mean(), 2),\n",
    "        'test_accuracy_std': round(cv_results['test_accuracy'].std(), 2),\n",
    "        'test_f1_macro': round(cv_results['test_f1_macro'].mean(), 2),\n",
    "        'test_f1_macro_std': round(cv_results['test_f1_macro'].std(), 2),\n",
    "        'test_precision_macro': round(cv_results['test_precision_macro'].mean(), 2),\n",
    "        'test_recall_macro': round(cv_results['test_recall_macro'].mean(), 2),\n",
    "        # √âcarts et overfitting\n",
    "        'accuracy_gap': round(accuracy_gap_cv, 2),\n",
    "        'f1_gap': round(f1_gap_cv, 2),\n",
    "        'overfitting': overfitting_flag_cv,\n",
    "        'confusion_matrix': 'N/A',\n",
    "    }\n",
    "\n",
    "    return results"
   ],
   "id": "b1443a7a9f5add6b",
   "outputs": [],
   "execution_count": 338
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fonction de Comparaison des Modeles\n",
    "\n",
    "- DummyClassifier : strategy='most_frequent'\n",
    "- RandomForestClassifier : n_estimators=100,max_depth=10\n",
    "- XGBClassifier : eval_metric='logloss'\n",
    "- CatBoostClassifier"
   ],
   "id": "18cf6638fe6c883b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:49:44.869540Z",
     "start_time": "2025-10-30T09:49:44.865433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compare_models(X, y, test_size=0.2, random_state=82, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Compare tous les mod√®les et g√©n√®re les fichiers CSV de r√©sultats.\n",
    "\n",
    "    Args:\n",
    "        X: Features (DataFrame ou array)\n",
    "        y: Labels (Series ou array)\n",
    "        test_size: Proportion du jeu de test (d√©faut: 0.2)\n",
    "        random_state: Seed pour la reproductibilit√© (d√©faut: 42)\n",
    "        cv_folds: Nombre de folds pour la validation crois√©e (d√©faut: 5)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (r√©sultats_df, rapports_d√©taill√©s)\n",
    "    \"\"\"\n",
    "    # Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "\n",
    "    # D√©finition des mod√®les\n",
    "    models = {\n",
    "        'DummyClassifier': DummyClassifier(\n",
    "            strategy='most_frequent',\n",
    "            random_state=random_state),\n",
    "\n",
    "        'RandomForestRegressor': RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            min_samples_split=2,\n",
    "            random_state=random_state),\n",
    "\n",
    "        'XGBClassifier': XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=3,\n",
    "            learning_rate=0.1,     # Plus petit = apprentissage plus lent\n",
    "            subsample=0.8,          # 80% des donn√©es par arbre\n",
    "            reg_lambda=2,         # R√©gularisation L2\n",
    "            eval_metric='logloss',\n",
    "            scale_pos_weight=5,\n",
    "            random_state=random_state)\n",
    "\n",
    "        #'CatBoostClassifier': CatBoostClassifier(\n",
    "        #    iterations=300,\n",
    "        #    depth=4,\n",
    "        #    learning_rate=0.05,\n",
    "        #    l2_leaf_reg=3,        # R√©gularisation\n",
    "        #    verbose=False,\n",
    "        #    random_state=random_state)\n",
    "    }\n",
    "\n",
    "    all_results = []\n",
    "    detailed_reports = []\n",
    "    all_curves = []\n",
    "    all_df_importances = []\n",
    "    all_importances_metadata = []\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"COMPARAISON DES MOD√àLES DE CLASSIFICATION\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n>>> √âvaluation de {model_name}...\")\n",
    "\n",
    "        # Train/Test\n",
    "        print(f\"  - Train/Test split...\")\n",
    "        tt_results, report, cm, curves, df_importances,  importances_metadata = train_test_evaluation(model, X_train, X_test, y_train, y_test, model_name)\n",
    "        all_results.append(tt_results)\n",
    "\n",
    "        # Sauvegarde du rapport d√©taill√©\n",
    "        detailed_reports.append({\n",
    "            'model': model_name,\n",
    "            'method': 'train_test',\n",
    "            'report': report,\n",
    "            'confusion_matrix': cm\n",
    "        })\n",
    "\n",
    "        # Sauvegarde des courbes\n",
    "        all_curves.append(curves)\n",
    "\n",
    "        # Sauvegarde des courbes\n",
    "        all_df_importances.append(df_importances)\n",
    "\n",
    "        # Sauvegarde des courbes\n",
    "        all_importances_metadata.append(importances_metadata)\n",
    "\n",
    "        # Cross-validation\n",
    "        print(f\"  - Validation crois√©e ({cv_folds} folds)...\")\n",
    "        cv_results = cross_validation_evaluation(model, X, y, model_name, cv=cv_folds)\n",
    "        all_results.append(cv_results)\n",
    "\n",
    "        print(f\"  ‚úì {model_name} termin√©\")\n",
    "\n",
    "    # Conversion en DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "\n",
    "    # R√©organisation des colonnes pour la lisibilit√©\n",
    "    cols_order = ['model', 'method',\n",
    "                  'train_accuracy', 'test_accuracy', 'accuracy_gap',\n",
    "                  'train_f1_macro', 'test_f1_macro', 'f1_gap',\n",
    "                  'overfitting',\n",
    "                  'train_precision_macro', 'test_precision_macro',\n",
    "                  'train_recall_macro', 'test_recall_macro']\n",
    "\n",
    "    # Ajout des colonnes std si elles existent\n",
    "    std_cols = [col for col in results_df.columns if '_std' in col]\n",
    "    cols_order.extend(std_cols)\n",
    "    cols_order.append('confusion_matrix')\n",
    "\n",
    "    # Colonnes pr√©sentes dans le DataFrame\n",
    "    cols_order = [col for col in cols_order if col in results_df.columns]\n",
    "    results_df = results_df[cols_order]\n",
    "\n",
    "    return results_df, detailed_reports, all_curves, all_df_importances, all_importances_metadata"
   ],
   "id": "42b712baf50c2528",
   "outputs": [],
   "execution_count": 339
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fonction de Sauvegarde des R√©sultats\n",
    "- classification_results_by_class.csv => Rapport de classification par classes\n",
    "- classification_results_confusion_matrices.csv => Matrices de confusion\n",
    "- classification_results_summary.csv => Tous les scores\n",
    "- classification_results_overfitting_analysis.csv => Analyse sp√©cifique de l'overfitting"
   ],
   "id": "731e5a3a58b0b674"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:49:44.874505Z",
     "start_time": "2025-10-30T09:49:44.871488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_results(results_df, detailed_reports, output_prefix, all_curves=None, all_df_importances=None, all_importances_metadata=None):\n",
    "    \"\"\"\n",
    "    Sauvegarde les r√©sultats dans des fichiers CSV.\n",
    "\n",
    "    Args:\n",
    "        results_df: DataFrame avec tous les r√©sultats\n",
    "        detailed_reports: Liste des rapports d√©taill√©s\n",
    "        output_prefix: Pr√©fixe pour les fichiers de sortie\n",
    "               models_data: Liste de dict avec {\n",
    "            'model': nom du mod√®le,\n",
    "            'y_true': vraies √©tiquettes,\n",
    "            'y_pred_proba': probabilit√©s pr√©dites (si disponible),\n",
    "            'y_pred': pr√©dictions\n",
    "        }\n",
    "    \"\"\"\n",
    "    # 1. Fichier principal avec toutes les m√©triques\n",
    "    results_df.to_csv(f'exports/{output_prefix}_summary.csv', index=False)\n",
    "    print(f\"\\n‚úì R√©sum√© sauvegard√©: {output_prefix}_summary.csv\")\n",
    "\n",
    "    # 2. Fichier avec les rapports d√©taill√©s de classification\n",
    "    detailed_data = []\n",
    "    for item in detailed_reports:\n",
    "        model = item['model']\n",
    "        report = item['report']\n",
    "\n",
    "        # Extraction des m√©triques par classe\n",
    "        for class_label, metrics in report.items():\n",
    "            if class_label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "                detailed_data.append({\n",
    "                    'model': model,\n",
    "                    'class': class_label,\n",
    "                    'precision': round(metrics['precision'], 2),\n",
    "                    'recall': round(metrics['recall'], 2),\n",
    "                    'f1-score': round(metrics['f1-score'], 2),\n",
    "                    'support': metrics['support']\n",
    "                })\n",
    "\n",
    "    detailed_df = pd.DataFrame(detailed_data)\n",
    "    detailed_df.to_csv(f'exports/{output_prefix}_by_class.csv', index=False)\n",
    "    print(f\"‚úì R√©sultats par classe sauvegard√©s: {output_prefix}_by_class.csv\")\n",
    "\n",
    "\n",
    "    # 4. Fichier sp√©cifique pour l'analyse d'overfitting\n",
    "    overfitting_data = results_df[['model', 'method', 'train_accuracy', 'test_accuracy',\n",
    "                                     'accuracy_gap', 'train_f1_macro', 'test_f1_macro',\n",
    "                                     'f1_gap', 'overfitting']].copy()\n",
    "    overfitting_data.to_csv(f'exports/{output_prefix}_overfitting_analysis.csv', index=False)\n",
    "    print(f\"‚úì Analyse d'overfitting sauvegard√©e: {output_prefix}_overfitting_analysis.csv\")\n",
    "\n",
    "    # 5. G√©n√©ration des graphiques ROC et Pr√©cision-Rappel\n",
    "    if all_curves is not None:\n",
    "        print(\"\\nüìà G√©n√©ration des graphiques ROC et Pr√©cision-Rappel...\")\n",
    "        _plot_curves(all_curves, output_prefix)\n",
    "\n",
    "    # 6. G√©n√©ration des graphiques Importances\n",
    "    if all_df_importances is not None:\n",
    "        print(\"\\nüìà G√©n√©ration des graphiques Importances...\")\n",
    "        _plot_importances(all_df_importances, all_importances_metadata, output_prefix)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TOUS LES R√âSULTATS ONT √âT√â SAUVEGARD√âS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Affichage d'un r√©sum√© de l'overfitting\n",
    "    print(\"\\nüìä R√âSUM√â DE L'OVERFITTING:\")\n",
    "    print(\"-\" * 70)\n",
    "    for _, row in overfitting_data.iterrows():\n",
    "        status = \"‚ö†Ô∏è  OVERFITTING D√âTECT√â\" if row['overfitting'] == 'OUI' else \"‚úì  Pas d'overfitting\"\n",
    "        print(f\"{row['model']:25s} ({row['method']:17s}): {status}\")\n",
    "        print(f\"  ‚Üí √âcart accuracy: {row['accuracy_gap']:+.4f} | √âcart F1: {row['f1_gap']:+.4f}\")\n",
    "    print(\"-\" * 70)\n"
   ],
   "id": "48f53b78c9c4a2f2",
   "outputs": [],
   "execution_count": 340
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fonctions G√©n√©ration Graphiques ROC et PR\n",
    "- Courbes ROC pour classification binaire.\n",
    "- Courbes Pr√©cision-Rappel pour classification binaire."
   ],
   "id": "d6385c0d2db064a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:49:44.880224Z",
     "start_time": "2025-10-30T09:49:44.876523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _plot_curves(all_curves, output_prefix):\n",
    "    \"\"\"\n",
    "    G√©n√®re et sauvegarde les courbes pour tous les mod√®les.\n",
    "    \"\"\"\n",
    "\n",
    "    _plot_binary_roc_curves(all_curves, output_prefix)\n",
    "    _plot_binary_pr_curves(all_curves, output_prefix)\n",
    "\n",
    "def _plot_binary_roc_curves(all_curves, output_prefix):\n",
    "    \"\"\"\n",
    "    Courbes ROC pour classification binaire.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    for data in all_curves:\n",
    "        model_name = data['model']\n",
    "        y_true = data['y_true']\n",
    "\n",
    "        # V√©rifier si les probabilit√©s sont disponibles\n",
    "        if 'y_pred_proba' in data and data['y_pred_proba'] is not None:\n",
    "            y_score = data['y_pred_proba']\n",
    "            # Si format (n_samples, n_classes), prendre la colonne de la classe positive\n",
    "            if len(y_score.shape) > 1:\n",
    "                y_score = y_score[:, 1]\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Pas de probabilit√©s pour {model_name}, utilisation des pr√©dictions\")\n",
    "            y_score = data['y_pred']\n",
    "\n",
    "        # Calculer la courbe ROC\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "    # Ligne diagonale (classificateur al√©atoire)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Al√©atoire (AUC = 0.500)')\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Taux de Faux Positifs', fontsize=12)\n",
    "    plt.ylabel('Taux de Vrais Positifs', fontsize=12)\n",
    "    plt.title('Courbes ROC - Comparaison des Mod√®les', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\", fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    filename = f'images/{output_prefix}_roc_curves.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úì Courbes ROC sauvegard√©es: {output_prefix}_roc_curves.png\")\n",
    "\n",
    "def _plot_binary_pr_curves(all_curves, output_prefix):\n",
    "    \"\"\"\n",
    "    Courbes Pr√©cision-Rappel pour classification binaire.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    for data in all_curves:\n",
    "        model_name = data['model']\n",
    "        y_true = data['y_true']\n",
    "\n",
    "        if 'y_pred_proba' in data and data['y_pred_proba'] is not None:\n",
    "            y_score = data['y_pred_proba']\n",
    "            if len(y_score.shape) > 1:\n",
    "                y_score = y_score[:, 1]\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Pas de probabilit√©s pour {model_name}, utilisation des pr√©dictions\")\n",
    "            y_score = data['y_pred']\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "        avg_precision = average_precision_score(y_true, y_score)\n",
    "\n",
    "        plt.plot(recall, precision, lw=2,\n",
    "                 label=f'{model_name} (AP = {avg_precision:.3f})')\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Rappel', fontsize=12)\n",
    "    plt.ylabel('Pr√©cision', fontsize=12)\n",
    "    plt.title('Courbes Pr√©cision-Rappel - Comparaison des Mod√®les',\n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower left\", fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    filename = f'images/{output_prefix}_precision_recall_curves.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úì Courbes Pr√©cision-Rappel sauvegard√©es: {output_prefix}_precision_recall_curves.png\")"
   ],
   "id": "ec7aef29cb7a79c9",
   "outputs": [],
   "execution_count": 341
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fonctions G√©n√©ration Graphiques Importance Features\n",
    "-\n",
    "-"
   ],
   "id": "9b01cfa1d1f5e15b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Affichage des R√©sultats",
   "id": "48c7fbcb6d31f141"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:49:44.889054Z",
     "start_time": "2025-10-30T09:49:44.882148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _plot_importances(all_df_importances, all_importances_metadata, output_prefix):\n",
    "    \"\"\"\n",
    "    G√©n√®re et sauvegarde les courbes pour tous les mod√®les.\n",
    "\n",
    "    Args:\n",
    "        all_df_importances: Liste de DataFrames contenant les importances de permutation\n",
    "        all_importances_metadata: Liste de dictionnaires contenant les m√©tadonn√©es (shap, feature_importances, etc.)\n",
    "        output_prefix: Pr√©fixe pour les noms de fichiers\n",
    "    \"\"\"\n",
    "    _plot_permutation_importance(all_df_importances, output_prefix)\n",
    "    _plot_native_feature_importance(all_df_importances, all_importances_metadata, output_prefix)\n",
    "    _plot_shap(all_importances_metadata, output_prefix)\n",
    "    _plot_shap_waterfall_combined(all_importances_metadata, output_prefix)\n",
    "\n",
    "def _plot_permutation_importance(all_df_importances, output_prefix):\n",
    "    \"\"\"\n",
    "    Plot de la permutation importance pour tous les mod√®les.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # D√©finir une palette de couleurs pour diff√©rencier les mod√®les\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#96CEB4', '#FFEAA7']\n",
    "\n",
    "    all_df_importances = [df for df in all_df_importances\n",
    "                      if not df['importance_mean'].isna().all() and (df['importance_mean'] != 0).any()]\n",
    "\n",
    "    # R√©cup√©rer les features du premier DataFrame et trier alphab√©tiquement\n",
    "    first_df = all_df_importances[0].copy()\n",
    "    features = sorted(first_df['feature'].tolist())\n",
    "\n",
    "    # Position des barres pour chaque mod√®le\n",
    "    bar_height = 0.8 / len(all_df_importances)  # Ajuster la hauteur selon le nombre de mod√®les\n",
    "    y_positions = range(len(features))\n",
    "\n",
    "    for idx, df in enumerate(all_df_importances):\n",
    "        # Trier le DataFrame selon l'ordre des features\n",
    "        df_sorted = df.set_index('feature').loc[features].reset_index()\n",
    "\n",
    "        # R√©cup√©rer le nom du mod√®le si disponible\n",
    "        model_name = df.get('model', [f'Mod√®le {idx+1}'])[0] if 'model' in df.columns else f'Mod√®le {idx+1}'\n",
    "\n",
    "        # D√©caler chaque mod√®le verticalement\n",
    "        y_offset = [y + idx * bar_height for y in y_positions]\n",
    "\n",
    "        plt.barh(y_offset, df_sorted['importance_mean'],\n",
    "                 height=bar_height,\n",
    "                 label=model_name,\n",
    "                 color=colors[idx % len(colors)],\n",
    "                 alpha=0.8)\n",
    "\n",
    "    # Ajuster les positions des labels sur l'axe Y\n",
    "    plt.yticks([y + bar_height * (len(all_df_importances) - 1) / 2 for y in y_positions], features)\n",
    "    plt.xlabel('Importance Mean', fontsize=12)\n",
    "    plt.ylabel('Features', fontsize=12)\n",
    "    plt.title(f'Permutation Importance - Comparaison des Mod√®les ({output_prefix})',\n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"best\", fontsize=10)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    filename = f'images/{output_prefix}_permutation_importance_comparison.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úì Graphique de comparaison sauvegard√©: {filename}\")\n",
    "\n",
    "def _plot_native_feature_importance(all_df_importances, all_importances_metadata, output_prefix):\n",
    "    \"\"\"\n",
    "    Plot de la feature importance native pour les mod√®les tree-based,\n",
    "    avec normalisation de chaque mod√®le pour les mettre sur la m√™me √©chelle.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filtrer uniquement les mod√®les qui ont des feature_importances\n",
    "    valid_data = [\n",
    "        (df.copy(), meta) for df, meta in zip(all_df_importances, all_importances_metadata)\n",
    "        if 'feature_importances' in df.columns and df['feature_importances'].notna().any()\n",
    "    ]\n",
    "\n",
    "    if not valid_data:\n",
    "        print(f\"‚ö† Aucun mod√®le avec feature_importances natives trouv√©. Graphique ignor√©.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # Palette de couleurs pour diff√©rencier les mod√®les\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#96CEB4', '#FFEAA7']\n",
    "\n",
    "    # Normalisation des importances pour chaque mod√®le (somme = 1)\n",
    "    for df, _ in valid_data:\n",
    "        total = df['feature_importances'].sum()\n",
    "        if total != 0:\n",
    "            df['feature_importances'] = df['feature_importances'] / total\n",
    "\n",
    "    # R√©cup√©rer la liste ordonn√©e des features du premier mod√®le\n",
    "    first_df = valid_data[0][0].sort_values(by='feature_importances', ascending=True)\n",
    "    features = first_df['feature'].tolist()\n",
    "\n",
    "    # Position verticale des barres\n",
    "    bar_height = 0.8 / len(valid_data)\n",
    "    y_positions = range(len(features))\n",
    "\n",
    "    for idx, (df, meta) in enumerate(valid_data):\n",
    "        # Trier selon l‚Äôordre des features du premier mod√®le\n",
    "        df_sorted = df.set_index('feature').loc[features].reset_index()\n",
    "\n",
    "        # Nom du mod√®le\n",
    "        model_name = df.get('model', [f'Mod√®le {idx + 1}'])[0] if 'model' in df.columns else f'Mod√®le {idx + 1}'\n",
    "\n",
    "        # D√©calage vertical pour √©viter le chevauchement\n",
    "        y_offset = [y + idx * bar_height for y in y_positions]\n",
    "\n",
    "        plt.barh(\n",
    "            y_offset,\n",
    "            df_sorted['feature_importances'],\n",
    "            height=bar_height,\n",
    "            label=model_name,\n",
    "            color=colors[idx % len(colors)],\n",
    "            alpha=0.8\n",
    "        )\n",
    "\n",
    "    # Mise en forme\n",
    "    plt.yticks([y + bar_height * (len(valid_data) - 1) / 2 for y in y_positions], features)\n",
    "    plt.xlabel('Feature Importance (normalis√©e)', fontsize=12)\n",
    "    plt.ylabel('Features', fontsize=12)\n",
    "    plt.title(f'Native Feature Importance Normalis√©e - Comparaison des Mod√®les ({output_prefix})',\n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"best\", fontsize=10)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    filename = f'images/{output_prefix}_native_feature_importance_comparison_normalized.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úì Graphique normalis√© sauvegard√© : {filename}\")\n",
    "\n",
    "def _plot_shap(all_importances_metadata, output_prefix):\n",
    "    \"\"\"\n",
    "    Plot des valeurs SHAP pour les mod√®les tree-based.\n",
    "\n",
    "    Args:\n",
    "        all_importances_metadata: Liste de dictionnaires contenant X_test et shap_values\n",
    "        output_prefix: Pr√©fixe pour les noms de fichiers\n",
    "    \"\"\"\n",
    "    for idx, meta in enumerate(all_importances_metadata):\n",
    "        if meta['shap_values'] is None:\n",
    "            continue\n",
    "\n",
    "        model_name = meta.get('model', f'Mod√®le {idx + 1}')\n",
    "        X_test = meta['X_test']\n",
    "        shap_values = meta['shap_values']\n",
    "\n",
    "        # Summary plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values, X_test, show=False)\n",
    "        plt.title(f'SHAP Summary Plot - {model_name}', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        filename = f'images/{output_prefix}_shap_summary_{model_name}.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"‚úì SHAP summary plot sauvegard√©: {filename}\")\n",
    "\n",
    "def _plot_shap_waterfall_combined(all_importances_metadata, output_prefix, n_samples=5):\n",
    "    \"\"\"\n",
    "    Plot combin√© des waterfall SHAP pour plusieurs individus sur une seule figure.\n",
    "    \"\"\"\n",
    "    for idx, meta in enumerate(all_importances_metadata):\n",
    "        shap_plots = meta.get('shap_plots', [])\n",
    "\n",
    "        if not shap_plots:\n",
    "            continue\n",
    "\n",
    "        model_name = meta.get('model', f'Mod√®le {idx + 1}')\n",
    "        n_plots = min(n_samples, len(shap_plots))\n",
    "\n",
    "        # Cr√©er une figure avec subplots\n",
    "        fig, axes = plt.subplots(n_plots, 1, figsize=(12, 5 * n_plots))\n",
    "        if n_plots == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for i in range(n_plots):\n",
    "            plot_data = shap_plots[i]\n",
    "            plt.sca(axes[i])\n",
    "\n",
    "            shap.plots.waterfall(plot_data['explanation'], show=False)\n",
    "\n",
    "            true_label = plot_data['true_label']\n",
    "            pred_label = plot_data['predicted_label']\n",
    "            individual_idx = plot_data['index']\n",
    "            match_status = \"‚úì\" if true_label == pred_label else \"‚úó\"\n",
    "\n",
    "            axes[i].set_title(\n",
    "                f'Individu {individual_idx} - Vraie: {true_label} | Pr√©dite: {pred_label} {match_status}',\n",
    "                fontsize=10, fontweight='bold'\n",
    "            )\n",
    "\n",
    "        fig.suptitle(f'SHAP Waterfall Plots - {model_name}', fontsize=14, fontweight='bold', y=1.001)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        filename = f'images/{output_prefix}_{model_name}_waterfall_combined.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"‚úì SHAP waterfall combin√© sauvegard√©: {filename}\")"
   ],
   "id": "d2223fffb29cc841",
   "outputs": [],
   "execution_count": 342
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:49:45.172312Z",
     "start_time": "2025-10-30T09:49:44.892020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Comparaison des mod√®les\n",
    "results_df, detailed_reports, all_curves, all_df_importances, all_importances_metadata = compare_models(X, y)\n",
    "\n",
    "# Sauvegarde des r√©sultats\n",
    "save_results(\n",
    "    results_df,\n",
    "    detailed_reports,\n",
    "    output_prefix='classification_results',\n",
    "    all_curves=all_curves,\n",
    "    all_importances_metadata=all_importances_metadata,\n",
    "    all_df_importances=all_df_importances\n",
    ")"
   ],
   "id": "61685b8de3eb957f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPARAISON DES MOD√àLES DE CLASSIFICATION\n",
      "======================================================================\n",
      "\n",
      ">>> √âvaluation de DummyClassifier...\n",
      "  - Train/Test split...\n",
      "‚ö† Mod√®le ignor√© (non compatible avec TreeExplainer): DummyClassifier\n",
      "  - Validation crois√©e (5 folds)...\n",
      "  ‚úì DummyClassifier termin√©\n",
      "\n",
      ">>> √âvaluation de RandomForestRegressor...\n",
      "  - Train/Test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[343]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Comparaison des mod√®les\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m results_df, detailed_reports, all_curves, all_df_importances, all_importances_metadata = \u001B[43mcompare_models\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# Sauvegarde des r√©sultats\u001B[39;00m\n\u001B[32m      5\u001B[39m save_results(\n\u001B[32m      6\u001B[39m     results_df,\n\u001B[32m      7\u001B[39m     detailed_reports,\n\u001B[32m   (...)\u001B[39m\u001B[32m     11\u001B[39m     all_df_importances=all_df_importances\n\u001B[32m     12\u001B[39m )\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[339]\u001B[39m\u001B[32m, line 64\u001B[39m, in \u001B[36mcompare_models\u001B[39m\u001B[34m(X, y, test_size, random_state, cv_folds)\u001B[39m\n\u001B[32m     62\u001B[39m \u001B[38;5;66;03m# Train/Test\u001B[39;00m\n\u001B[32m     63\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m  - Train/Test split...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m64\u001B[39m tt_results, report, cm, curves, df_importances,  importances_metadata = \u001B[43mtrain_test_evaluation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     65\u001B[39m all_results.append(tt_results)\n\u001B[32m     67\u001B[39m \u001B[38;5;66;03m# Sauvegarde du rapport d√©taill√©\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[337]\u001B[39m\u001B[32m, line 30\u001B[39m, in \u001B[36mtrain_test_evaluation\u001B[39m\u001B[34m(model, X_train, X_test, y_train, y_test, model_name, n_shap_samples)\u001B[39m\n\u001B[32m     28\u001B[39m \u001B[38;5;66;03m# Pr√©dictions sur TRAIN\u001B[39;00m\n\u001B[32m     29\u001B[39m y_pred_train = pipeline.predict(X_train)\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m report_train = \u001B[43mclassification_report\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     32\u001B[39m \u001B[38;5;66;03m# Pr√©dictions sur TEST\u001B[39;00m\n\u001B[32m     33\u001B[39m y_pred_test = pipeline.predict(X_test)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:218\u001B[39m, in \u001B[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    212\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    213\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m    214\u001B[39m         skip_parameter_validation=(\n\u001B[32m    215\u001B[39m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m    216\u001B[39m         )\n\u001B[32m    217\u001B[39m     ):\n\u001B[32m--> \u001B[39m\u001B[32m218\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    220\u001B[39m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[32m    222\u001B[39m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[32m    224\u001B[39m     msg = re.sub(\n\u001B[32m    225\u001B[39m         \u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mw+ must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    226\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    227\u001B[39m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[32m    228\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2948\u001B[39m, in \u001B[36mclassification_report\u001B[39m\u001B[34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001B[39m\n\u001B[32m   2840\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Build a text report showing the main classification metrics.\u001B[39;00m\n\u001B[32m   2841\u001B[39m \n\u001B[32m   2842\u001B[39m \u001B[33;03mRead more in the :ref:`User Guide <classification_report>`.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   2944\u001B[39m \u001B[33;03m<BLANKLINE>\u001B[39;00m\n\u001B[32m   2945\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   2947\u001B[39m y_true, y_pred = attach_unique(y_true, y_pred)\n\u001B[32m-> \u001B[39m\u001B[32m2948\u001B[39m y_type, y_true, y_pred = \u001B[43m_check_targets\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2950\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   2951\u001B[39m     labels = unique_labels(y_true, y_pred)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:106\u001B[39m, in \u001B[36m_check_targets\u001B[39m\u001B[34m(y_true, y_pred)\u001B[39m\n\u001B[32m    103\u001B[39m     y_type = {\u001B[33m\"\u001B[39m\u001B[33mmulticlass\u001B[39m\u001B[33m\"\u001B[39m}\n\u001B[32m    105\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(y_type) > \u001B[32m1\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m106\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    107\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mClassification metrics can\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt handle a mix of \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[33m and \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[33m targets\u001B[39m\u001B[33m\"\u001B[39m.format(\n\u001B[32m    108\u001B[39m             type_true, type_pred\n\u001B[32m    109\u001B[39m         )\n\u001B[32m    110\u001B[39m     )\n\u001B[32m    112\u001B[39m \u001B[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001B[39;00m\n\u001B[32m    113\u001B[39m y_type = y_type.pop()\n",
      "\u001B[31mValueError\u001B[39m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "execution_count": 343
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Modele NON LINEAIRE\n",
    "- RandomForest, XGBoost ou CatBoost\n",
    "- M√©triques d‚Äô√©valuation en classification : matrice de confusion, rappel et pr√©cision.\n",
    "- Scores (pr√©sence d‚Äôoverfit ou non, capacit√© d‚Äô√©viter les faux positifs ou faux n√©gatifs)"
   ],
   "id": "cdbcdda4b8fc0779"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Am√©lioration de la classification\n",
    "- demandez-vous si √©viter des faux positifs est plus important qu‚Äô√©viter des faux n√©gatifs."
   ],
   "id": "fcd52f7d66ce3752"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Feature Importance\n",
    "\n",
    "### Feature Importance globale : approche comparative\n",
    "- Permutation Importance (sklearn) -> PERMUTATION\n",
    "- Feature Importance native (si mod√®le √† base d‚Äôarbre) -> NATIVE\n",
    "- SHAP Global Importance (Beeswarm Plot) -> SHAP\n",
    "\n",
    "### Shapley Values\n",
    "- SHAP Global Importance (Beeswarm Plot) -> SHAP\n",
    "\n",
    "### Feature Importance local\n",
    "- Feature Importance locale (Waterfall Plot)"
   ],
   "id": "8df47121e35fadb5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# OPTIMISATION DES HYPER-PARAMETRES",
   "id": "c7724358cbe70863"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:49:45.175436Z",
     "start_time": "2025-10-30T08:41:51.177704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_param_grids():\n",
    "    \"\"\"\n",
    "    D√©finit les grilles de param√®tres pour chaque mod√®le.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionnaire avec les grilles de param√®tres pour chaque mod√®le\n",
    "    \"\"\"\n",
    "    param_grids = {\n",
    "        'DummyClassifier': {\n",
    "            'strategy': ['most_frequent', 'stratified', 'uniform']\n",
    "        },\n",
    "\n",
    "        'RandomForestClassifier': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3,4,5,6,7, None],\n",
    "            'min_samples_split': [2, 5, 10, 15],\n",
    "            'min_samples_leaf': [1, 5, 10],\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        },\n",
    "\n",
    "        'XGBClassifier': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.3],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0],\n",
    "            'reg_alpha': [0, 0.1, 1],\n",
    "            'reg_lambda': [1, 2, 5]\n",
    "        },\n",
    "\n",
    "        'CatBoostClassifier': {\n",
    "            'iterations': [100, 200, 300],\n",
    "            'depth': [4, 6, 8],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'l2_leaf_reg': [1, 3, 5],\n",
    "            'subsample': [0.8, 1.0]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return param_grids\n",
    "\n",
    "def get_small_param_grids():\n",
    "    \"\"\"\n",
    "    Grilles r√©duites pour des tests rapides.\n",
    "\n",
    "    Returns:\n",
    "        dict: Grilles de param√®tres r√©duites\n",
    "    \"\"\"\n",
    "    param_grids = {\n",
    "        'DummyClassifier': {\n",
    "            'strategy': ['most_frequent', 'stratified']\n",
    "        },\n",
    "\n",
    "        'RandomForestClassifier': {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [5, 10, None],\n",
    "            'min_samples_split': [2, 10],\n",
    "            'max_features': ['sqrt']\n",
    "        },\n",
    "\n",
    "        'XGBClassifier': {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [3, 5],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'reg_lambda': [1, 2]\n",
    "        },\n",
    "\n",
    "        'CatBoostClassifier': {\n",
    "            'iterations': [50, 100],\n",
    "            'depth': [4, 6],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'l2_leaf_reg': [1, 3]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return param_grids\n",
    "\n",
    "def perform_grid_search(model, param_grid, X_train, y_train, model_name,\n",
    "                       cv=5, scoring='f1_macro', n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Effectue un GridSearchCV pour un mod√®le donn√©.\n",
    "\n",
    "    Args:\n",
    "        model: Le mod√®le √† optimiser\n",
    "        param_grid: Grille de param√®tres\n",
    "        X_train, y_train: Donn√©es d'entra√Ænement\n",
    "        model_name: Nom du mod√®le\n",
    "        cv: Nombre de folds pour la validation crois√©e\n",
    "        scoring: M√©trique d'optimisation\n",
    "        n_jobs: Nombre de processus parall√®les\n",
    "\n",
    "    Returns:\n",
    "        tuple: (GridSearchCV object, r√©sultats dict)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üîç Grid Search pour {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Nombre de combinaisons √† tester: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "    print(f\"M√©trique d'optimisation: {scoring}\")\n",
    "    print(f\"Validation crois√©e: {cv} folds\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Configuration de la validation crois√©e stratifi√©e\n",
    "    cv_strategy = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "    # GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv_strategy,\n",
    "        scoring=scoring,\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=0,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Entra√Ænement\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # R√©sultats\n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'best_score': round(grid_search.best_score_, 2),\n",
    "        'best_params': str(grid_search.best_params_),\n",
    "        'n_combinations': len(grid_search.cv_results_['params']),\n",
    "        'time_seconds': round(elapsed_time, 0)\n",
    "    }\n",
    "\n",
    "    print(f\"‚úì Termin√© en {elapsed_time:.2f} secondes\")\n",
    "    print(f\"üìä Meilleur score ({scoring}): {grid_search.best_score_:.4f}\")\n",
    "    print(f\"üèÜ Meilleurs param√®tres:\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"   - {param}: {value}\")\n",
    "\n",
    "    return grid_search, results\n",
    "\n",
    "def evaluate_best_model(grid_search, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    √âvalue le meilleur mod√®le trouv√© par GridSearch sur le jeu de test.\n",
    "\n",
    "    Args:\n",
    "        grid_search: Objet GridSearchCV entra√Æn√©\n",
    "        X_train, X_test: Features\n",
    "        y_train, y_test: Labels\n",
    "        model_name: Nom du mod√®le\n",
    "\n",
    "    Returns:\n",
    "        dict: R√©sultats d'√©valuation\n",
    "    \"\"\"\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Pr√©dictions\n",
    "    y_pred_train = best_model.predict(X_train)\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "    # Rapports\n",
    "    report_train = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "    report_test = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "\n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "    # Calcul des √©carts (overfitting)\n",
    "    accuracy_gap = report_train['accuracy'] - report_test['accuracy']\n",
    "    f1_gap = report_train['macro avg']['f1-score'] - report_test['macro avg']['f1-score']\n",
    "    overfitting = 'OUI' if (accuracy_gap > 0.12 or f1_gap > 0.12) else 'NON'\n",
    "\n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'train_accuracy': round(report_train['accuracy'], 2),\n",
    "        'test_accuracy': round(report_test['accuracy'], 2),\n",
    "        'train_f1_macro': round(report_train['macro avg']['f1-score'], 2),\n",
    "        'test_f1_macro': round(report_test['macro avg']['f1-score'], 2),\n",
    "        'train_precision': round(report_train['macro avg']['precision'], 2),\n",
    "        'test_precision': round(report_test['macro avg']['precision'], 2),\n",
    "        'train_recall': round(report_train['macro avg']['recall'], 2),\n",
    "        'test_recall': round(report_test['macro avg']['recall'], 2),\n",
    "        'accuracy_gap': round(accuracy_gap, 2),\n",
    "        'f1_gap': round(f1_gap, 2),\n",
    "        'overfitting': overfitting,\n",
    "        'confusion_matrix': str(cm.tolist())\n",
    "    }\n",
    "\n",
    "    print(f\"\\nüìà √âvaluation sur le jeu de test:\")\n",
    "    print(f\"   Train accuracy: {report_train['accuracy']:.4f}\")\n",
    "    print(f\"   Test accuracy:  {report_test['accuracy']:.4f}\")\n",
    "    print(f\"   √âcart accuracy: {accuracy_gap:+.4f}\")\n",
    "    print(f\"   Overfitting:    {overfitting}\")\n",
    "\n",
    "    return results, report_test, cm\n",
    "\n",
    "def compare_models_gridsearch(X, y, param_grids='full', test_size=0.15,\n",
    "                              cv=5, scoring='f1_macro', random_state=82):\n",
    "    \"\"\"\n",
    "    Compare tous les mod√®les avec GridSearchCV.\n",
    "\n",
    "    Args:\n",
    "        X: Features\n",
    "        y: Labels\n",
    "        param_grids: 'full', 'small', ou dict personnalis√©\n",
    "        test_size: Proportion du jeu de test\n",
    "        cv: Nombre de folds\n",
    "        scoring: M√©trique d'optimisation\n",
    "        random_state: Seed\n",
    "\n",
    "    Returns:\n",
    "        tuple: (r√©sultats_grid, r√©sultats_eval, grid_objects)\n",
    "    \"\"\"\n",
    "    # Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"GRIDSEARCH - COMPARAISON DES MOD√àLES\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Taille du dataset: {len(X)} √©chantillons\")\n",
    "    print(f\"Train: {len(X_train)} | Test: {len(X_test)}\")\n",
    "    print(f\"Distribution des classes: {dict(pd.Series(y).value_counts())}\")\n",
    "\n",
    "    # S√©lection de la grille\n",
    "    if param_grids == 'full':\n",
    "        grids = get_param_grids()\n",
    "    elif param_grids == 'small':\n",
    "        grids = get_small_param_grids()\n",
    "    else:\n",
    "        grids = param_grids\n",
    "\n",
    "    # Mod√®les de base\n",
    "    base_models = {\n",
    "        'DummyClassifier': DummyClassifier(random_state=random_state),\n",
    "        'RandomForestClassifier': RandomForestClassifier(random_state=random_state),\n",
    "        'XGBClassifier': XGBClassifier(random_state=random_state, eval_metric='logloss'),\n",
    "        'CatBoostClassifier': CatBoostClassifier(random_state=random_state, verbose=False)\n",
    "    }\n",
    "\n",
    "    grid_results = []\n",
    "    eval_results = []\n",
    "    grid_objects = {}\n",
    "    detailed_reports = []\n",
    "\n",
    "    # GridSearch pour chaque mod√®le\n",
    "    for model_name, base_model in base_models.items():\n",
    "        param_grid = grids[model_name]\n",
    "\n",
    "        # GridSearch\n",
    "        grid_search, grid_res = perform_grid_search(\n",
    "            base_model, param_grid, X_train, y_train,\n",
    "            model_name, cv=cv, scoring=scoring\n",
    "        )\n",
    "        grid_results.append(grid_res)\n",
    "        grid_objects[model_name] = grid_search\n",
    "\n",
    "        # √âvaluation du meilleur mod√®le\n",
    "        eval_res, report, cm = evaluate_best_model(\n",
    "            grid_search, X_train, X_test, y_train, y_test, model_name\n",
    "        )\n",
    "        eval_results.append(eval_res)\n",
    "\n",
    "        detailed_reports.append({\n",
    "            'model': model_name,\n",
    "            'report': report,\n",
    "            'confusion_matrix': cm\n",
    "        })\n",
    "\n",
    "    # Conversion en DataFrames\n",
    "    grid_df = pd.DataFrame(grid_results)\n",
    "    eval_df = pd.DataFrame(eval_results)\n",
    "\n",
    "    return grid_df, eval_df, grid_objects, detailed_reports\n",
    "\n",
    "def save_gridsearch_results(grid_df, eval_df, grid_objects, detailed_reports,\n",
    "                            output_prefix='gridsearch_results'):\n",
    "    \"\"\"\n",
    "    Sauvegarde tous les r√©sultats du GridSearch.\n",
    "\n",
    "    Args:\n",
    "        grid_df: DataFrame avec r√©sultats du GridSearch\n",
    "        eval_df: DataFrame avec √©valuation finale\n",
    "        grid_objects: Dict des objets GridSearchCV\n",
    "        detailed_reports: Rapports d√©taill√©s\n",
    "        output_prefix: Pr√©fixe des fichiers\n",
    "    \"\"\"\n",
    "    # 1. R√©sum√© du GridSearch\n",
    "    grid_df.to_csv(f'exports/{output_prefix}_grid_summary.csv', index=False)\n",
    "    print(f\"\\n‚úì R√©sum√© GridSearch: {output_prefix}_grid_summary.csv\")\n",
    "\n",
    "    # 2. √âvaluation finale\n",
    "    eval_df.to_csv(f'exports/{output_prefix}_evaluation.csv', index=False)\n",
    "    print(f\"‚úì √âvaluation finale: {output_prefix}_evaluation.csv\")\n",
    "\n",
    "    # 3. D√©tails complets du GridSearch (tous les r√©sultats)\n",
    "    all_cv_results = []\n",
    "    for model_name, grid_search in grid_objects.items():\n",
    "        cv_res = pd.DataFrame(grid_search.cv_results_)\n",
    "        cv_res.insert(0, 'model', model_name)\n",
    "        all_cv_results.append(cv_res)\n",
    "\n",
    "    full_cv_df = pd.concat(all_cv_results, ignore_index=True)\n",
    "    full_cv_df.to_csv(f'exports/{output_prefix}_full_cv_results.csv', index=False)\n",
    "    print(f\"‚úì R√©sultats CV complets: {output_prefix}_full_cv_results.csv\")\n",
    "\n",
    "    # 4. Meilleurs param√®tres\n",
    "    best_params_data = []\n",
    "    for model_name, grid_search in grid_objects.items():\n",
    "        for param, value in grid_search.best_params_.items():\n",
    "            best_params_data.append({\n",
    "                'model': model_name,\n",
    "                'parameter': param,\n",
    "                'value': str(value)\n",
    "            })\n",
    "\n",
    "    best_params_df = pd.DataFrame(best_params_data)\n",
    "    best_params_df.to_csv(f'exports/{output_prefix}_best_params.csv', index=False)\n",
    "    print(f\"‚úì Meilleurs param√®tres: {output_prefix}_best_params.csv\")\n",
    "\n",
    "    # 5. Matrices de confusion\n",
    "    #cm_data = []\n",
    "    #for item in detailed_reports:\n",
    "    #    cm_data.append({\n",
    "    #        'model': item['model'],\n",
    "    #        'confusion_matrix': str(item['confusion_matrix'].tolist())\n",
    "    #    })\n",
    "    #cm_df = pd.DataFrame(cm_data)\n",
    "    #cm_df.to_csv(f'exports/{output_prefix}_confusion_matrices.csv', index=False)\n",
    "    #print(f\"‚úì Matrices de confusion: {output_prefix}_confusion_matrices.csv\")\n",
    "\n",
    "    # 6. Analyse d'overfitting\n",
    "    overfitting_cols = ['model', 'train_accuracy', 'test_accuracy', 'accuracy_gap',\n",
    "                       'train_f1_macro', 'test_f1_macro', 'f1_gap', 'overfitting']\n",
    "    overfitting_df = eval_df[overfitting_cols]\n",
    "    overfitting_df.to_csv(f'exports/{output_prefix}_overfitting.csv', index=False)\n",
    "    print(f\"‚úì Analyse overfitting: {output_prefix}_overfitting.csv\")\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TOUS LES R√âSULTATS ONT √âT√â SAUVEGARD√âS\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Affichage du classement\n",
    "    print(f\"\\nüèÜ CLASSEMENT DES MOD√àLES (par {grid_objects[list(grid_objects.keys())[0]].scoring}):\")\n",
    "    print(\"-\" * 70)\n",
    "    ranking = grid_df.sort_values('best_score', ascending=False)\n",
    "    for i, row in ranking.iterrows():\n",
    "        print(f\"{i+1}. {row['model']:25s} - Score: {row['best_score']:.4f} - Temps: {row['time_seconds']:.1f}s\")\n",
    "\n",
    "    print(\"\\nüìä D√âTECTION D'OVERFITTING:\")\n",
    "    print(\"-\" * 70)\n",
    "    for _, row in eval_df.iterrows():\n",
    "        status = \"‚ö†Ô∏è  OVERFITTING\" if row['overfitting'] == 'OUI' else \"‚úì  Pas d'overfitting\"\n",
    "        print(f\"{row['model']:25s}: {status} (√©cart: {row['accuracy_gap']:+.4f})\")\n",
    "\n",
    "# GridSearch avec grille r√©duite (rapide pour test)\n",
    "# Utilisez param_grids='full' pour une recherche compl√®te\n",
    "grid_df, eval_df, grid_objects, detailed_reports = compare_models_gridsearch(\n",
    "    X, y,\n",
    "    param_grids='full',  # 'small', 'full', ou dict personnalis√©\n",
    "    test_size=0.15,\n",
    "    cv=5,  # 3 pour test rapide, 5 recommand√©\n",
    "    scoring='f1_macro',\n",
    "    random_state=82\n",
    ")\n",
    "\n",
    "# Sauvegarde\n",
    "save_gridsearch_results(\n",
    "    grid_df, eval_df, grid_objects, detailed_reports,\n",
    "    output_prefix='gridsearch_results'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"APER√áU DES MEILLEURS R√âSULTATS\")\n",
    "print(\"=\"*70)\n",
    "print(grid_df.round(4).to_string(index=False))"
   ],
   "id": "5b89ecdc8742d28a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GRIDSEARCH - COMPARAISON DES MOD√àLES\n",
      "======================================================================\n",
      "Taille du dataset: 1470 √©chantillons\n",
      "Train: 1249 | Test: 221\n",
      "Distribution des classes: {False: np.int64(1233), True: np.int64(237)}\n",
      "\n",
      "======================================================================\n",
      "üîç Grid Search pour DummyClassifier\n",
      "======================================================================\n",
      "Nombre de combinaisons √† tester: 3\n",
      "M√©trique d'optimisation: f1_macro\n",
      "Validation crois√©e: 5 folds\n",
      "‚úì Termin√© en 1.69 secondes\n",
      "üìä Meilleur score (f1_macro): 0.5202\n",
      "üèÜ Meilleurs param√®tres:\n",
      "   - strategy: stratified\n",
      "\n",
      "üìà √âvaluation sur le jeu de test:\n",
      "   Train accuracy: 0.7462\n",
      "   Test accuracy:  0.7149\n",
      "   √âcart accuracy: +0.0313\n",
      "   Overfitting:    NON\n",
      "\n",
      "======================================================================\n",
      "üîç Grid Search pour RandomForestClassifier\n",
      "======================================================================\n",
      "Nombre de combinaisons √† tester: 432\n",
      "M√©trique d'optimisation: f1_macro\n",
      "Validation crois√©e: 5 folds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[232]\u001B[39m\u001B[32m, line 358\u001B[39m\n\u001B[32m    354\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrow[\u001B[33m'\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m25s\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstatus\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m (√©cart: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrow[\u001B[33m'\u001B[39m\u001B[33maccuracy_gap\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m+.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m)\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    356\u001B[39m \u001B[38;5;66;03m# GridSearch avec grille r√©duite (rapide pour test)\u001B[39;00m\n\u001B[32m    357\u001B[39m \u001B[38;5;66;03m# Utilisez param_grids='full' pour une recherche compl√®te\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m358\u001B[39m grid_df, eval_df, grid_objects, detailed_reports = \u001B[43mcompare_models_gridsearch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43mparam_grids\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mfull\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# 'small', 'full', ou dict personnalis√©\u001B[39;49;00m\n\u001B[32m    361\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.15\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    362\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcv\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# 3 pour test rapide, 5 recommand√©\u001B[39;49;00m\n\u001B[32m    363\u001B[39m \u001B[43m    \u001B[49m\u001B[43mscoring\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mf1_macro\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    364\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m82\u001B[39;49m\n\u001B[32m    365\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m    367\u001B[39m \u001B[38;5;66;03m# Sauvegarde\u001B[39;00m\n\u001B[32m    368\u001B[39m save_gridsearch_results(\n\u001B[32m    369\u001B[39m     grid_df, eval_df, grid_objects, detailed_reports,\n\u001B[32m    370\u001B[39m     output_prefix=\u001B[33m'\u001B[39m\u001B[33mgridsearch_results\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m    371\u001B[39m )\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[232]\u001B[39m\u001B[32m, line 251\u001B[39m, in \u001B[36mcompare_models_gridsearch\u001B[39m\u001B[34m(X, y, param_grids, test_size, cv, scoring, random_state)\u001B[39m\n\u001B[32m    248\u001B[39m param_grid = grids[model_name]\n\u001B[32m    250\u001B[39m \u001B[38;5;66;03m# GridSearch\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m251\u001B[39m grid_search, grid_res = \u001B[43mperform_grid_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbase_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_grid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    253\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcv\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscoring\u001B[49m\u001B[43m=\u001B[49m\u001B[43mscoring\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    255\u001B[39m grid_results.append(grid_res)\n\u001B[32m    256\u001B[39m grid_objects[model_name] = grid_search\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[232]\u001B[39m\u001B[32m, line 120\u001B[39m, in \u001B[36mperform_grid_search\u001B[39m\u001B[34m(model, param_grid, X_train, y_train, model_name, cv, scoring, n_jobs)\u001B[39m\n\u001B[32m    109\u001B[39m grid_search = GridSearchCV(\n\u001B[32m    110\u001B[39m     estimator=model,\n\u001B[32m    111\u001B[39m     param_grid=param_grid,\n\u001B[32m   (...)\u001B[39m\u001B[32m    116\u001B[39m     return_train_score=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    117\u001B[39m )\n\u001B[32m    119\u001B[39m \u001B[38;5;66;03m# Entra√Ænement\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m \u001B[43mgrid_search\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    122\u001B[39m elapsed_time = time.time() - start_time\n\u001B[32m    124\u001B[39m \u001B[38;5;66;03m# R√©sultats\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/base.py:1365\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1358\u001B[39m     estimator._validate_params()\n\u001B[32m   1360\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1361\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1362\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1363\u001B[39m     )\n\u001B[32m   1364\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1365\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1051\u001B[39m, in \u001B[36mBaseSearchCV.fit\u001B[39m\u001B[34m(self, X, y, **params)\u001B[39m\n\u001B[32m   1045\u001B[39m     results = \u001B[38;5;28mself\u001B[39m._format_results(\n\u001B[32m   1046\u001B[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[32m   1047\u001B[39m     )\n\u001B[32m   1049\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[32m-> \u001B[39m\u001B[32m1051\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1053\u001B[39m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[32m   1054\u001B[39m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[32m   1055\u001B[39m first_test_score = all_out[\u001B[32m0\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mtest_scores\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1605\u001B[39m, in \u001B[36mGridSearchCV._run_search\u001B[39m\u001B[34m(self, evaluate_candidates)\u001B[39m\n\u001B[32m   1603\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[32m   1604\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1605\u001B[39m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:997\u001B[39m, in \u001B[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[39m\u001B[34m(candidate_params, cv, more_results)\u001B[39m\n\u001B[32m    989\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.verbose > \u001B[32m0\u001B[39m:\n\u001B[32m    990\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[32m    991\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[33m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[33m candidates,\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    992\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m fits\u001B[39m\u001B[33m\"\u001B[39m.format(\n\u001B[32m    993\u001B[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001B[32m    994\u001B[39m         )\n\u001B[32m    995\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m997\u001B[39m out = \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    998\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    999\u001B[39m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1000\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1001\u001B[39m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1002\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1003\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1004\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1005\u001B[39m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1006\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1007\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1008\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1009\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1010\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1011\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplitter\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1012\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1013\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1015\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) < \u001B[32m1\u001B[39m:\n\u001B[32m   1016\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1017\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mNo fits were performed. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1018\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWas the CV iterator empty? \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1019\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWere there no candidates?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1020\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:82\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m     73\u001B[39m warning_filters = warnings.filters\n\u001B[32m     74\u001B[39m iterable_with_config_and_warning_filters = (\n\u001B[32m     75\u001B[39m     (\n\u001B[32m     76\u001B[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001B[32m   (...)\u001B[39m\u001B[32m     80\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[32m     81\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m82\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config_and_warning_filters\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/joblib/parallel.py:2072\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   2066\u001B[39m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[32m   2067\u001B[39m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[32m   2068\u001B[39m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[32m   2069\u001B[39m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[32m   2070\u001B[39m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[32m-> \u001B[39m\u001B[32m2072\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/joblib/parallel.py:1682\u001B[39m, in \u001B[36mParallel._get_outputs\u001B[39m\u001B[34m(self, iterator, pre_dispatch)\u001B[39m\n\u001B[32m   1679\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[32m   1681\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backend.retrieval_context():\n\u001B[32m-> \u001B[39m\u001B[32m1682\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m._retrieve()\n\u001B[32m   1684\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[32m   1685\u001B[39m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[32m   1686\u001B[39m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[32m   1687\u001B[39m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[32m   1688\u001B[39m     \u001B[38;5;28mself\u001B[39m._exception = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/joblib/parallel.py:1800\u001B[39m, in \u001B[36mParallel._retrieve\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1789\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_ordered:\n\u001B[32m   1790\u001B[39m     \u001B[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001B[39;00m\n\u001B[32m   1791\u001B[39m     \u001B[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1795\u001B[39m     \u001B[38;5;66;03m# control only have to be done on the amount of time the next\u001B[39;00m\n\u001B[32m   1796\u001B[39m     \u001B[38;5;66;03m# dispatched job is pending.\u001B[39;00m\n\u001B[32m   1797\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m (nb_jobs == \u001B[32m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m   1798\u001B[39m         \u001B[38;5;28mself\u001B[39m._jobs[\u001B[32m0\u001B[39m].get_status(timeout=\u001B[38;5;28mself\u001B[39m.timeout) == TASK_PENDING\n\u001B[32m   1799\u001B[39m     ):\n\u001B[32m-> \u001B[39m\u001B[32m1800\u001B[39m         \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1801\u001B[39m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m   1803\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m nb_jobs == \u001B[32m0\u001B[39m:\n\u001B[32m   1804\u001B[39m     \u001B[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001B[39;00m\n\u001B[32m   1805\u001B[39m     \u001B[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1811\u001B[39m     \u001B[38;5;66;03m# timeouts before any other dispatched job has completed and\u001B[39;00m\n\u001B[32m   1812\u001B[39m     \u001B[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 232
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "cdc654ce69a92aac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
