{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import des modules",
   "id": "1475dfadac83f672"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T10:13:33.453761Z",
     "start_time": "2025-11-03T10:13:33.451099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Selection\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, KFold, StratifiedKFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import shap\n",
    "\n",
    "#Modèles\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Metriques\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, precision_recall_curve, average_precision_score, roc_curve, auc\n"
   ],
   "id": "5a67d0009f86326a",
   "outputs": [],
   "execution_count": 251
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T10:13:33.463335Z",
     "start_time": "2025-11-03T10:13:33.457015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fc = pd.read_csv('fc_after_feature_engineering.csv')\n",
    "print(fc.info())"
   ],
   "id": "8536898289714cfe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1470 entries, 0 to 1469\n",
      "Data columns (total 48 columns):\n",
      " #   Column                                     Non-Null Count  Dtype  \n",
      "---  ------                                     --------------  -----  \n",
      " 0   a_quitte_l_entreprise                      1470 non-null   int64  \n",
      " 1   age                                        1470 non-null   int64  \n",
      " 2   annees_dans_l_entreprise                   1470 non-null   int64  \n",
      " 3   annees_dans_le_poste_actuel                1470 non-null   int64  \n",
      " 4   annees_depuis_la_derniere_promotion        1470 non-null   int64  \n",
      " 5   annees_experience_totale                   1470 non-null   int64  \n",
      " 6   annes_sous_responsable_actuel              1470 non-null   int64  \n",
      " 7   augmentation_salaire_precedente            1470 non-null   int64  \n",
      " 8   departement_Commercial                     1470 non-null   int64  \n",
      " 9   departement_Consulting                     1470 non-null   int64  \n",
      " 10  departement_Ressources Humaines            1470 non-null   int64  \n",
      " 11  distance_domicile_travail                  1470 non-null   int64  \n",
      " 12  domaine_etude_Autre                        1470 non-null   int64  \n",
      " 13  domaine_etude_Entrepreunariat              1470 non-null   int64  \n",
      " 14  domaine_etude_Infra & Cloud                1470 non-null   int64  \n",
      " 15  domaine_etude_Marketing                    1470 non-null   int64  \n",
      " 16  domaine_etude_Ressources Humaines          1470 non-null   int64  \n",
      " 17  domaine_etude_Transformation Digitale      1470 non-null   int64  \n",
      " 18  frequence_deplacement                      1470 non-null   float64\n",
      " 19  genre                                      1470 non-null   int64  \n",
      " 20  heure_supplementaires                      1470 non-null   int64  \n",
      " 21  id_employee                                1470 non-null   int64  \n",
      " 22  nb_formations_suivies                      1470 non-null   int64  \n",
      " 23  niveau_education                           1470 non-null   int64  \n",
      " 24  niveau_hierarchique_poste                  1470 non-null   int64  \n",
      " 25  nombre_experiences_precedentes             1470 non-null   int64  \n",
      " 26  nombre_participation_pee                   1470 non-null   int64  \n",
      " 27  note_evaluation_precedente                 1470 non-null   int64  \n",
      " 28  poste_Assistant de Direction               1470 non-null   int64  \n",
      " 29  poste_Cadre Commercial                     1470 non-null   int64  \n",
      " 30  poste_Consultant                           1470 non-null   int64  \n",
      " 31  poste_Directeur Technique                  1470 non-null   int64  \n",
      " 32  poste_Manager                              1470 non-null   int64  \n",
      " 33  poste_Representant Commercial              1470 non-null   int64  \n",
      " 34  poste_Ressources Humaines                  1470 non-null   int64  \n",
      " 35  poste_Senior Manager                       1470 non-null   int64  \n",
      " 36  poste_Tech Lead                            1470 non-null   int64  \n",
      " 37  ratio_experience_interne                   1470 non-null   float64\n",
      " 38  revenu_mensuel                             1470 non-null   int64  \n",
      " 39  satisfaction_employee_environnement        1470 non-null   int64  \n",
      " 40  satisfaction_employee_equilibre_pro_perso  1470 non-null   int64  \n",
      " 41  satisfaction_employee_equipe               1470 non-null   int64  \n",
      " 42  satisfaction_employee_nature_travail       1470 non-null   int64  \n",
      " 43  satisfaction_globale                       1470 non-null   float64\n",
      " 44  score_salaire                              1470 non-null   float64\n",
      " 45  statut_marital_Celibataire                 1470 non-null   int64  \n",
      " 46  statut_marital_Divorce(e)                  1470 non-null   int64  \n",
      " 47  statut_marital_Marie(e)                    1470 non-null   int64  \n",
      "dtypes: float64(4), int64(44)\n",
      "memory usage: 551.4 KB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 252
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Déclaration des Dataframes X et y\n",
    "- Un DataFrame contenant les features => X\n",
    "- Un Pandas Series contenant la colonne cible => y"
   ],
   "id": "af11a0eccb59eebf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T10:13:33.467895Z",
     "start_time": "2025-11-03T10:13:33.465099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Un homme , Célibataire , Entre 30 et 40 ans , Consultant , Entre 1 et 7 années dans l’entreprise, Un revenu compris entre 2500€ et 6000€, Domaine d’etude Infra & Cloud, Qui a moins de 5 années sous son responsable actuel, Qui a une distance domicile travail entre 3 et 17km\n",
    "\n",
    "# Test\n",
    "#columns_base = ['heure_supplementaires','genre','revenu_mensuel', 'age', 'distance_domicile_travail','score_salaire', 'augmentation_salaire_precedente','frequence_deplacement', 'nb_formations_suivies', 'niveau_education', 'niveau_hierarchique_poste', 'nombre_experiences_precedentes', 'nombre_participation_pee', 'note_evaluation_precedente','ratio_experience_interne']\n",
    "\n",
    "columns_base = ['heure_supplementaires','nombre_participation_pee','score_salaire', 'nombre_experiences_precedentes'] # OPTIMISATION XGB\n",
    "\n",
    "columns_annees = [col for col in fc.columns if col.startswith('annees')]\n",
    "columns_domaine_etude = [col for col in fc.columns if col.startswith('domaine_etude')]\n",
    "columns_poste = [col for col in fc.columns if col.startswith('poste')]\n",
    "columns_statut_marital = [col for col in fc.columns if col.startswith('statut_marital')]\n",
    "columns_satisfaction = [col for col in fc.columns if col.startswith('satisfaction')]\n",
    "\n",
    "all_columns = columns_base + columns_statut_marital + columns_domaine_etude + columns_poste + columns_satisfaction + columns_annees\n",
    "\n",
    "X = fc[all_columns]\n",
    "y = fc['a_quitte_l_entreprise']"
   ],
   "id": "27791c8c368a589d",
   "outputs": [],
   "execution_count": 253
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Split train/test",
   "id": "7a80e9fb09b6656"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T10:13:33.475309Z",
     "start_time": "2025-11-03T10:13:33.470701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "random_state = 82\n",
    "test_size = 0.15\n",
    "\n",
    "# Le terme stratification désigne une méthode d’échantillonnage qui garantit que la répartition des classes reste proportionnelle entre les jeux d’entraînement et de test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)"
   ],
   "id": "94b88355994fb7b4",
   "outputs": [],
   "execution_count": 254
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Entrainement Modeles Non Lineaires\n",
   "id": "34e2c433faba1d20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T10:13:33.480521Z",
     "start_time": "2025-11-03T10:13:33.477050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, X_train, X_test, y_train, y_test, model_name, overfit_threshold=0.12):\n",
    "    \"\"\"\n",
    "    Entraîne un modèle et retourne les métriques train/test avec détection d'overfitting.\n",
    "\n",
    "    Args:\n",
    "        model: Le modèle à entraîner\n",
    "        X_train, X_test: Features d'entraînement et de test\n",
    "        y_train, y_test: Labels d'entraînement et de test\n",
    "        model_name: Nom du modèle (string)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (results, report_test, cm, curves, pipeline)\n",
    "    \"\"\"\n",
    "    # Poids\n",
    "    weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "    # Création d'un pipeline avec standardisation puis modèle\n",
    "    tt_pipeline = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Entraînement avec les poids\n",
    "    tt_pipeline.fit(X_train, y_train, model__sample_weight=weights)\n",
    "\n",
    "    # Prédictions sur TRAIN\n",
    "    y_pred_train = tt_pipeline.predict(X_train)\n",
    "    class_report_train = classification_report(y_train, y_pred_train, output_dict=True, zero_division=0)\n",
    "\n",
    "    # Prédictions sur TEST\n",
    "    y_pred_test = tt_pipeline.predict(X_test)\n",
    "    class_report_test = classification_report(y_test, y_pred_test, output_dict=True, zero_division=0)\n",
    "\n",
    "    # Matrice de confusion (test)\n",
    "    tt_cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "    # Calcul des écarts (train - test) pour détecter l'overfitting\n",
    "    accuracy_gap = class_report_train['accuracy'] - class_report_test['accuracy']\n",
    "    f1_gap = class_report_train['macro avg']['f1-score'] - class_report_test['macro avg']['f1-score']\n",
    "\n",
    "    # Indicateur d'overfitting\n",
    "    overfitting_flag = 'OUI' if (accuracy_gap >= overfit_threshold or f1_gap >= overfit_threshold) else 'NON'\n",
    "\n",
    "    # Extraction des métriques principales\n",
    "    tt_results = {\n",
    "        'model': model_name,\n",
    "        'method': 'train_test',\n",
    "        # Métriques TRAIN\n",
    "        'train_accuracy': round(class_report_train['accuracy'], 2),\n",
    "        'train_f1_macro': round(class_report_train['macro avg']['f1-score'], 2),\n",
    "        'train_precision_macro': round(class_report_train['macro avg']['precision'], 2),\n",
    "        'train_recall_macro': round(class_report_train['macro avg']['recall'], 2),\n",
    "        # Métriques TEST\n",
    "        'test_accuracy': round(class_report_test['accuracy'], 2),\n",
    "        'test_f1_macro': round(class_report_test['macro avg']['f1-score'], 2),\n",
    "        'test_precision_macro': round(class_report_test['macro avg']['precision'], 2),\n",
    "        'test_recall_macro': round(class_report_test['macro avg']['recall'], 2),\n",
    "        # Écarts et overfitting\n",
    "        'accuracy_gap': round(accuracy_gap, 2),\n",
    "        'f1_gap': round(f1_gap, 2),\n",
    "        'overfitting': overfitting_flag,\n",
    "        'confusion_matrix': str(tt_cm.tolist()),\n",
    "    }\n",
    "\n",
    "    # Récupérer le modèle entraîné du pipeline\n",
    "    trained_model = tt_pipeline.named_steps['model']\n",
    "    X_test_scaled = tt_pipeline.named_steps['scaler'].transform(X_test)\n",
    "\n",
    "    # Vérifier si le modèle supporte predict_proba\n",
    "    try:\n",
    "        y_pred_proba = trained_model.predict_proba(X_test_scaled)\n",
    "    except AttributeError:\n",
    "        y_pred_proba = None\n",
    "\n",
    "    # Données pour les courbes ROC et PR\n",
    "    tt_curves = {\n",
    "        'model': model_name,\n",
    "        'y_true': y_test.copy() if hasattr(y_test, 'copy') else list(y_test),\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'y_pred': y_pred_test\n",
    "    }\n",
    "\n",
    "    return tt_results, class_report_test, tt_cm, tt_curves, tt_pipeline"
   ],
   "id": "8bc9c5190647db7e",
   "outputs": [],
   "execution_count": 255
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### <font color=\"orange\">Validation Croisée</font>\n",
   "id": "463b3aab5c8b7c6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T10:13:33.485489Z",
     "start_time": "2025-11-03T10:13:33.482623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cross_validation_evaluation(model, X, y, model_name, cv=5, overfit_threshold=0.12):\n",
    "    \"\"\"\n",
    "    Effectue une validation croisée et retourne les métriques avec détection d'overfitting.\n",
    "\n",
    "    Args:\n",
    "        model: Le modèle à évaluer\n",
    "        X: Features complètes\n",
    "        y: Labels complets\n",
    "        model_name: Nom du modèle (string)\n",
    "        cv: Nombre de folds (défaut: 5)\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionnaire contenant les résultats moyens avec scores train et test\n",
    "    \"\"\"\n",
    "\n",
    "    # Création du pipeline avec scaling\n",
    "    pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Définition des métriques à calculer\n",
    "    scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "\n",
    "    # Validation croisée avec return_train_score=True pour détecter overfitting\n",
    "    # Par défaut, cross_validate utilisera KFold, qui ne conserve pas la proportion de classes.\n",
    "    # results_cross_validate = cross_validate(pipeline, X, y, cv=cv, scoring=scoring, return_train_score=True, error_score='raise')\n",
    "\n",
    "    cv_strategy = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "    results_cross_validate = cross_validate(\n",
    "        pipeline, X, y, cv=cv_strategy, scoring=scoring,\n",
    "        return_train_score=True, error_score='raise'\n",
    "    )\n",
    "\n",
    "    # Calcul des écarts moyens (train - test)\n",
    "    accuracy_gap_cv = results_cross_validate['train_accuracy'].mean() - results_cross_validate['test_accuracy'].mean()\n",
    "    f1_gap_cv = results_cross_validate['train_f1_macro'].mean() - results_cross_validate['test_f1_macro'].mean()\n",
    "\n",
    "    # Indicateur d'overfitting\n",
    "    overfitting_flag_cv = 'OUI' if (accuracy_gap_cv >= overfit_threshold or f1_gap_cv >= overfit_threshold) else 'NON'\n",
    "\n",
    "    # Calcul des moyennes et écarts-types\n",
    "    cv_results = {\n",
    "        'model': model_name,\n",
    "        'method': 'cross_validation',\n",
    "        # Métriques TRAIN\n",
    "        'train_accuracy': round(results_cross_validate['train_accuracy'].mean(), 2),\n",
    "        'train_accuracy_std': round(results_cross_validate['train_accuracy'].std(), 2),\n",
    "        'train_f1_macro': round(results_cross_validate['train_f1_macro'].mean(), 2),\n",
    "        'train_f1_macro_std': round(results_cross_validate['train_f1_macro'].std(), 2),\n",
    "        'train_precision_macro': round(results_cross_validate['train_precision_macro'].mean(), 2),\n",
    "        'train_recall_macro': round(results_cross_validate['train_recall_macro'].mean(), 2),\n",
    "        # Métriques TEST\n",
    "        'test_accuracy': round(results_cross_validate['test_accuracy'].mean(), 2),\n",
    "        'test_accuracy_std': round(results_cross_validate['test_accuracy'].std(), 2),\n",
    "        'test_f1_macro': round(results_cross_validate['test_f1_macro'].mean(), 2),\n",
    "        'test_f1_macro_std': round(results_cross_validate['test_f1_macro'].std(), 2),\n",
    "        'test_precision_macro': round(results_cross_validate['test_precision_macro'].mean(), 2),\n",
    "        'test_recall_macro': round(results_cross_validate['test_recall_macro'].mean(), 2),\n",
    "        # Écarts et overfitting\n",
    "        'accuracy_gap': round(accuracy_gap_cv, 2),\n",
    "        'f1_gap': round(f1_gap_cv, 2),\n",
    "        'overfitting': overfitting_flag_cv,\n",
    "        'confusion_matrix': 'N/A',\n",
    "    }\n",
    "\n",
    "    print(f\"{model_name:<20} | F1 Test: {cv_results['test_f1_macro']:.2f} | \"\n",
    "      f\"Accuracy: {cv_results['test_accuracy']:.2f} | Overfit: {cv_results['overfitting']}\")\n",
    "\n",
    "    return cv_results"
   ],
   "id": "4790789f5d83882",
   "outputs": [],
   "execution_count": 256
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### <font color=\"orange\">Permutation Importance</font>",
   "id": "d7556aabeb402f89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T10:13:33.492924Z",
     "start_time": "2025-11-03T10:13:33.487476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_permutation_importance(pipeline, X_test, y_test, model_name, n_repeats=10):\n",
    "    \"\"\"\n",
    "    Calcule la permutation importance pour un modèle entraîné.\n",
    "\n",
    "    Args:\n",
    "        pipeline: Pipeline entraîné contenant scaler et modèle\n",
    "        X_test: Features de test\n",
    "        y_test: Labels de test\n",
    "        model_name: Nom du modèle\n",
    "        n_repeats: Nombre de répétitions pour la permutation (défaut: 10)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame avec les importances par feature\n",
    "    \"\"\"\n",
    "    trained_model = pipeline.named_steps['model']\n",
    "    X_test_scaled = pipeline.named_steps['scaler'].transform(X_test)\n",
    "\n",
    "    # Permutation importance sur les données scalées\n",
    "    pi = permutation_importance(\n",
    "        trained_model,\n",
    "        X_test_scaled,\n",
    "        y_test,\n",
    "        n_repeats=n_repeats,\n",
    "        random_state=82\n",
    "    )\n",
    "\n",
    "    # Construction du DataFrame d'importances\n",
    "    df_compute_perm_imp = pd.DataFrame({\n",
    "        'model': model_name,\n",
    "        'feature': X_test.columns,\n",
    "        'importance_mean': pi.importances_mean,\n",
    "        'importance_std': pi.importances_std\n",
    "    })\n",
    "\n",
    "    # Ajouter les feature importances natives si disponibles\n",
    "    trained_model = pipeline.named_steps['model']\n",
    "    if hasattr(trained_model, 'feature_importances_'):\n",
    "        df_compute_perm_imp['feature_importances'] = trained_model.feature_importances_\n",
    "\n",
    "    return df_compute_perm_imp"
   ],
   "id": "41e5b07e4919c86",
   "outputs": [],
   "execution_count": 257
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### <font color=\"orange\">SHAP</font>\n",
    "TreeExplainer"
   ],
   "id": "daa95bc4c5a85ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T10:13:33.496399Z",
     "start_time": "2025-11-03T10:13:33.494546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_shap_values(pipeline, X_test, model_name):\n",
    "    \"\"\"\n",
    "    Calcule les valeurs SHAP pour un modèle tree-based.\n",
    "\n",
    "    Args:\n",
    "        pipeline: Pipeline entraîné contenant scaler et modèle\n",
    "        X_test: Features de test\n",
    "        model_name: Nom du modèle\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionnaire contenant les valeurs SHAP et métadonnées\n",
    "              ou None si le modèle n'est pas compatible\n",
    "    \"\"\"\n",
    "    trained_model = pipeline.named_steps['model']\n",
    "    X_test_scaled = pipeline.named_steps['scaler'].transform(X_test)\n",
    "\n",
    "    # SHAP uniquement pour modèles tree-based\n",
    "    tree_based_models = (XGBClassifier, CatBoostClassifier)\n",
    "\n",
    "    if not isinstance(trained_model, tree_based_models):\n",
    "        print(f\"⚠ Modèle ignoré (non compatible avec TreeExplainer): {type(trained_model).__name__}\")\n",
    "        return None\n",
    "\n",
    "    # Calcul des valeurs SHAP\n",
    "    explainer = shap.TreeExplainer(trained_model)\n",
    "    shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "    # Récupérer les noms de features\n",
    "    feature_names_in = (\n",
    "        trained_model.feature_names_in_\n",
    "        if hasattr(trained_model, 'feature_names_in_')\n",
    "        else X_test.columns.tolist()\n",
    "    )\n",
    "\n",
    "    # Créer un DataFrame avec les bonnes colonnes pour le plot\n",
    "    X_test_scaled_df = pd.DataFrame(\n",
    "        X_test_scaled,\n",
    "        columns=feature_names_in,\n",
    "        index=X_test.index\n",
    "    )\n",
    "\n",
    "    # Métadonnées SHAP\n",
    "    shap_metadata = {\n",
    "        'model': model_name,\n",
    "        'shap_values': shap_values,\n",
    "        'X_test_scaled': X_test_scaled_df,  # DataFrame avec noms de colonnes\n",
    "        'feature_names': feature_names_in,\n",
    "        'explainer': explainer\n",
    "    }\n",
    "\n",
    "    return shap_metadata"
   ],
   "id": "79bec1a6be418aac",
   "outputs": [],
   "execution_count": 258
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### <font color=\"orange\">SHAP</font>\n",
    "plots"
   ],
   "id": "16c8af0aa5e9d91f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T10:13:33.498649Z",
     "start_time": "2025-11-03T10:13:33.497662Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1751dd02670d5c92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Split et Entrainement Modeles Classification",
   "id": "51298c00b833fd17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T10:13:35.137933Z",
     "start_time": "2025-11-03T10:13:33.500243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Définition des modèles\n",
    "models = {\n",
    "    'DummyClassifier': DummyClassifier(\n",
    "        strategy='stratified',\n",
    "        random_state=random_state),\n",
    "\n",
    "    'LogisticRegression': LogisticRegression(\n",
    "        C=1,\n",
    "        max_iter=5000,\n",
    "        penalty='l2',\n",
    "        solver='saga',\n",
    "        random_state=random_state),\n",
    "\n",
    "    'XGBClassifier': XGBClassifier(\n",
    "        n_estimators=100,       # Nombre d’arbres construits par le modèle.\n",
    "        max_depth=5,            # Profondeur maximale de chaque arbre.\n",
    "        learning_rate=0.1,      # Taux d’apprentissage\n",
    "        subsample=0.8,          # 80% des données par arbre\n",
    "        reg_lambda=5,           # Régularisation L2 (pénalité sur les poids trop grands).\n",
    "        eval_metric='logloss',  # Fonction de perte utilisée pour mesurer l’erreur pendant l’entraînement.\n",
    "        scale_pos_weight=5,     # Permet de compenser un déséquilibre entre les classes.\n",
    "        random_state=random_state),\n",
    "\n",
    "    'CatBoostClassifier': CatBoostClassifier(\n",
    "        iterations=100,\n",
    "        depth=5,\n",
    "        learning_rate=0.1,\n",
    "        l2_leaf_reg=5,        # Régularisation\n",
    "        verbose=False,\n",
    "        random_state=random_state)\n",
    "}\n",
    "\n",
    "results_prov = []\n",
    "results_global = []\n",
    "class_report = []\n",
    "all_curves = []\n",
    "all_compute_perm_imp = []\n",
    "all_shap_metadata = []\n",
    "all_importances_metadata = []\n",
    "waterfall_data = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\">>> Évaluation de {model_name}...\")\n",
    "\n",
    "    # Entraînement\n",
    "    tt_results, class_report_test, tt_cm, tt_curves, tt_pipeline = train_model(model, X_train, X_test, y_train, y_test, model_name)\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_results = cross_validation_evaluation(model, X, y, model_name, cv=5)\n",
    "\n",
    "    # Permutation importance\n",
    "    compute_perm_imp = compute_permutation_importance(tt_pipeline, X_test, y_test, model_name)\n",
    "\n",
    "    # SHAP\n",
    "    shap_metadata = compute_shap_values(tt_pipeline, X_test, model_name)\n",
    "\n",
    "    # Formater les métadonnées pour garder la compatibilité\n",
    "    importances_metadata = {\n",
    "        'model': model_name,\n",
    "        'X_test': X_test,\n",
    "        #'X_test_scaled': tt_pipeline.named_steps['scaler'].transform(X_test),\n",
    "        'feature_names_in': X_test.columns.tolist(),\n",
    "    }\n",
    "\n",
    "    class_report.append({\n",
    "        'model': model_name,\n",
    "        'method': 'train_test',\n",
    "        'report': class_report_test,\n",
    "        'confusion_matrix': tt_cm\n",
    "    })\n",
    "\n",
    "    # Sauvegarde des resultats\n",
    "    results_prov.append(tt_results)\n",
    "    results_prov.append(cv_results)\n",
    "\n",
    "    # Conversion en DataFrame\n",
    "    results_global = pd.DataFrame(results_prov)\n",
    "    # Réorganisation des colonnes pour la lisibilité\n",
    "    cols_order = ['model', 'method','train_accuracy', 'test_accuracy', 'accuracy_gap',\n",
    "                  'train_f1_macro', 'test_f1_macro', 'f1_gap','overfitting',\n",
    "                  'train_precision_macro', 'test_precision_macro','train_recall_macro', 'test_recall_macro']\n",
    "    # Ajout des colonnes std si elles existent\n",
    "    std_cols = [col for col in results_global.columns if '_std' in col]\n",
    "    cols_order.extend(std_cols)\n",
    "    cols_order.append('confusion_matrix')\n",
    "    # Colonnes présentes dans le DataFrame\n",
    "    cols_order = [col for col in cols_order if col in results_global.columns]\n",
    "    results_global = results_global[cols_order]\n",
    "\n",
    "    all_curves.append(tt_curves)\n",
    "    all_compute_perm_imp.append(compute_perm_imp)\n",
    "    all_shap_metadata.append(shap_metadata)\n",
    "    all_importances_metadata.append(importances_metadata)\n"
   ],
   "id": "859f5f84b56a52f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Évaluation de DummyClassifier...\n",
      "DummyClassifier      | F1 Test: 0.48 | Accuracy: 0.71 | Overfit: NON\n",
      "⚠ Modèle ignoré (non compatible avec TreeExplainer): DummyClassifier\n",
      ">>> Évaluation de LogisticRegression...\n",
      "LogisticRegression   | F1 Test: 0.69 | Accuracy: 0.87 | Overfit: NON\n",
      "⚠ Modèle ignoré (non compatible avec TreeExplainer): LogisticRegression\n",
      ">>> Évaluation de XGBClassifier...\n",
      "XGBClassifier        | F1 Test: 0.70 | Accuracy: 0.83 | Overfit: OUI\n",
      ">>> Évaluation de CatBoostClassifier...\n",
      "CatBoostClassifier   | F1 Test: 0.68 | Accuracy: 0.87 | Overfit: OUI\n"
     ]
    }
   ],
   "execution_count": 259
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sauvegardes des résultats",
   "id": "dbd24b41eccabea5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T10:13:35.143425Z",
     "start_time": "2025-11-03T10:13:35.140086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fichier principal avec toutes les métriques\n",
    "results_global.to_csv(f'exports/classification_summary.csv', index=False)\n",
    "print(f\"\\n✓ Résumé sauvegardé: classification_summary.csv\")\n",
    "\n",
    "# Fichier avec les rapports détaillés de classification\n",
    "detailed_data = []\n",
    "for item in class_report:\n",
    "    model = item['model']\n",
    "    report = item['report']\n",
    "\n",
    "    # Extraction des métriques par classe\n",
    "    for class_label, metrics in report.items():\n",
    "        if class_label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "            detailed_data.append({\n",
    "                'model': model,\n",
    "                'class': class_label,\n",
    "                'precision': round(metrics['precision'], 2),\n",
    "                'recall': round(metrics['recall'], 2),\n",
    "                'f1-score': round(metrics['f1-score'], 2),\n",
    "                'support': metrics['support']\n",
    "            })\n",
    "\n",
    "detailed_df = pd.DataFrame(detailed_data)\n",
    "detailed_df.to_csv(f'exports/classification_by_class.csv', index=False)\n",
    "print(f\"✓ Résultats par classe sauvegardés: classification_by_class.csv\")"
   ],
   "id": "18a27c4a993fb702",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Résumé sauvegardé: classification_summary.csv\n",
      "✓ Résultats par classe sauvegardés: classification_by_class.csv\n"
     ]
    }
   ],
   "execution_count": 260
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Génération des graphiques",
   "id": "571e2ce2d2b6910d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T10:13:35.146799Z",
     "start_time": "2025-11-03T10:13:35.144748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _plot_binary_roc_curves(all_curves):\n",
    "    \"\"\"\n",
    "    Courbes ROC pour classification binaire.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    for data in all_curves:\n",
    "        model_name = data['model']\n",
    "        y_true = data['y_true']\n",
    "\n",
    "        # Vérifier si les probabilités sont disponibles\n",
    "        if 'y_pred_proba' in data and data['y_pred_proba'] is not None:\n",
    "            y_score = data['y_pred_proba']\n",
    "            # Si format (n_samples, n_classes), prendre la colonne de la classe positive\n",
    "            if len(y_score.shape) > 1:\n",
    "                y_score = y_score[:, 1]\n",
    "        else:\n",
    "            print(f\"⚠️  Pas de probabilités pour {model_name}, utilisation des prédictions\")\n",
    "            y_score = data['y_pred']\n",
    "\n",
    "        # Calculer la courbe ROC\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "    # Ligne diagonale (classificateur aléatoire)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Aléatoire (AUC = 0.500)')\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Taux de Faux Positifs', fontsize=12)\n",
    "    plt.ylabel('Taux de Vrais Positifs', fontsize=12)\n",
    "    plt.title('Courbes ROC - Comparaison des Modèles', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\", fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    filename = f'images/classification_roc_curves.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✓ Courbes ROC sauvegardées: classification_roc_curves.png\")"
   ],
   "id": "119fbb7d716c5a55",
   "outputs": [],
   "execution_count": 261
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T10:13:35.150067Z",
     "start_time": "2025-11-03T10:13:35.148082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _plot_binary_pr_curves(all_curves):\n",
    "    \"\"\"\n",
    "    Courbes Précision-Rappel pour classification binaire.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    for data in all_curves:\n",
    "        model_name = data['model']\n",
    "        y_true = data['y_true']\n",
    "\n",
    "        if 'y_pred_proba' in data and data['y_pred_proba'] is not None:\n",
    "            y_score = data['y_pred_proba']\n",
    "            if len(y_score.shape) > 1:\n",
    "                y_score = y_score[:, 1]\n",
    "        else:\n",
    "            print(f\"⚠️  Pas de probabilités pour {model_name}, utilisation des prédictions\")\n",
    "            y_score = data['y_pred']\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "        avg_precision = average_precision_score(y_true, y_score)\n",
    "\n",
    "        plt.plot(recall, precision, lw=2,\n",
    "                 label=f'{model_name} (AP = {avg_precision:.3f})')\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Rappel', fontsize=12)\n",
    "    plt.ylabel('Précision', fontsize=12)\n",
    "    plt.title('Courbes Précision-Rappel - Comparaison des Modèles',\n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower left\", fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    filename = f'images/classification_precision_recall_curves.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✓ Courbes Précision-Rappel sauvegardées: classification_precision_recall_curves.png\")"
   ],
   "id": "eccf354fea5a1ae8",
   "outputs": [],
   "execution_count": 262
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T10:13:35.153479Z",
     "start_time": "2025-11-03T10:13:35.151505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _plot_permutation_importance(all_compute_perm_imp):\n",
    "    \"\"\"\n",
    "    Plot de la permutation importance pour chaque modèle (un graphique par modèle).\n",
    "    \"\"\"\n",
    "    # Définir une palette de couleurs pour différencier les modèles\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#96CEB4', '#FFEAA7']\n",
    "\n",
    "    # Filtrage des DataFrames valides\n",
    "    df_perm_imp = [df for df in all_compute_perm_imp\n",
    "                   if not df['importance_mean'].isna().all() and (df['importance_mean'] != 0).any()]\n",
    "\n",
    "    # Vérifier qu'il reste des DataFrames après le filtrage\n",
    "    if not df_perm_imp:\n",
    "        print(\"Aucun DataFrame valide à tracer.\")\n",
    "        return\n",
    "\n",
    "    # Créer un graphique pour chaque modèle\n",
    "    for idx, df in enumerate(df_perm_imp):\n",
    "        # Trier par importance décroissante\n",
    "        df_sorted = df.sort_values('importance_mean', ascending=True)\n",
    "\n",
    "        # Récupérer le nom du modèle\n",
    "        model_name = df['model'].iloc[0]\n",
    "\n",
    "        # Créer la figure\n",
    "        plt.figure(figsize=(10, max(6, len(df_sorted) * 0.3)))\n",
    "\n",
    "        # Créer le barplot horizontal\n",
    "        plt.barh(df_sorted['feature'],\n",
    "                 df_sorted['importance_mean'],\n",
    "                 color=colors[idx % len(colors)],\n",
    "                 alpha=0.8)\n",
    "\n",
    "        # Personnalisation\n",
    "        plt.xlabel('Importance Mean', fontsize=12)\n",
    "        plt.ylabel('Features', fontsize=12)\n",
    "        plt.title(f'Permutation Importance - {model_name}',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Sauvegarder avec un nom de fichier spécifique au modèle\n",
    "        filename = f'images/classification_permutation_importance_{model_name.replace(\" \", \"_\")}.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"✓ Graphique sauvegardé: {filename}\")"
   ],
   "id": "3ccb92fabc03f787",
   "outputs": [],
   "execution_count": 263
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T10:13:35.156895Z",
     "start_time": "2025-11-03T10:13:35.154683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _plot_native_feature_importance(all_compute_perm_imp):\n",
    "    \"\"\"\n",
    "    Plot de la feature importance native pour les modèles tree-based.\n",
    "    Génère un graphique séparé pour chaque modèle.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filtrer uniquement les modèles qui ont des feature_importances\n",
    "    valid_data = [\n",
    "        df for df in all_compute_perm_imp\n",
    "        if 'feature_importances' in df.columns and df['feature_importances'].notna().any()\n",
    "    ]\n",
    "\n",
    "    if not valid_data:\n",
    "        print(f\"⚠ Aucun modèle avec feature_importances natives trouvé. Graphique ignoré.\")\n",
    "        return\n",
    "\n",
    "    # Palette de couleurs pour différencier les modèles\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#96CEB4', '#FFEAA7']\n",
    "\n",
    "    # Créer un graphique pour chaque modèle\n",
    "    for idx, df in enumerate(valid_data):\n",
    "        # Normalisation des importances (somme = 1)\n",
    "        df = df.copy()  # Éviter de modifier l'original\n",
    "        total = df['feature_importances'].sum()\n",
    "        if total != 0:\n",
    "            df['feature_importances'] = df['feature_importances'] / total\n",
    "\n",
    "        # Trier par importance croissante\n",
    "        df_sorted = df.sort_values('feature_importances', ascending=True)\n",
    "\n",
    "        # Récupérer le nom du modèle\n",
    "        model_name = df['model'].iloc[0]\n",
    "\n",
    "        # Créer la figure\n",
    "        plt.figure(figsize=(10, max(6, len(df_sorted) * 0.3)))\n",
    "\n",
    "        # Créer le barplot horizontal\n",
    "        plt.barh(\n",
    "            df_sorted['feature'],\n",
    "            df_sorted['feature_importances'],\n",
    "            color=colors[idx % len(colors)],\n",
    "            alpha=0.8\n",
    "        )\n",
    "\n",
    "        # Personnalisation\n",
    "        plt.xlabel('Feature Importance (Normalized)', fontsize=12)\n",
    "        plt.ylabel('Features', fontsize=12)\n",
    "        plt.title(f'Native Feature Importance - {model_name}',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Sauvegarder avec un nom de fichier spécifique au modèle\n",
    "        filename = f'images/classification_native_feature_importance_{model_name.replace(\" \", \"_\")}.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"✓ Graphique sauvegardé: {filename}\")"
   ],
   "id": "d1cb14f74eb8fd1c",
   "outputs": [],
   "execution_count": 264
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T10:13:35.160786Z",
     "start_time": "2025-11-03T10:13:35.158243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _plot_shap(all_shap_metadata):\n",
    "    \"\"\"\n",
    "    Plot des valeurs SHAP pour les modèles tree-based.\n",
    "\n",
    "    Args:\n",
    "        all_shap_metadata: Liste de dictionnaires contenant les métadonnées SHAP\n",
    "    \"\"\"\n",
    "    for meta in all_shap_metadata:\n",
    "        # Skip si le modèle n'est pas compatible\n",
    "        if meta is None:\n",
    "            continue\n",
    "\n",
    "        model_name = meta['model']\n",
    "        shap_values = meta['shap_values']\n",
    "        X_test_scaled = meta['X_test_scaled']\n",
    "\n",
    "        # Gérer le cas multiclasse vs binaire\n",
    "        if isinstance(shap_values, list):\n",
    "            # Multiclasse : on plot pour chaque classe\n",
    "            for class_idx, shap_vals_class in enumerate(shap_values):\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                shap.summary_plot(\n",
    "                    shap_vals_class,\n",
    "                    X_test_scaled,\n",
    "                    show=False,\n",
    "                    plot_type=\"dot\"\n",
    "                )\n",
    "                plt.title(\n",
    "                    f'SHAP Summary Plot - {model_name} (Classe {class_idx})',\n",
    "                    fontsize=14,\n",
    "                    fontweight='bold'\n",
    "                )\n",
    "                plt.tight_layout()\n",
    "\n",
    "                filename = f'images/classification_shap_summary_{model_name}_class{class_idx}.png'\n",
    "                plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                print(f\"✓ SHAP summary plot sauvegardé: {filename}\")\n",
    "\n",
    "            # Plot global (toutes classes confondues)\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            shap.summary_plot(\n",
    "                shap_values,\n",
    "                X_test_scaled,\n",
    "                show=False,\n",
    "                plot_type=\"dot\"\n",
    "            )\n",
    "            plt.title(\n",
    "                f'SHAP Summary Plot - {model_name} (Toutes classes)',\n",
    "                fontsize=14,\n",
    "                fontweight='bold'\n",
    "            )\n",
    "            plt.tight_layout()\n",
    "\n",
    "            filename = f'images/classification_shap_summary_{model_name}_all.png'\n",
    "            plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(f\"✓ SHAP summary plot sauvegardé: {filename}\")\n",
    "\n",
    "        else:\n",
    "            # Binaire : un seul plot\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            shap.summary_plot(\n",
    "                shap_values,\n",
    "                X_test_scaled,\n",
    "                show=False,\n",
    "                plot_type=\"dot\"\n",
    "            )\n",
    "            plt.title(\n",
    "                f'SHAP Summary Plot - {model_name}',\n",
    "                fontsize=14,\n",
    "                fontweight='bold'\n",
    "            )\n",
    "            plt.tight_layout()\n",
    "\n",
    "            filename = f'images/classification_shap_summary_{model_name}.png'\n",
    "            plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(f\"✓ SHAP summary plot sauvegardé: {filename}\")"
   ],
   "id": "28c620b9b60f8305",
   "outputs": [],
   "execution_count": 265
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T10:13:36.921482Z",
     "start_time": "2025-11-03T10:13:35.162136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_plot_binary_roc_curves(all_curves)\n",
    "_plot_binary_pr_curves(all_curves)\n",
    "_plot_permutation_importance(all_compute_perm_imp)\n",
    "_plot_native_feature_importance(all_compute_perm_imp) # , all_importances_metadata\n",
    "_plot_shap(all_shap_metadata)"
   ],
   "id": "a1e856f4d0cfa551",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Courbes ROC sauvegardées: classification_roc_curves.png\n",
      "✓ Courbes Précision-Rappel sauvegardées: classification_precision_recall_curves.png\n",
      "✓ Graphique sauvegardé: images/classification_permutation_importance_LogisticRegression.png\n",
      "✓ Graphique sauvegardé: images/classification_permutation_importance_XGBClassifier.png\n",
      "✓ Graphique sauvegardé: images/classification_permutation_importance_CatBoostClassifier.png\n",
      "✓ Graphique sauvegardé: images/classification_native_feature_importance_XGBClassifier.png\n",
      "✓ Graphique sauvegardé: images/classification_native_feature_importance_CatBoostClassifier.png\n",
      "✓ SHAP summary plot sauvegardé: images/classification_shap_summary_XGBClassifier.png\n",
      "✓ SHAP summary plot sauvegardé: images/classification_shap_summary_CatBoostClassifier.png\n"
     ]
    }
   ],
   "execution_count": 266
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Optimisation des Hypers-Parametres",
   "id": "b4ee7706391b8ae9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T10:13:44.722577Z",
     "start_time": "2025-11-03T10:13:36.923309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_param_grids():\n",
    "    \"\"\"Définit les grilles de paramètres pour chaque modèle.\"\"\"\n",
    "    return {\n",
    "        'DummyClassifier': {\n",
    "            'strategy': ['most_frequent', 'stratified', 'uniform']\n",
    "        },\n",
    "        'LogisticRegression': {\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'C': [0.01, 0.1, 1, 10],\n",
    "            'solver': ['liblinear', 'saga'],  # saga supporte l1 et l2\n",
    "            'max_iter': [5000]  # Augmenter si nécessaire\n",
    "        },\n",
    "        'XGBClassifier': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.3],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0],\n",
    "            'reg_alpha': [0, 0.1, 1],\n",
    "            'reg_lambda': [1, 2, 5]\n",
    "        },\n",
    "        'CatBoostClassifier': {\n",
    "            'iterations': [100, 200, 300],\n",
    "            'depth': [4, 6, 8],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'l2_leaf_reg': [1, 3, 5],\n",
    "            'subsample': [0.8, 1.0]\n",
    "        }\n",
    "    }\n",
    "\n",
    "def perform_grid_search(model, param_grid, X_train, y_train, cv=5, scoring='f1_macro'):\n",
    "    \"\"\"Effectue un GridSearchCV pour un modèle donné.\"\"\"\n",
    "    cv_strategy = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv_strategy,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search\n",
    "\n",
    "def compare_models_gridsearch(X, y, test_size, cv=5, scoring='f1_macro'):\n",
    "    \"\"\"Compare tous les modèles avec GridSearchCV.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    # Standardisation pour la régression logistique\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    param_grids = get_param_grids()\n",
    "\n",
    "    base_models = {\n",
    "        'DummyClassifier': (DummyClassifier(random_state=random_state), X_train, X_test),\n",
    "        'LogisticRegression': (LogisticRegression(random_state=random_state, max_iter=5000), X_train_scaled, X_test_scaled),\n",
    "        'XGBClassifier': (XGBClassifier(random_state=random_state, eval_metric='logloss'), X_train, X_test),\n",
    "        'CatBoostClassifier': (CatBoostClassifier(random_state=random_state, verbose=False), X_train, X_test)\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"GridSearch en cours sur {len(base_models)} modèles...\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    for i, (model_name, (base_model, X_tr, X_te)) in enumerate(base_models.items(), 1):\n",
    "        print(f\"[{i}/{len(base_models)}] {model_name}...\", end=' ', flush=True)\n",
    "\n",
    "        grid_search = perform_grid_search(\n",
    "            base_model,\n",
    "            param_grids[model_name],\n",
    "            X_tr,\n",
    "            y_train,\n",
    "            cv=cv,\n",
    "            scoring=scoring\n",
    "        )\n",
    "\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred_test = best_model.predict(X_te)\n",
    "        report_test = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "\n",
    "        result = {\n",
    "            'model': model_name,\n",
    "            'best_cv_score': round(grid_search.best_score_, 4),\n",
    "            'test_accuracy': round(report_test['accuracy'], 4),\n",
    "            'test_f1_macro': round(report_test['macro avg']['f1-score'], 4),\n",
    "            'test_precision': round(report_test['macro avg']['precision'], 4),\n",
    "            'test_recall': round(report_test['macro avg']['recall'], 4),\n",
    "            'best_params': str(grid_search.best_params_)\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "        print(f\"✓ F1: {result['test_f1_macro']:.4f}\")\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Résultats finaux:\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    print(df[['model', 'best_cv_score', 'test_f1_macro', 'test_accuracy']].to_string(index=False))\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Exécution\n",
    "results_df = compare_models_gridsearch(\n",
    "    X, y,\n",
    "    test_size=test_size,\n",
    "    cv=5,\n",
    "    scoring='f1_macro'\n",
    ")\n",
    "\n",
    "# Sauvegarde\n",
    "results_df.to_csv('exports/gridsearch_best_results.csv', index=False)"
   ],
   "id": "ec3aae9bbc969bfb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GridSearch en cours sur 4 modèles...\n",
      "======================================================================\n",
      "\n",
      "[1/4] DummyClassifier... ✓ F1: 0.4831\n",
      "[2/4] LogisticRegression... ✓ F1: 0.7359\n",
      "[3/4] XGBClassifier... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function: <bound method DataIter._next_wrapper of <xgboost.data.SingleBatchInternalIter object at 0x1228b1c40>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 640, in _next_wrapper\n",
      "Exception ignored on calling ctypes callback function: <bound method DataIter._next_wrapper of <xgboost.data.SingleBatchInternalIter object at 0x1430963f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 585, in _next_wrapper\n",
      "    def _next_wrapper(self, this: None) -> int:  # pylint: disable=unused-argument\n",
      "    return self._handle_exception(lambda: int(self.next(input_data)), 0)\n",
      "\n",
      "KeyboardInterrupt: \n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 553, in _handle_exception\n",
      "    return fn()\n",
      "           ^^^^\n",
      "  File \"/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 640, in <lambda>\n",
      "    return self._handle_exception(lambda: int(self.next(input_data)), 0)\n",
      "                                              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/xgboost/data.py\", line 1654, in next\n",
      "    input_data(**self.kwargs)\n",
      "  File \"/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 629, in input_data\n",
      "    self.proxy.set_info(\n",
      "  File \"/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 975, in set_info\n",
      "    self.feature_names = feature_names\n",
      "    ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tico/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 1369, in feature_names\n",
      "    c_feature_names = (ctypes.c_char_p * len(feature_names_bytes))(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[267]\u001B[39m\u001B[32m, line 112\u001B[39m\n\u001B[32m    109\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n\u001B[32m    111\u001B[39m \u001B[38;5;66;03m# Exécution\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m112\u001B[39m results_df = \u001B[43mcompare_models_gridsearch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    113\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    114\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtest_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    115\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcv\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    116\u001B[39m \u001B[43m    \u001B[49m\u001B[43mscoring\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mf1_macro\u001B[39;49m\u001B[33;43m'\u001B[39;49m\n\u001B[32m    117\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m    119\u001B[39m \u001B[38;5;66;03m# Sauvegarde\u001B[39;00m\n\u001B[32m    120\u001B[39m results_df.to_csv(\u001B[33m'\u001B[39m\u001B[33mexports/gridsearch_best_results.csv\u001B[39m\u001B[33m'\u001B[39m, index=\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[267]\u001B[39m\u001B[32m, line 75\u001B[39m, in \u001B[36mcompare_models_gridsearch\u001B[39m\u001B[34m(X, y, test_size, cv, scoring)\u001B[39m\n\u001B[32m     72\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, (model_name, (base_model, X_tr, X_te)) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(base_models.items(), \u001B[32m1\u001B[39m):\n\u001B[32m     73\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m[\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(base_models)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m] \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m...\u001B[39m\u001B[33m\"\u001B[39m, end=\u001B[33m'\u001B[39m\u001B[33m \u001B[39m\u001B[33m'\u001B[39m, flush=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m---> \u001B[39m\u001B[32m75\u001B[39m     grid_search = \u001B[43mperform_grid_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     76\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbase_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     77\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparam_grids\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     78\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX_tr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     79\u001B[39m \u001B[43m        \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     80\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcv\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     81\u001B[39m \u001B[43m        \u001B[49m\u001B[43mscoring\u001B[49m\u001B[43m=\u001B[49m\u001B[43mscoring\u001B[49m\n\u001B[32m     82\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     84\u001B[39m     best_model = grid_search.best_estimator_\n\u001B[32m     85\u001B[39m     y_pred_test = best_model.predict(X_te)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[267]\u001B[39m\u001B[32m, line 44\u001B[39m, in \u001B[36mperform_grid_search\u001B[39m\u001B[34m(model, param_grid, X_train, y_train, cv, scoring)\u001B[39m\n\u001B[32m     33\u001B[39m cv_strategy = StratifiedKFold(n_splits=cv, shuffle=\u001B[38;5;28;01mTrue\u001B[39;00m, random_state=random_state)\n\u001B[32m     35\u001B[39m grid_search = GridSearchCV(\n\u001B[32m     36\u001B[39m     estimator=model,\n\u001B[32m     37\u001B[39m     param_grid=param_grid,\n\u001B[32m   (...)\u001B[39m\u001B[32m     41\u001B[39m     verbose=\u001B[32m0\u001B[39m\n\u001B[32m     42\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m44\u001B[39m \u001B[43mgrid_search\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     45\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m grid_search\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/base.py:1365\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1358\u001B[39m     estimator._validate_params()\n\u001B[32m   1360\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1361\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1362\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1363\u001B[39m     )\n\u001B[32m   1364\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1365\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1051\u001B[39m, in \u001B[36mBaseSearchCV.fit\u001B[39m\u001B[34m(self, X, y, **params)\u001B[39m\n\u001B[32m   1045\u001B[39m     results = \u001B[38;5;28mself\u001B[39m._format_results(\n\u001B[32m   1046\u001B[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[32m   1047\u001B[39m     )\n\u001B[32m   1049\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[32m-> \u001B[39m\u001B[32m1051\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1053\u001B[39m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[32m   1054\u001B[39m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[32m   1055\u001B[39m first_test_score = all_out[\u001B[32m0\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mtest_scores\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1605\u001B[39m, in \u001B[36mGridSearchCV._run_search\u001B[39m\u001B[34m(self, evaluate_candidates)\u001B[39m\n\u001B[32m   1603\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[32m   1604\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1605\u001B[39m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:997\u001B[39m, in \u001B[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[39m\u001B[34m(candidate_params, cv, more_results)\u001B[39m\n\u001B[32m    989\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.verbose > \u001B[32m0\u001B[39m:\n\u001B[32m    990\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[32m    991\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[33m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[33m candidates,\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    992\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m fits\u001B[39m\u001B[33m\"\u001B[39m.format(\n\u001B[32m    993\u001B[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001B[32m    994\u001B[39m         )\n\u001B[32m    995\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m997\u001B[39m out = \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    998\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    999\u001B[39m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1000\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1001\u001B[39m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1002\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1003\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1004\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1005\u001B[39m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1006\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1007\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1008\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1009\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1010\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1011\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplitter\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1012\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1013\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1015\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) < \u001B[32m1\u001B[39m:\n\u001B[32m   1016\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1017\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mNo fits were performed. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1018\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWas the CV iterator empty? \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1019\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWere there no candidates?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1020\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:82\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m     73\u001B[39m warning_filters = warnings.filters\n\u001B[32m     74\u001B[39m iterable_with_config_and_warning_filters = (\n\u001B[32m     75\u001B[39m     (\n\u001B[32m     76\u001B[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001B[32m   (...)\u001B[39m\u001B[32m     80\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[32m     81\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m82\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config_and_warning_filters\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/joblib/parallel.py:2072\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   2066\u001B[39m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[32m   2067\u001B[39m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[32m   2068\u001B[39m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[32m   2069\u001B[39m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[32m   2070\u001B[39m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[32m-> \u001B[39m\u001B[32m2072\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/joblib/parallel.py:1682\u001B[39m, in \u001B[36mParallel._get_outputs\u001B[39m\u001B[34m(self, iterator, pre_dispatch)\u001B[39m\n\u001B[32m   1679\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[32m   1681\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backend.retrieval_context():\n\u001B[32m-> \u001B[39m\u001B[32m1682\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m._retrieve()\n\u001B[32m   1684\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[32m   1685\u001B[39m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[32m   1686\u001B[39m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[32m   1687\u001B[39m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[32m   1688\u001B[39m     \u001B[38;5;28mself\u001B[39m._exception = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GIT/PredictDataBuilding/.venv/lib/python3.12/site-packages/joblib/parallel.py:1800\u001B[39m, in \u001B[36mParallel._retrieve\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1789\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_ordered:\n\u001B[32m   1790\u001B[39m     \u001B[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001B[39;00m\n\u001B[32m   1791\u001B[39m     \u001B[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1795\u001B[39m     \u001B[38;5;66;03m# control only have to be done on the amount of time the next\u001B[39;00m\n\u001B[32m   1796\u001B[39m     \u001B[38;5;66;03m# dispatched job is pending.\u001B[39;00m\n\u001B[32m   1797\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m (nb_jobs == \u001B[32m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m   1798\u001B[39m         \u001B[38;5;28mself\u001B[39m._jobs[\u001B[32m0\u001B[39m].get_status(timeout=\u001B[38;5;28mself\u001B[39m.timeout) == TASK_PENDING\n\u001B[32m   1799\u001B[39m     ):\n\u001B[32m-> \u001B[39m\u001B[32m1800\u001B[39m         \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1801\u001B[39m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m   1803\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m nb_jobs == \u001B[32m0\u001B[39m:\n\u001B[32m   1804\u001B[39m     \u001B[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001B[39;00m\n\u001B[32m   1805\u001B[39m     \u001B[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1811\u001B[39m     \u001B[38;5;66;03m# timeouts before any other dispatched job has completed and\u001B[39;00m\n\u001B[32m   1812\u001B[39m     \u001B[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 267
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
