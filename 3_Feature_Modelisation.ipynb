{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import des modules",
   "id": "1475dfadac83f672"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:58:46.262403Z",
     "start_time": "2025-11-05T10:58:46.257496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Selection\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, KFold, StratifiedKFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import shap\n",
    "\n",
    "#Modèles\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Metriques\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, precision_recall_curve, average_precision_score, roc_curve, auc\n",
    "\n",
    "# Undersampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n"
   ],
   "id": "5a67d0009f86326a",
   "outputs": [],
   "execution_count": 570
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:58:46.281100Z",
     "start_time": "2025-11-05T10:58:46.270872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fc = pd.read_csv('fc_after_feature_engineering.csv')\n",
    "print(fc.info())"
   ],
   "id": "8536898289714cfe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1470 entries, 0 to 1469\n",
      "Data columns (total 51 columns):\n",
      " #   Column                                     Non-Null Count  Dtype  \n",
      "---  ------                                     --------------  -----  \n",
      " 0   a_quitte_l_entreprise                      1470 non-null   int64  \n",
      " 1   age                                        1470 non-null   int64  \n",
      " 2   annees_dans_l_entreprise                   1470 non-null   int64  \n",
      " 3   annees_dans_le_poste_actuel                1470 non-null   int64  \n",
      " 4   annees_depuis_la_derniere_promotion        1470 non-null   int64  \n",
      " 5   annees_experience_totale                   1470 non-null   int64  \n",
      " 6   annes_sous_responsable_actuel              1470 non-null   int64  \n",
      " 7   augmentation_salaire_precedente            1470 non-null   int64  \n",
      " 8   departement_Commercial                     1470 non-null   int64  \n",
      " 9   departement_Consulting                     1470 non-null   int64  \n",
      " 10  departement_Ressources Humaines            1470 non-null   int64  \n",
      " 11  distance_domicile_travail                  1470 non-null   int64  \n",
      " 12  domaine_etude_Autre                        1470 non-null   int64  \n",
      " 13  domaine_etude_Entrepreunariat              1470 non-null   int64  \n",
      " 14  domaine_etude_Infra & Cloud                1470 non-null   int64  \n",
      " 15  domaine_etude_Marketing                    1470 non-null   int64  \n",
      " 16  domaine_etude_Ressources Humaines          1470 non-null   int64  \n",
      " 17  domaine_etude_Transformation Digitale      1470 non-null   int64  \n",
      " 18  frequence_deplacement                      1470 non-null   float64\n",
      " 19  genre                                      1470 non-null   int64  \n",
      " 20  heure_supplementaires                      1470 non-null   int64  \n",
      " 21  id_employee                                1470 non-null   int64  \n",
      " 22  nb_formations_suivies                      1470 non-null   int64  \n",
      " 23  niveau_education                           1470 non-null   int64  \n",
      " 24  niveau_hierarchique_poste                  1470 non-null   int64  \n",
      " 25  nombre_experiences_precedentes             1470 non-null   int64  \n",
      " 26  nombre_participation_pee                   1470 non-null   int64  \n",
      " 27  note_evaluation_precedente                 1470 non-null   int64  \n",
      " 28  poste_Assistant de Direction               1470 non-null   int64  \n",
      " 29  poste_Cadre Commercial                     1470 non-null   int64  \n",
      " 30  poste_Consultant                           1470 non-null   int64  \n",
      " 31  poste_Directeur Technique                  1470 non-null   int64  \n",
      " 32  poste_Manager                              1470 non-null   int64  \n",
      " 33  poste_Representant Commercial              1470 non-null   int64  \n",
      " 34  poste_Ressources Humaines                  1470 non-null   int64  \n",
      " 35  poste_Senior Manager                       1470 non-null   int64  \n",
      " 36  poste_Tech Lead                            1470 non-null   int64  \n",
      " 37  revenu_mensuel                             1470 non-null   int64  \n",
      " 38  satisfaction_employee_environnement        1470 non-null   int64  \n",
      " 39  satisfaction_employee_equilibre_pro_perso  1470 non-null   int64  \n",
      " 40  satisfaction_employee_equipe               1470 non-null   int64  \n",
      " 41  satisfaction_employee_nature_travail       1470 non-null   int64  \n",
      " 42  satisfaction_globale                       1470 non-null   float64\n",
      " 43  score_environnement_travail                1470 non-null   float64\n",
      " 44  score_equilibre_pro_perso                  1470 non-null   float64\n",
      " 45  score_mobilite_externe                     1470 non-null   float64\n",
      " 46  score_mobilite_interne                     1470 non-null   float64\n",
      " 47  score_revenu                               1470 non-null   float64\n",
      " 48  statut_marital_Celibataire                 1470 non-null   int64  \n",
      " 49  statut_marital_Divorce(e)                  1470 non-null   int64  \n",
      " 50  statut_marital_Marie(e)                    1470 non-null   int64  \n",
      "dtypes: float64(7), int64(44)\n",
      "memory usage: 585.8 KB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 571
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Déclaration des Dataframes X et y\n",
    "- Un DataFrame contenant les features => X\n",
    "- Un Pandas Series contenant la colonne cible => y"
   ],
   "id": "af11a0eccb59eebf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:58:46.290772Z",
     "start_time": "2025-11-05T10:58:46.287271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# hommes ou femmes = genre\n",
    "# célibataires = statut_marital\n",
    "# Jeunes = age\n",
    "# Faible Revenus = revenu_mensuel\n",
    "# poste de consultant ou cadre commercial = poste\n",
    "# moins satisfaits = satisfaction_globale\n",
    "#  = score_mobilite_externe\n",
    "#  = score_mobilite_interne\n",
    "#  = nombre_experiences_precedentes\n",
    "# score_equilibre_pro_perso = heures supplémentaires + déplacement professionnels fréquents +  distance domicile/travail plus élevée\n",
    "# heures supplémentaires\n",
    "# déplacement professionnels fréquents\n",
    "# distance domicile/travail plus élevée\n",
    "\n",
    "# Test 1\n",
    "#columns_base = ['heure_supplementaires','genre','revenu_mensuel', 'age', 'distance_domicile_travail', 'augmentation_salaire_precedente','frequence_deplacement', 'nb_formations_suivies', 'niveau_education', 'niveau_hierarchique_poste', 'nombre_experiences_precedentes', 'nombre_participation_pee', 'note_evaluation_precedente']\n",
    "\n",
    "# Test 2\n",
    "#columns_base = ['genre', 'age', 'revenu_mensuel', 'satisfaction_globale', 'score_mobilite_externe', 'score_mobilite_interne', 'nombre_experiences_precedentes', 'score_equilibre_pro_perso']\n",
    "\n",
    "# Test 3\n",
    "# columns_base = ['genre', 'age', 'revenu_mensuel', 'satisfaction_globale', 'score_mobilite_externe', 'score_mobilite_interne', 'nombre_experiences_precedentes', 'heure_supplementaires' ,'frequence_deplacement', 'distance_domicile_travail']\n",
    "\n",
    "# FINAL\n",
    "columns_base = ['heure_supplementaires', 'age', 'distance_domicile_travail', 'frequence_deplacement', 'nb_formations_suivies', 'nombre_participation_pee', 'note_evaluation_precedente', 'satisfaction_globale', 'annees_dans_l_entreprise', 'annees_depuis_la_derniere_promotion', 'score_mobilite_externe', 'score_mobilite_interne', 'statut_marital_Divorce(e)' ]\n",
    "\n",
    "columns_annees = [col for col in fc.columns if col.startswith('annees')]\n",
    "columns_domaine_etude = [col for col in fc.columns if col.startswith('domaine_etude')]\n",
    "columns_poste = [col for col in fc.columns if col.startswith('poste')]\n",
    "columns_statut_marital = [col for col in fc.columns if col.startswith('statut_marital')]\n",
    "columns_satisfaction = [col for col in fc.columns if col.startswith('satisfaction')]\n",
    "\n",
    "# Test 1\n",
    "#all_columns = columns_base + columns_statut_marital + columns_domaine_etude + columns_poste + columns_satisfaction + columns_annees\n",
    "\n",
    "# Test 2 + Test 3\n",
    "# all_columns = columns_base + columns_statut_marital + columns_poste\n",
    "\n",
    "# FINAL\n",
    "all_columns = columns_base  + columns_poste\n",
    "\n",
    "X = fc[all_columns]\n",
    "y = fc['a_quitte_l_entreprise']"
   ],
   "id": "27791c8c368a589d",
   "outputs": [],
   "execution_count": 572
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Split train/test\n",
    "- test_size = 15%\n",
    "- stratify=y"
   ],
   "id": "7a80e9fb09b6656"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:58:46.297792Z",
     "start_time": "2025-11-05T10:58:46.294491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "random_state = 82\n",
    "test_size = 0.15\n",
    "\n",
    "# Le terme stratification désigne une méthode d’échantillonnage qui garantit que la répartition des classes reste proportionnelle entre les jeux d’entraînement et de test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)"
   ],
   "id": "94b88355994fb7b4",
   "outputs": [],
   "execution_count": 573
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Entrainement Modele Classification\n",
   "id": "34e2c433faba1d20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:58:46.304587Z",
     "start_time": "2025-11-05T10:58:46.299783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, X_train, X_test, y_train, y_test, model_name, overfit_threshold=0.12, use_sampling=False, use_weights=True):\n",
    "    \"\"\"\n",
    "    Entraîne un modèle et retourne les métriques train/test avec détection d'overfitting.\n",
    "\n",
    "    Args:\n",
    "        model: Le modèle à entraîner\n",
    "        X_train, X_test: Features d'entraînement et de test\n",
    "        y_train, y_test: Labels d'entraînement et de test\n",
    "        model_name: Nom du modèle (string)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (results, report_test, cm, curves, pipeline)\n",
    "    \"\"\"\n",
    "\n",
    "    # Undersampling AVANT la création du pipeline\n",
    "    if use_sampling:\n",
    "        tt_pipeline = ImbPipeline(steps=[\n",
    "            ('scaler', StandardScaler()),      # Standardiser D'ABORD\n",
    "            ('over', SMOTE(sampling_strategy=0.1, random_state=random_state)),\n",
    "            ('under', RandomUnderSampler(sampling_strategy=0.5, random_state=random_state)),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        tt_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Entraînement avec les poids\n",
    "    if use_weights:\n",
    "        tt_pipeline = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', model)\n",
    "        ])\n",
    "        weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "        tt_pipeline.fit(X_train, y_train, model__sample_weight=weights)\n",
    "\n",
    "\n",
    "    # Prédictions sur TRAIN\n",
    "    y_pred_train = tt_pipeline.predict(X_train)\n",
    "    class_report_train = classification_report(y_train, y_pred_train, output_dict=True, zero_division=0)\n",
    "\n",
    "    # Prédictions sur TEST\n",
    "    y_pred_test = tt_pipeline.predict(X_test)\n",
    "    class_report_test = classification_report(y_test, y_pred_test, output_dict=True, zero_division=0)\n",
    "\n",
    "    # Matrice de confusion (test)\n",
    "    tt_cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "    # Calcul des écarts (train - test) pour détecter l'overfitting\n",
    "    accuracy_gap = class_report_train['accuracy'] - class_report_test['accuracy']\n",
    "    f1_gap = class_report_train['macro avg']['f1-score'] - class_report_test['macro avg']['f1-score']\n",
    "\n",
    "    # Indicateur d'overfitting\n",
    "    overfitting_flag = 'OUI' if (accuracy_gap >= overfit_threshold or f1_gap >= overfit_threshold) else 'NON'\n",
    "\n",
    "    # Extraction des métriques principales\n",
    "    tt_results = {\n",
    "        'model': model_name,\n",
    "        'method': 'train_test',\n",
    "        # Métriques TRAIN\n",
    "        'train_accuracy': round(class_report_train['accuracy'], 2),\n",
    "        'train_f1_macro': round(class_report_train['macro avg']['f1-score'], 2),\n",
    "        'train_precision_macro': round(class_report_train['macro avg']['precision'], 2),\n",
    "        'train_recall_macro': round(class_report_train['macro avg']['recall'], 2),\n",
    "        # Métriques TEST\n",
    "        'test_accuracy': round(class_report_test['accuracy'], 2),\n",
    "        'test_f1_macro': round(class_report_test['macro avg']['f1-score'], 2),\n",
    "        'test_precision_macro': round(class_report_test['macro avg']['precision'], 2),\n",
    "        'test_recall_macro': round(class_report_test['macro avg']['recall'], 2),\n",
    "        # Écarts et overfitting\n",
    "        'accuracy_gap': round(accuracy_gap, 2),\n",
    "        'f1_gap': round(f1_gap, 2),\n",
    "        'overfitting': overfitting_flag,\n",
    "        'confusion_matrix': str(tt_cm.tolist()),\n",
    "    }\n",
    "\n",
    "    print(\">>>>>> Train Test <<<<<<\")\n",
    "    print(f\"> accuracy        : {tt_results['test_accuracy']:.2f}\")\n",
    "    print(f\"> f1              : {tt_results['test_f1_macro']:.2f}\")\n",
    "    print(f\"> precision       : {tt_results['test_precision_macro']:.2f}\")\n",
    "    print(f\"> recall          : {tt_results['test_recall_macro']:.2f}\")\n",
    "    print(f\"> accuracy_gap    : {tt_results['accuracy_gap']:.2f}\")\n",
    "    print(f\"> f1_gap          : {tt_results['f1_gap']:.2f}\")\n",
    "    print(f\"> confusion_matrix: {tt_results['confusion_matrix']}\")\n",
    "\n",
    "    # Récupérer le modèle entraîné du pipeline\n",
    "    trained_model = tt_pipeline.named_steps['model']\n",
    "    X_test_scaled = tt_pipeline.named_steps['scaler'].transform(X_test)\n",
    "    y_true = y_test.copy() if hasattr(y_test, 'copy') else list(y_test)\n",
    "\n",
    "    # Vérifier si le modèle supporte predict_proba\n",
    "    try:\n",
    "        y_pred_proba = trained_model.predict_proba(X_test_scaled)\n",
    "        y_score = y_pred_proba[:, 1]\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        print(f\"> AUC ROC       : {roc_auc:.2f}\")\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "        avg_precision = average_precision_score(y_true, y_score)\n",
    "        print(f\"> AUC PRC       : {avg_precision:.2f}\")\n",
    "\n",
    "    except AttributeError:\n",
    "        y_pred_proba = None\n",
    "\n",
    "    # Données pour les courbes ROC et PR\n",
    "    tt_curves = {\n",
    "        'model': model_name,\n",
    "        'y_true': y_true,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'y_pred': y_pred_test\n",
    "    }\n",
    "\n",
    "    return tt_results, class_report_test, tt_cm, tt_curves, tt_pipeline"
   ],
   "id": "8bc9c5190647db7e",
   "outputs": [],
   "execution_count": 574
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### <font color=\"orange\">Validation Croisée</font>\n",
    "- StratifiedKFold"
   ],
   "id": "463b3aab5c8b7c6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:58:46.310110Z",
     "start_time": "2025-11-05T10:58:46.306663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cross_validation_evaluation(model, X, y, cv=5, overfit_threshold=0.12):\n",
    "    \"\"\"\n",
    "    Effectue une validation croisée et retourne les métriques avec détection d'overfitting.\n",
    "\n",
    "    Args:\n",
    "        model: Le modèle à évaluer\n",
    "        X: Features complètes\n",
    "        y: Labels complets\n",
    "        cv: Nombre de folds (défaut: 5)\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionnaire contenant les résultats moyens avec scores train et test\n",
    "    \"\"\"\n",
    "\n",
    "    # Création du pipeline avec scaling\n",
    "    pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Définition des métriques à calculer\n",
    "    scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "\n",
    "    # Validation croisée avec return_train_score=True pour détecter overfitting\n",
    "    # Par défaut, cross_validate utilisera KFold, qui ne conserve pas la proportion de classes.\n",
    "    # results_cross_validate = cross_validate(pipeline, X, y, cv=cv, scoring=scoring, return_train_score=True, error_score='raise')\n",
    "\n",
    "    cv_strategy = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "    results_cross_validate = cross_validate(\n",
    "        pipeline, X, y, cv=cv_strategy, scoring=scoring,\n",
    "        return_train_score=True, error_score='raise'\n",
    "    )\n",
    "\n",
    "    # Calcul des écarts moyens (train - test)\n",
    "    accuracy_gap_cv = results_cross_validate['train_accuracy'].mean() - results_cross_validate['test_accuracy'].mean()\n",
    "    f1_gap_cv = results_cross_validate['train_f1_macro'].mean() - results_cross_validate['test_f1_macro'].mean()\n",
    "\n",
    "    # Indicateur d'overfitting\n",
    "    overfitting_flag_cv = 'OUI' if (accuracy_gap_cv >= overfit_threshold or f1_gap_cv >= overfit_threshold) else 'NON'\n",
    "\n",
    "    # Calcul des moyennes et écarts-types\n",
    "    cv_results = {\n",
    "        'model': type(model).__name__,\n",
    "        'method': 'cross_validation',\n",
    "        # Métriques TRAIN\n",
    "        'train_accuracy': round(results_cross_validate['train_accuracy'].mean(), 2),\n",
    "        'train_accuracy_std': round(results_cross_validate['train_accuracy'].std(), 2),\n",
    "        'train_f1_macro': round(results_cross_validate['train_f1_macro'].mean(), 2),\n",
    "        'train_f1_macro_std': round(results_cross_validate['train_f1_macro'].std(), 2),\n",
    "        'train_precision_macro': round(results_cross_validate['train_precision_macro'].mean(), 2),\n",
    "        'train_recall_macro': round(results_cross_validate['train_recall_macro'].mean(), 2),\n",
    "        # Métriques TEST\n",
    "        'test_accuracy': round(results_cross_validate['test_accuracy'].mean(), 2),\n",
    "        'test_accuracy_std': round(results_cross_validate['test_accuracy'].std(), 2),\n",
    "        'test_f1_macro': round(results_cross_validate['test_f1_macro'].mean(), 2),\n",
    "        'test_f1_macro_std': round(results_cross_validate['test_f1_macro'].std(), 2),\n",
    "        'test_precision_macro': round(results_cross_validate['test_precision_macro'].mean(), 2),\n",
    "        'test_recall_macro': round(results_cross_validate['test_recall_macro'].mean(), 2),\n",
    "        # Écarts et overfitting\n",
    "        'accuracy_gap': round(accuracy_gap_cv, 2),\n",
    "        'f1_gap': round(f1_gap_cv, 2),\n",
    "        'overfitting': overfitting_flag_cv,\n",
    "        'confusion_matrix': 'N/A',\n",
    "    }\n",
    "\n",
    "    print(\">>>>>> Cross Validate <<<<<<\")\n",
    "    print(f\"> accuracy        : {cv_results['test_accuracy']:.2f}\")\n",
    "    print(f\"> f1              : {cv_results['test_f1_macro']:.2f}\")\n",
    "    print(f\"> precision       : {cv_results['test_precision_macro']:.2f}\")\n",
    "    print(f\"> recall          : {cv_results['test_recall_macro']:.2f}\")\n",
    "    print(f\"> accuracy_gap    : {cv_results['accuracy_gap']:.2f}\")\n",
    "    print(f\"> f1_gap          : {cv_results['f1_gap']:.2f}\")\n",
    "\n",
    "    return cv_results"
   ],
   "id": "4790789f5d83882",
   "outputs": [],
   "execution_count": 575
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### <font color=\"orange\">Permutation Importance</font>",
   "id": "d7556aabeb402f89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:58:46.313676Z",
     "start_time": "2025-11-05T10:58:46.311888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_permutation_importance(pipeline, X_test, y_test, n_repeats=10):\n",
    "    \"\"\"\n",
    "    Calcule la permutation importance pour un modèle entraîné.\n",
    "    Args:\n",
    "        pipeline: Pipeline entraîné contenant scaler et modèle\n",
    "        X_test: Features de test\n",
    "        y_test: Labels de test\n",
    "        n_repeats: Nombre de répétitions pour la permutation (défaut: 10)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame avec les importances par feature\n",
    "    \"\"\"\n",
    "    trained_model = pipeline.named_steps['model']\n",
    "    X_test_scaled = pipeline.named_steps['scaler'].transform(X_test)\n",
    "\n",
    "    # Permutation importance sur les données scalées\n",
    "    pi = permutation_importance(\n",
    "        trained_model,\n",
    "        X_test_scaled,\n",
    "        y_test,\n",
    "        n_repeats=n_repeats,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Construction du DataFrame d'importances\n",
    "    df_compute_perm_imp = pd.DataFrame({\n",
    "        'model': type(trained_model).__name__,\n",
    "        'feature': X_test.columns,\n",
    "        'importance_mean': pi.importances_mean,\n",
    "        'importance_std': pi.importances_std\n",
    "    })\n",
    "\n",
    "    # Ajouter les feature importances natives si disponibles\n",
    "    if hasattr(trained_model, 'feature_importances_'):\n",
    "        df_compute_perm_imp['feature_importances'] = trained_model.feature_importances_\n",
    "\n",
    "    return df_compute_perm_imp"
   ],
   "id": "41e5b07e4919c86",
   "outputs": [],
   "execution_count": 576
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### <font color=\"orange\">SHAP VALUES</font>\n",
    "- Beeswarm Plot\n",
    "- TreeExplainer"
   ],
   "id": "daa95bc4c5a85ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:58:46.317123Z",
     "start_time": "2025-11-05T10:58:46.315210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_shap_values(pipeline, X_test):\n",
    "    \"\"\"\n",
    "    Calcule les valeurs SHAP pour un modèle tree-based.\n",
    "    Args:\n",
    "        pipeline: Pipeline entraîné contenant scaler et modèle\n",
    "        X_test: Features de test\n",
    "    Returns:\n",
    "        dict: Dictionnaire contenant les valeurs SHAP et métadonnées\n",
    "              ou None si le modèle n'est pas compatible\n",
    "    \"\"\"\n",
    "    trained_model = pipeline.named_steps['model']\n",
    "    X_test_scaled = pipeline.named_steps['scaler'].transform(X_test)\n",
    "\n",
    "    # SHAP uniquement pour modèles tree-based\n",
    "    tree_based_models = (XGBClassifier, CatBoostClassifier)\n",
    "\n",
    "    if not isinstance(trained_model, tree_based_models):\n",
    "        print(f\"⚠ Modèle ignoré (non compatible avec TreeExplainer): {type(trained_model).__name__}\")\n",
    "        return None\n",
    "\n",
    "    # Calcul des valeurs SHAP\n",
    "    explainer = shap.TreeExplainer(trained_model)\n",
    "    shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "    # Récupérer les noms de features\n",
    "    feature_names_in = (\n",
    "        trained_model.feature_names_in_\n",
    "        if hasattr(trained_model, 'feature_names_in_')\n",
    "        else X_test.columns.tolist()\n",
    "    )\n",
    "\n",
    "    # Créer un DataFrame avec les bonnes colonnes pour le plot\n",
    "    X_test_scaled_df = pd.DataFrame(\n",
    "        X_test_scaled,\n",
    "        columns=feature_names_in,\n",
    "        index=X_test.index\n",
    "    )\n",
    "\n",
    "    # Métadonnées SHAP\n",
    "    shap_metadata = {\n",
    "        'model': type(trained_model).__name__,\n",
    "        'shap_values': shap_values,\n",
    "        'X_test_scaled': X_test_scaled_df,  # DataFrame avec noms de colonnes\n",
    "        'feature_names': feature_names_in,\n",
    "        'explainer': explainer\n",
    "    }\n",
    "\n",
    "    return shap_metadata"
   ],
   "id": "79bec1a6be418aac",
   "outputs": [],
   "execution_count": 577
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### <font color=\"orange\">SHAP VALUES</font>\n",
    "- Waterfall Plot\n",
    "- Feature Importance locale\n",
    "- Explainer"
   ],
   "id": "16c8af0aa5e9d91f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:58:46.321339Z",
     "start_time": "2025-11-05T10:58:46.318795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_shap_values_local(model, X_test, y_pred):\n",
    "\n",
    "    # Pour la feature importance locale, utilisez le Waterfall Plot\n",
    "    # SHAP uniquement pour modèles tree-based\n",
    "    tree_based_models = (XGBClassifier, CatBoostClassifier)\n",
    "\n",
    "    if not isinstance(model, tree_based_models):\n",
    "        print(f\"⚠ Modèle ignoré (non compatible avec TreeExplainer): {type(model).__name__}\")\n",
    "        return None\n",
    "\n",
    "    # Calcul des valeurs SHAP\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X_test)\n",
    "\n",
    "    #print(shap_values.values.shape)\n",
    "    #y_pred = tt_curves['y_pred']\n",
    "\n",
    "    idx_class_0a = np.where(y_pred == 0)[0][0]  # 1er échantillon de classe 0\n",
    "    idx_class_0b = np.where(y_pred == 0)[0][1]  # 2eme échantillon de classe 0\n",
    "    idx_class_1a = np.where(y_pred == 1)[0][0]  # 1er échantillon de classe 1\n",
    "    idx_class_1b = np.where(y_pred == 1)[0][1]  # 2eme échantillon de classe 1\n",
    "\n",
    "    print(f\"Echantillon 1 classe 0: {idx_class_0a}\")\n",
    "    print(f\"Echantillon 2 classe 0: {idx_class_0b}\")\n",
    "    print(f\"Echantillon 1 classe 1: {idx_class_1a}\")\n",
    "    print(f\"Echantillon 2 classe 1: {idx_class_1b}\")\n",
    "\n",
    "    model_name = type(model).__name__\n",
    "    # Visualiser les valeurs SHAP pour chaque classe\n",
    "    shap.plots.waterfall(shap_values[idx_class_0a], show=False)\n",
    "    plt.savefig(f\"images/classification_waterfall_plot_{model_name}_class0a.png\", bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    shap.plots.waterfall(shap_values[idx_class_0b], show=False)\n",
    "    plt.savefig(f\"images/classification_waterfall_plot_{model_name}_class0b.png\", bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    shap.plots.waterfall(shap_values[idx_class_1a], show=False)\n",
    "    plt.savefig(f\"images/classification_waterfall_plot_{model_name}_class1a.png\", bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    shap.plots.waterfall(shap_values[idx_class_1b], show=False)\n",
    "    plt.savefig(f\"images/classification_waterfall_plot_{model_name}_class1b.png\", bbox_inches='tight', dpi=300)\n",
    "    plt.close()"
   ],
   "id": "7160cffd3ccf2e27",
   "outputs": [],
   "execution_count": 578
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Split et Entrainement Modeles Classification\n",
    "\n",
    "- DummyClassifier\n",
    "- LogisticRegression\n",
    "- XGBClassifier\n",
    "- CatBoostClassifier"
   ],
   "id": "51298c00b833fd17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:58:49.181549Z",
     "start_time": "2025-11-05T10:58:46.323353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Définition des modèles\n",
    "models = {\n",
    "    'DummyClassifier': DummyClassifier(\n",
    "        strategy='stratified',\n",
    "        random_state=random_state),\n",
    "\n",
    "    'LogisticRegression': LogisticRegression(\n",
    "        C=1,\n",
    "        max_iter=5000,\n",
    "        penalty='l1',\n",
    "        solver='liblinear',\n",
    "        random_state=random_state),\n",
    "\n",
    "    'XGBClassifier': XGBClassifier(\n",
    "        n_estimators=200,       # Nombre d’arbres construits par le modèle.\n",
    "        max_depth=3,            # Profondeur maximale de chaque arbre.\n",
    "        learning_rate=0.3,      # Taux d’apprentissage\n",
    "        subsample=0.8,          # 80% des données par arbre\n",
    "        reg_lambda=5,           # Régularisation L2 (pénalité sur les poids trop grands).\n",
    "        reg_alpha=0.1,\n",
    "        eval_metric='logloss',  # Fonction de perte utilisée pour mesurer l’erreur pendant l’entraînement.\n",
    "        scale_pos_weight=5,     # Permet de compenser un déséquilibre entre les classes.\n",
    "        random_state=random_state),\n",
    "\n",
    "    'CatBoostClassifier': CatBoostClassifier(\n",
    "        iterations=100,\n",
    "        depth=6,\n",
    "        learning_rate=0.1,\n",
    "        l2_leaf_reg=3,        # Régularisation\n",
    "        subsample=0.8,\n",
    "        verbose=False,\n",
    "        random_state=random_state)\n",
    "}\n",
    "\n",
    "results_prov = []\n",
    "results_global = []\n",
    "class_report = []\n",
    "all_curves = []\n",
    "all_compute_perm_imp = []\n",
    "all_shap_metadata = []\n",
    "all_importances_metadata = []\n",
    "waterfall_data = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"#################################\")\n",
    "    print(f\"##### {model_name}\")\n",
    "    print(f\"#################################\")\n",
    "\n",
    "    # Entraînement\n",
    "    tt_results, class_report_test, tt_cm, tt_curves, tt_pipeline = train_model(model, X_train, X_test, y_train, y_test, model_name)\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_results = cross_validation_evaluation(model, X, y, cv=5)\n",
    "\n",
    "    # Permutation importance\n",
    "    compute_perm_imp = compute_permutation_importance(tt_pipeline, X_test, y_test)\n",
    "\n",
    "    # SHAP\n",
    "    shap_metadata = compute_shap_values(tt_pipeline, X_test)\n",
    "\n",
    "    # SHAP Feature Importance Local\n",
    "    compute_shap_values_local(model, X_test, y_pred=tt_curves['y_pred'])\n",
    "\n",
    "    # Formater les métadonnées pour garder la compatibilité\n",
    "    importances_metadata = {\n",
    "        'model': model_name,\n",
    "        'X_test': X_test,\n",
    "        #'X_test_scaled': tt_pipeline.named_steps['scaler'].transform(X_test),\n",
    "        'feature_names_in': X_test.columns.tolist(),\n",
    "    }\n",
    "\n",
    "    class_report.append({\n",
    "        'model': model_name,\n",
    "        'method': 'train_test',\n",
    "        'report': class_report_test,\n",
    "        'confusion_matrix': tt_cm\n",
    "    })\n",
    "\n",
    "    # Sauvegarde des resultats\n",
    "    results_prov.append(tt_results)\n",
    "    results_prov.append(cv_results)\n",
    "\n",
    "    # Conversion en DataFrame\n",
    "    results_global = pd.DataFrame(results_prov)\n",
    "    # Réorganisation des colonnes pour la lisibilité\n",
    "    cols_order = ['model', 'method','train_accuracy', 'test_accuracy', 'accuracy_gap',\n",
    "                  'train_f1_macro', 'test_f1_macro', 'f1_gap','overfitting',\n",
    "                  'train_precision_macro', 'test_precision_macro','train_recall_macro', 'test_recall_macro']\n",
    "    # Ajout des colonnes std si elles existent\n",
    "    std_cols = [col for col in results_global.columns if '_std' in col]\n",
    "    cols_order.extend(std_cols)\n",
    "    cols_order.append('confusion_matrix')\n",
    "    # Colonnes présentes dans le DataFrame\n",
    "    cols_order = [col for col in cols_order if col in results_global.columns]\n",
    "    results_global = results_global[cols_order]\n",
    "\n",
    "    all_curves.append(tt_curves)\n",
    "    all_compute_perm_imp.append(compute_perm_imp)\n",
    "    all_shap_metadata.append(shap_metadata)\n",
    "    all_importances_metadata.append(importances_metadata)\n"
   ],
   "id": "859f5f84b56a52f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################\n",
      "##### DummyClassifier\n",
      "#################################\n",
      ">>>>>> Train Test <<<<<<\n",
      "> accuracy        : 0.50\n",
      "> f1              : 0.43\n",
      "> precision       : 0.50\n",
      "> recall          : 0.50\n",
      "> accuracy_gap    : -0.00\n",
      "> f1_gap          : -0.00\n",
      "> confusion_matrix: [[92, 93], [18, 18]]\n",
      "> AUC ROC       : 0.50\n",
      "> AUC PRC       : 0.16\n",
      ">>>>>> Cross Validate <<<<<<\n",
      "> accuracy        : 0.71\n",
      "> f1              : 0.48\n",
      "> precision       : 0.49\n",
      "> recall          : 0.48\n",
      "> accuracy_gap    : 0.03\n",
      "> f1_gap          : 0.02\n",
      "⚠ Modèle ignoré (non compatible avec TreeExplainer): DummyClassifier\n",
      "⚠ Modèle ignoré (non compatible avec TreeExplainer): DummyClassifier\n",
      "#################################\n",
      "##### LogisticRegression\n",
      "#################################\n",
      ">>>>>> Train Test <<<<<<\n",
      "> accuracy        : 0.75\n",
      "> f1              : 0.68\n",
      "> precision       : 0.67\n",
      "> recall          : 0.78\n",
      "> accuracy_gap    : 0.01\n",
      "> f1_gap          : 0.00\n",
      "> confusion_matrix: [[136, 49], [6, 30]]\n",
      "> AUC ROC       : 0.85\n",
      "> AUC PRC       : 0.64\n",
      ">>>>>> Cross Validate <<<<<<\n",
      "> accuracy        : 0.88\n",
      "> f1              : 0.72\n",
      "> precision       : 0.81\n",
      "> recall          : 0.68\n",
      "> accuracy_gap    : 0.01\n",
      "> f1_gap          : 0.01\n",
      "⚠ Modèle ignoré (non compatible avec TreeExplainer): LogisticRegression\n",
      "⚠ Modèle ignoré (non compatible avec TreeExplainer): LogisticRegression\n",
      "#################################\n",
      "##### XGBClassifier\n",
      "#################################\n",
      ">>>>>> Train Test <<<<<<\n",
      "> accuracy        : 0.83\n",
      "> f1              : 0.75\n",
      "> precision       : 0.72\n",
      "> recall          : 0.83\n",
      "> accuracy_gap    : 0.09\n",
      "> f1_gap          : 0.13\n",
      "> confusion_matrix: [[153, 32], [6, 30]]\n",
      "> AUC ROC       : 0.88\n",
      "> AUC PRC       : 0.58\n",
      ">>>>>> Cross Validate <<<<<<\n",
      "> accuracy        : 0.84\n",
      "> f1              : 0.72\n",
      "> precision       : 0.71\n",
      "> recall          : 0.72\n",
      "> accuracy_gap    : 0.14\n",
      "> f1_gap          : 0.25\n",
      "Echantillon 1 classe 0: 0\n",
      "Echantillon 2 classe 0: 1\n",
      "Echantillon 1 classe 1: 5\n",
      "Echantillon 2 classe 1: 8\n",
      "#################################\n",
      "##### CatBoostClassifier\n",
      "#################################\n",
      ">>>>>> Train Test <<<<<<\n",
      "> accuracy        : 0.86\n",
      "> f1              : 0.75\n",
      "> precision       : 0.74\n",
      "> recall          : 0.76\n",
      "> accuracy_gap    : 0.11\n",
      "> f1_gap          : 0.19\n",
      "> confusion_matrix: [[167, 18], [14, 22]]\n",
      "> AUC ROC       : 0.85\n",
      "> AUC PRC       : 0.64\n",
      ">>>>>> Cross Validate <<<<<<\n",
      "> accuracy        : 0.87\n",
      "> f1              : 0.68\n",
      "> precision       : 0.80\n",
      "> recall          : 0.64\n",
      "> accuracy_gap    : 0.09\n",
      "> f1_gap          : 0.24\n",
      "Echantillon 1 classe 0: 0\n",
      "Echantillon 2 classe 0: 1\n",
      "Echantillon 1 classe 1: 4\n",
      "Echantillon 2 classe 1: 5\n"
     ]
    }
   ],
   "execution_count": 579
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sauvegardes des résultats",
   "id": "dbd24b41eccabea5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:58:49.187021Z",
     "start_time": "2025-11-05T10:58:49.184200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fichier principal avec toutes les métriques\n",
    "results_global.to_csv(f'exports/classification_summary.csv', index=False)\n",
    "\n",
    "# Fichier avec les rapports détaillés de classification\n",
    "detailed_data = []\n",
    "for item in class_report:\n",
    "    model = item['model']\n",
    "    report = item['report']\n",
    "\n",
    "    # Extraction des métriques par classe\n",
    "    for class_label, metrics in report.items():\n",
    "        if class_label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "            detailed_data.append({\n",
    "                'model': model,\n",
    "                'class': class_label,\n",
    "                'precision': round(metrics['precision'], 2),\n",
    "                'recall': round(metrics['recall'], 2),\n",
    "                'f1-score': round(metrics['f1-score'], 2),\n",
    "                'support': metrics['support']\n",
    "            })\n",
    "\n",
    "detailed_df = pd.DataFrame(detailed_data)\n",
    "detailed_df.to_csv(f'exports/classification_by_class.csv', index=False)"
   ],
   "id": "18a27c4a993fb702",
   "outputs": [],
   "execution_count": 580
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Génération des graphiques",
   "id": "571e2ce2d2b6910d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:58:49.190918Z",
     "start_time": "2025-11-05T10:58:49.188647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _plot_binary_roc_curves(all_curves):\n",
    "    \"\"\"\n",
    "    Courbes ROC pour classification binaire.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    for data in all_curves:\n",
    "        model_name = data['model']\n",
    "        y_true = data['y_true']\n",
    "\n",
    "        # Vérifier si les probabilités sont disponibles\n",
    "        if 'y_pred_proba' in data and data['y_pred_proba'] is not None:\n",
    "            y_score = data['y_pred_proba']\n",
    "            # Si format (n_samples, n_classes), prendre la colonne de la classe positive\n",
    "            if len(y_score.shape) > 1:\n",
    "                y_score = y_score[:, 1]\n",
    "        else:\n",
    "            print(f\"⚠️  Pas de probabilités pour {model_name}, utilisation des prédictions\")\n",
    "            y_score = data['y_pred']\n",
    "\n",
    "        # Calculer la courbe ROC\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    # Ligne diagonale (classificateur aléatoire)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Aléatoire (AUC = 0.500)')\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Taux de Faux Positifs', fontsize=12)\n",
    "    plt.ylabel('Taux de Vrais Positifs', fontsize=12)\n",
    "    plt.title('Courbes ROC - Comparaison des Modèles', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\", fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    filename = f'images/classification_roc_curves.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ],
   "id": "119fbb7d716c5a55",
   "outputs": [],
   "execution_count": 581
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:58:49.194606Z",
     "start_time": "2025-11-05T10:58:49.192623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _plot_binary_pr_curves(all_curves):\n",
    "    \"\"\"\n",
    "    Courbes Précision-Rappel pour classification binaire.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    for data in all_curves:\n",
    "        model_name = data['model']\n",
    "        y_true = data['y_true']\n",
    "\n",
    "        if 'y_pred_proba' in data and data['y_pred_proba'] is not None:\n",
    "            y_score = data['y_pred_proba']\n",
    "            if len(y_score.shape) > 1:\n",
    "                y_score = y_score[:, 1]\n",
    "        else:\n",
    "            print(f\"⚠️  Pas de probabilités pour {model_name}, utilisation des prédictions\")\n",
    "            y_score = data['y_pred']\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "        avg_precision = average_precision_score(y_true, y_score)\n",
    "\n",
    "        plt.plot(recall, precision, lw=2,\n",
    "                 label=f'{model_name} (AP = {avg_precision:.2f})')\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Rappel', fontsize=12)\n",
    "    plt.ylabel('Précision', fontsize=12)\n",
    "    plt.title('Courbes Précision-Rappel - Comparaison des Modèles',\n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower left\", fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    filename = f'images/classification_precision_recall_curves.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ],
   "id": "eccf354fea5a1ae8",
   "outputs": [],
   "execution_count": 582
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:58:49.198079Z",
     "start_time": "2025-11-05T10:58:49.196063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _plot_permutation_importance(all_compute_perm_imp):\n",
    "    \"\"\"\n",
    "    Plot de la permutation importance pour chaque modèle (un graphique par modèle).\n",
    "    \"\"\"\n",
    "    # Définir une palette de couleurs pour différencier les modèles\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#96CEB4', '#FFEAA7']\n",
    "\n",
    "    # Filtrage des DataFrames valides\n",
    "    df_perm_imp = [df for df in all_compute_perm_imp\n",
    "                   if not df['importance_mean'].isna().all() and (df['importance_mean'] != 0).any()]\n",
    "\n",
    "    # Vérifier qu'il reste des DataFrames après le filtrage\n",
    "    if not df_perm_imp:\n",
    "        print(\"Aucun DataFrame valide à tracer.\")\n",
    "        return\n",
    "\n",
    "    # Créer un graphique pour chaque modèle\n",
    "    for idx, df in enumerate(df_perm_imp):\n",
    "        # Trier par importance décroissante\n",
    "        df_sorted = df.sort_values('importance_mean', ascending=True)\n",
    "\n",
    "        # Récupérer le nom du modèle\n",
    "        model_name = df['model'].iloc[0]\n",
    "\n",
    "        # Créer la figure\n",
    "        plt.figure(figsize=(10, max(6, len(df_sorted) * 0.3)))\n",
    "\n",
    "        # Créer le barplot horizontal\n",
    "        plt.barh(df_sorted['feature'],\n",
    "                 df_sorted['importance_mean'],\n",
    "                 color=colors[idx % len(colors)],\n",
    "                 alpha=0.8)\n",
    "\n",
    "        # Personnalisation\n",
    "        plt.xlabel('Importance Mean', fontsize=12)\n",
    "        plt.ylabel('Features', fontsize=12)\n",
    "        plt.title(f'Permutation Importance - {model_name}',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Sauvegarder avec un nom de fichier spécifique au modèle\n",
    "        filename = f'images/classification_permutation_importance_{model_name.replace(\" \", \"_\")}.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()"
   ],
   "id": "3ccb92fabc03f787",
   "outputs": [],
   "execution_count": 583
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:58:49.201685Z",
     "start_time": "2025-11-05T10:58:49.199556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _plot_native_feature_importance(all_compute_perm_imp):\n",
    "    \"\"\"\n",
    "    Plot de la feature importance native pour les modèles tree-based.\n",
    "    Génère un graphique séparé pour chaque modèle.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filtrer uniquement les modèles qui ont des feature_importances\n",
    "    valid_data = [\n",
    "        df for df in all_compute_perm_imp\n",
    "        if 'feature_importances' in df.columns and df['feature_importances'].notna().any()\n",
    "    ]\n",
    "\n",
    "    if not valid_data:\n",
    "        print(f\"⚠ Aucun modèle avec feature_importances natives trouvé. Graphique ignoré.\")\n",
    "        return\n",
    "\n",
    "    # Palette de couleurs pour différencier les modèles\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#96CEB4', '#FFEAA7']\n",
    "\n",
    "    # Créer un graphique pour chaque modèle\n",
    "    for idx, df in enumerate(valid_data):\n",
    "        # Normalisation des importances (somme = 1)\n",
    "        df = df.copy()  # Éviter de modifier l'original\n",
    "        total = df['feature_importances'].sum()\n",
    "        if total != 0:\n",
    "            df['feature_importances'] = df['feature_importances'] / total\n",
    "\n",
    "        # Trier par importance croissante\n",
    "        df_sorted = df.sort_values('feature_importances', ascending=True)\n",
    "\n",
    "        # Récupérer le nom du modèle\n",
    "        model_name = df['model'].iloc[0]\n",
    "\n",
    "        # Créer la figure\n",
    "        plt.figure(figsize=(10, max(6, len(df_sorted) * 0.3)))\n",
    "\n",
    "        # Créer le barplot horizontal\n",
    "        plt.barh(\n",
    "            df_sorted['feature'],\n",
    "            df_sorted['feature_importances'],\n",
    "            color=colors[idx % len(colors)],\n",
    "            alpha=0.8\n",
    "        )\n",
    "\n",
    "        # Personnalisation\n",
    "        plt.xlabel('Feature Importance (Normalized)', fontsize=12)\n",
    "        plt.ylabel('Features', fontsize=12)\n",
    "        plt.title(f'Native Feature Importance - {model_name}',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Sauvegarder avec un nom de fichier spécifique au modèle\n",
    "        filename = f'images/classification_native_feature_importance_{model_name.replace(\" \", \"_\")}.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()"
   ],
   "id": "d1cb14f74eb8fd1c",
   "outputs": [],
   "execution_count": 584
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:58:49.205648Z",
     "start_time": "2025-11-05T10:58:49.203303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _plot_shap(all_shap_metadata):\n",
    "    \"\"\"\n",
    "    Plot des valeurs SHAP pour les modèles tree-based.\n",
    "\n",
    "    Args:\n",
    "        all_shap_metadata: Liste de dictionnaires contenant les métadonnées SHAP\n",
    "    \"\"\"\n",
    "    for meta in all_shap_metadata:\n",
    "        # Skip si le modèle n'est pas compatible\n",
    "        if meta is None:\n",
    "            continue\n",
    "\n",
    "        model_name = meta['model']\n",
    "        shap_values = meta['shap_values']\n",
    "        X_test_scaled = meta['X_test_scaled']\n",
    "\n",
    "        # Gérer le cas multiclasse vs binaire\n",
    "        if isinstance(shap_values, list):\n",
    "            # Multiclasse : on plot pour chaque classe\n",
    "            for class_idx, shap_vals_class in enumerate(shap_values):\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                shap.summary_plot(\n",
    "                    shap_vals_class,\n",
    "                    X_test_scaled,\n",
    "                    show=False,\n",
    "                    plot_type=\"dot\"\n",
    "                )\n",
    "                plt.title(\n",
    "                    f'SHAP Summary Plot - {model_name} (Classe {class_idx})',\n",
    "                    fontsize=14,\n",
    "                    fontweight='bold'\n",
    "                )\n",
    "                plt.tight_layout()\n",
    "\n",
    "                filename = f'images/classification_shap_summary_{model_name}_class{class_idx}.png'\n",
    "                plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                print(f\"✓ SHAP summary plot sauvegardé: {filename}\")\n",
    "\n",
    "            # Plot global (toutes classes confondues)\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            shap.summary_plot(\n",
    "                shap_values,\n",
    "                X_test_scaled,\n",
    "                show=False,\n",
    "                plot_type=\"dot\"\n",
    "            )\n",
    "            plt.title(\n",
    "                f'SHAP Summary Plot - {model_name} (Toutes classes)',\n",
    "                fontsize=14,\n",
    "                fontweight='bold'\n",
    "            )\n",
    "            plt.tight_layout()\n",
    "\n",
    "            filename = f'images/classification_shap_summary_{model_name}_all.png'\n",
    "            plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(f\"✓ SHAP summary plot sauvegardé: {filename}\")\n",
    "\n",
    "        else:\n",
    "            # Binaire : un seul plot\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            shap.summary_plot(\n",
    "                shap_values,\n",
    "                X_test_scaled,\n",
    "                show=False,\n",
    "                plot_type=\"dot\"\n",
    "            )\n",
    "            plt.title(\n",
    "                f'SHAP Summary Plot - {model_name}',\n",
    "                fontsize=14,\n",
    "                fontweight='bold'\n",
    "            )\n",
    "            plt.tight_layout()\n",
    "\n",
    "            filename = f'images/classification_shap_summary_{model_name}.png'\n",
    "            plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "            plt.close()"
   ],
   "id": "28c620b9b60f8305",
   "outputs": [],
   "execution_count": 585
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:58:50.705877Z",
     "start_time": "2025-11-05T10:58:49.207094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_plot_binary_roc_curves(all_curves)\n",
    "_plot_binary_pr_curves(all_curves)\n",
    "_plot_permutation_importance(all_compute_perm_imp)\n",
    "_plot_native_feature_importance(all_compute_perm_imp) # , all_importances_metadata\n",
    "_plot_shap(all_shap_metadata)"
   ],
   "id": "a1e856f4d0cfa551",
   "outputs": [],
   "execution_count": 586
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Optimisation des Hypers-Parametres",
   "id": "b4ee7706391b8ae9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:58:50.712872Z",
     "start_time": "2025-11-05T10:58:50.708342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_param_grids():\n",
    "    \"\"\"Définit les grilles de paramètres pour chaque modèle.\"\"\"\n",
    "    return {\n",
    "        'DummyClassifier': {\n",
    "            'strategy': ['most_frequent', 'stratified', 'uniform']\n",
    "        },\n",
    "        'LogisticRegression': {\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'C': [0.01, 0.1, 1, 10],\n",
    "            'solver': ['liblinear', 'saga'],  # saga supporte l1 et l2\n",
    "            'max_iter': [5000]  # Augmenter si nécessaire\n",
    "        },\n",
    "        'XGBClassifier': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.3],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0],\n",
    "            'reg_alpha': [0, 0.1, 1],\n",
    "            'reg_lambda': [1, 2, 5]\n",
    "        },\n",
    "        'CatBoostClassifier': {\n",
    "            'iterations': [100, 200, 300],\n",
    "            'depth': [4, 6, 8],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'l2_leaf_reg': [1, 3, 5],\n",
    "            'subsample': [0.8, 1.0]\n",
    "        }\n",
    "    }\n",
    "\n",
    "def perform_grid_search(model, param_grid, X_train, y_train, cv=5, scoring='f1_macro'):\n",
    "    \"\"\"Effectue un GridSearchCV pour un modèle donné.\"\"\"\n",
    "    cv_strategy = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv_strategy,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search\n",
    "\n",
    "def compare_models_gridsearch(X, y, test_size, cv=5, scoring='f1_macro'):\n",
    "    \"\"\"Compare tous les modèles avec GridSearchCV.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    # Standardisation pour la régression logistique\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    param_grids = get_param_grids()\n",
    "\n",
    "    base_models = {\n",
    "        'DummyClassifier': (DummyClassifier(random_state=random_state), X_train, X_test),\n",
    "        'LogisticRegression': (LogisticRegression(random_state=random_state, max_iter=5000), X_train_scaled, X_test_scaled),\n",
    "        'XGBClassifier': (XGBClassifier(random_state=random_state, eval_metric='logloss'), X_train, X_test),\n",
    "        'CatBoostClassifier': (CatBoostClassifier(random_state=random_state, verbose=False), X_train, X_test)\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"GridSearch en cours sur {len(base_models)} modèles...\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    for i, (model_name, (base_model, X_tr, X_te)) in enumerate(base_models.items(), 1):\n",
    "        print(f\"[{i}/{len(base_models)}] {model_name}...\", end=' ', flush=True)\n",
    "\n",
    "        grid_search = perform_grid_search(\n",
    "            base_model,\n",
    "            param_grids[model_name],\n",
    "            X_tr,\n",
    "            y_train,\n",
    "            cv=cv,\n",
    "            scoring=scoring\n",
    "        )\n",
    "\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred_test = best_model.predict(X_te)\n",
    "        report_test = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "\n",
    "        result = {\n",
    "            'model': model_name,\n",
    "            'best_cv_score': round(grid_search.best_score_, 4),\n",
    "            'test_accuracy': round(report_test['accuracy'], 4),\n",
    "            'test_f1_macro': round(report_test['macro avg']['f1-score'], 4),\n",
    "            'test_precision': round(report_test['macro avg']['precision'], 4),\n",
    "            'test_recall': round(report_test['macro avg']['recall'], 4),\n",
    "            'best_params': str(grid_search.best_params_)\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "        print(f\"✓ F1: {result['test_f1_macro']:.4f}\")\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Résultats finaux:\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    print(df[['model', 'best_cv_score', 'test_f1_macro', 'test_accuracy']].to_string(index=False))\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Exécution\n",
    "#results_df = compare_models_gridsearch(X, y,test_size=test_size,cv=5,scoring='f1_macro')\n",
    "\n",
    "# Sauvegarde\n",
    "#results_df.to_csv('exports/gridsearch_best_results.csv', index=False)"
   ],
   "id": "ec3aae9bbc969bfb",
   "outputs": [],
   "execution_count": 587
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
