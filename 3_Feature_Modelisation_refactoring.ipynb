{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import des modules\n",
   "id": "e99a8f1c4f4bd3b6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-02T13:09:43.357227Z",
     "start_time": "2025-10-02T13:09:43.350290Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Selection\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, KFold\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "#Preprocess\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MultiLabelBinarizer, MinMaxScaler\n",
    "\n",
    "#Modèles\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "#Metriques\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n"
   ],
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T13:09:43.374031Z",
     "start_time": "2025-10-02T13:09:43.361807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fc = pd.read_csv('fc_after_feature_engineering.csv')\n",
    "print(fc.info())"
   ],
   "id": "ae9d3797053beb5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1470 entries, 0 to 1469\n",
      "Data columns (total 40 columns):\n",
      " #   Column                                     Non-Null Count  Dtype  \n",
      "---  ------                                     --------------  -----  \n",
      " 0   a_quitte_l_entreprise                      1470 non-null   bool   \n",
      " 1   age                                        1470 non-null   int64  \n",
      " 2   annees_dans_l_entreprise                   1470 non-null   int64  \n",
      " 3   annees_dans_le_poste_actuel                1470 non-null   int64  \n",
      " 4   annees_depuis_la_derniere_promotion        1470 non-null   int64  \n",
      " 5   annees_experience_totale                   1470 non-null   int64  \n",
      " 6   annes_sous_responsable_actuel              1470 non-null   int64  \n",
      " 7   augmentation_salaire_precedente            1470 non-null   int64  \n",
      " 8   distance_domicile_travail                  1470 non-null   int64  \n",
      " 9   domaine_etude_Entrepreunariat              1470 non-null   float64\n",
      " 10  domaine_etude_Infra & Cloud                1470 non-null   float64\n",
      " 11  domaine_etude_Marketing                    1470 non-null   float64\n",
      " 12  domaine_etude_Ressources Humaines          1470 non-null   float64\n",
      " 13  domaine_etude_Transformation Digitale      1470 non-null   float64\n",
      " 14  frequence_deplacement                      1470 non-null   float64\n",
      " 15  genre                                      1470 non-null   bool   \n",
      " 16  heure_supplementaires                      1470 non-null   bool   \n",
      " 17  id_employee                                1470 non-null   int64  \n",
      " 18  nb_formations_suivies                      1470 non-null   int64  \n",
      " 19  niveau_education                           1470 non-null   int64  \n",
      " 20  niveau_hierarchique_poste                  1470 non-null   int64  \n",
      " 21  nombre_experiences_precedentes             1470 non-null   int64  \n",
      " 22  nombre_participation_pee                   1470 non-null   int64  \n",
      " 23  note_evaluation_actuelle                   1470 non-null   int64  \n",
      " 24  note_evaluation_precedente                 1470 non-null   int64  \n",
      " 25  poste_Cadre Commercial                     1470 non-null   float64\n",
      " 26  poste_Consultant                           1470 non-null   float64\n",
      " 27  poste_Directeur Technique                  1470 non-null   float64\n",
      " 28  poste_Manager                              1470 non-null   float64\n",
      " 29  poste_Representant Commercial              1470 non-null   float64\n",
      " 30  poste_Ressources Humaines                  1470 non-null   float64\n",
      " 31  poste_Senior Manager                       1470 non-null   float64\n",
      " 32  poste_Tech Lead                            1470 non-null   float64\n",
      " 33  revenu_mensuel                             1470 non-null   int64  \n",
      " 34  satisfaction_employee_environnement        1470 non-null   int64  \n",
      " 35  satisfaction_employee_equilibre_pro_perso  1470 non-null   int64  \n",
      " 36  satisfaction_employee_equipe               1470 non-null   int64  \n",
      " 37  satisfaction_employee_nature_travail       1470 non-null   int64  \n",
      " 38  satisfaction_globale                       1470 non-null   float64\n",
      " 39  statut_marital                             1470 non-null   float64\n",
      "dtypes: bool(3), float64(16), int64(21)\n",
      "memory usage: 429.4 KB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataframes\n",
    "- Un DataFrame contenant les features => X\n",
    "- Un Pandas Series contenant la colonne cible => y"
   ],
   "id": "492e0eedd144db3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T13:09:43.385185Z",
     "start_time": "2025-10-02T13:09:43.381983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Un homme , Célibataire , Entre 30 et 40 ans , Consultant , Entre 1 et 7 années dans l’entreprise, Un revenu compris entre 2500€ et 6000€, Domaine d’etude Infra & Cloud, Qui a moins de 5 années sous son responsable actuel, Qui a une distance domicile travail entre 3 et 17km\n",
    "\n",
    "columns_base = ['genre', 'statut_marital', 'age', 'annees_dans_l_entreprise', 'revenu_mensuel', 'distance_domicile_travail', 'satisfaction_globale']\n",
    "\n",
    "columns_domaine_etude = [col for col in fc.columns if col.startswith('domaine_etude')]\n",
    "columns_poste = [col for col in fc.columns if col.startswith('poste')]\n",
    "\n",
    "all_columns = columns_base + columns_domaine_etude + columns_poste\n",
    "\n",
    "X = fc[all_columns]\n",
    "y = fc['a_quitte_l_entreprise']\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.70, random_state=666)"
   ],
   "id": "9f209612188d56f5",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Séparation Train Test\n",
    "- Des métriques d’évaluation calculées pour chaque modèle, sur le jeu d’apprentissage et le jeu de test."
   ],
   "id": "6adf016352b1d3b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T13:09:43.391161Z",
     "start_time": "2025-10-02T13:09:43.387239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_test_evaluation(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Entraîne un modèle et retourne les métriques train/test avec détection d'overfitting.\n",
    "\n",
    "    Args:\n",
    "        model: Le modèle à entraîner\n",
    "        X_train, X_test: Features d'entraînement et de test\n",
    "        y_train, y_test: Labels d'entraînement et de test\n",
    "        model_name: Nom du modèle (string)\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionnaire contenant les résultats avec métriques train et test\n",
    "    \"\"\"\n",
    "    # Entraînement\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Prédictions sur TRAIN\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    report_train = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "\n",
    "    # Prédictions sur TEST\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    report_test = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "\n",
    "    # Matrice de confusion (test)\n",
    "    cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "    # Calcul des écarts (train - test) pour détecter l'overfitting\n",
    "    accuracy_gap = report_train['accuracy'] - report_test['accuracy']\n",
    "    f1_gap = report_train['macro avg']['f1-score'] - report_test['macro avg']['f1-score']\n",
    "\n",
    "    # Indicateur d'overfitting (seuil à 5% d'écart)\n",
    "    overfitting_flag = 'OUI' if (accuracy_gap > 0.05 or f1_gap > 0.05) else 'NON'\n",
    "\n",
    "    # Extraction des métriques principales\n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'method': 'train_test',\n",
    "        # Métriques TRAIN\n",
    "        'train_accuracy': report_train['accuracy'],\n",
    "        'train_f1_macro': report_train['macro avg']['f1-score'],\n",
    "        'train_precision_macro': report_train['macro avg']['precision'],\n",
    "        'train_recall_macro': report_train['macro avg']['recall'],\n",
    "        # Métriques TEST\n",
    "        'test_accuracy': report_test['accuracy'],\n",
    "        'test_f1_macro': report_test['macro avg']['f1-score'],\n",
    "        'test_precision_macro': report_test['macro avg']['precision'],\n",
    "        'test_recall_macro': report_test['macro avg']['recall'],\n",
    "        # Écarts et overfitting\n",
    "        'accuracy_gap': accuracy_gap,\n",
    "        'f1_gap': f1_gap,\n",
    "        'overfitting': overfitting_flag,\n",
    "        'confusion_matrix': str(cm.tolist())\n",
    "    }\n",
    "\n",
    "    return results, report_test, cm"
   ],
   "id": "ab9ae595cb3d2815",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Validation croisée\n",
    "- cross_validate"
   ],
   "id": "6f174397daf9030d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T13:09:43.396498Z",
     "start_time": "2025-10-02T13:09:43.393230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cross_validation_evaluation(model, X, y, model_name, cv=5):\n",
    "    \"\"\"\n",
    "    Effectue une validation croisée et retourne les métriques avec détection d'overfitting.\n",
    "\n",
    "    Args:\n",
    "        model: Le modèle à évaluer\n",
    "        X: Features complètes\n",
    "        y: Labels complets\n",
    "        model_name: Nom du modèle (string)\n",
    "        cv: Nombre de folds (défaut: 5)\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionnaire contenant les résultats moyens avec scores train et test\n",
    "    \"\"\"\n",
    "    # Définition des métriques à calculer\n",
    "    scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "\n",
    "    # Validation croisée avec return_train_score=True pour détecter overfitting\n",
    "    cv_results = cross_validate(model, X, y, cv=cv, scoring=scoring,\n",
    "                                return_train_score=True)\n",
    "\n",
    "    # Calcul des écarts moyens (train - test)\n",
    "    accuracy_gap_cv = cv_results['train_accuracy'].mean() - cv_results['test_accuracy'].mean()\n",
    "    f1_gap_cv = cv_results['train_f1_macro'].mean() - cv_results['test_f1_macro'].mean()\n",
    "\n",
    "    # Indicateur d'overfitting\n",
    "    overfitting_flag_cv = 'OUI' if (accuracy_gap_cv > 0.05 or f1_gap_cv > 0.05) else 'NON'\n",
    "\n",
    "    # Calcul des moyennes et écarts-types\n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'method': 'cross_validation',\n",
    "        # Métriques TRAIN\n",
    "        'train_accuracy': cv_results['train_accuracy'].mean(),\n",
    "        'train_accuracy_std': cv_results['train_accuracy'].std(),\n",
    "        'train_f1_macro': cv_results['train_f1_macro'].mean(),\n",
    "        'train_f1_macro_std': cv_results['train_f1_macro'].std(),\n",
    "        'train_precision_macro': cv_results['train_precision_macro'].mean(),\n",
    "        'train_recall_macro': cv_results['train_recall_macro'].mean(),\n",
    "        # Métriques TEST\n",
    "        'test_accuracy': cv_results['test_accuracy'].mean(),\n",
    "        'test_accuracy_std': cv_results['test_accuracy'].std(),\n",
    "        'test_f1_macro': cv_results['test_f1_macro'].mean(),\n",
    "        'test_f1_macro_std': cv_results['test_f1_macro'].std(),\n",
    "        'test_precision_macro': cv_results['test_precision_macro'].mean(),\n",
    "        'test_recall_macro': cv_results['test_recall_macro'].mean(),\n",
    "        # Écarts et overfitting\n",
    "        'accuracy_gap': accuracy_gap_cv,\n",
    "        'f1_gap': f1_gap_cv,\n",
    "        'overfitting': overfitting_flag_cv,\n",
    "        'confusion_matrix': 'N/A'\n",
    "    }\n",
    "\n",
    "    return results"
   ],
   "id": "b1443a7a9f5add6b",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Comparaison des modeles\n",
    "\n",
    "- DummyClassifier : strategy='most_frequent'\n",
    "- RandomForestClassifier : n_estimators=100,max_depth=10\n",
    "- XGBClassifier : eval_metric='logloss'\n",
    "- CatBoostClassifier"
   ],
   "id": "18cf6638fe6c883b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T13:09:43.402942Z",
     "start_time": "2025-10-02T13:09:43.398637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compare_models(X, y, test_size=0.2, random_state=666, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Compare tous les modèles et génère les fichiers CSV de résultats.\n",
    "\n",
    "    Args:\n",
    "        X: Features (DataFrame ou array)\n",
    "        y: Labels (Series ou array)\n",
    "        test_size: Proportion du jeu de test (défaut: 0.2)\n",
    "        random_state: Seed pour la reproductibilité (défaut: 42)\n",
    "        cv_folds: Nombre de folds pour la validation croisée (défaut: 5)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (résultats_df, rapports_détaillés)\n",
    "    \"\"\"\n",
    "    # Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    # Définition des modèles\n",
    "    models = {\n",
    "        'DummyClassifier': DummyClassifier(\n",
    "            strategy='most_frequent',\n",
    "            random_state=random_state),\n",
    "\n",
    "        'RandomForestClassifier': RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            random_state=random_state),\n",
    "\n",
    "        'XGBClassifier': XGBClassifier(\n",
    "            max_depth=6,\n",
    "            learning_rate=0.01,     # Plus petit = apprentissage plus lent\n",
    "            subsample=0.8,          # 80% des données par arbre\n",
    "            colsample_bytree=0.8,   # 80% des features par arbre\n",
    "            reg_alpha=0.1,          # Régularisation L1\n",
    "            reg_lambda=1.0,         # Régularisation L2\n",
    "            min_child_weight=5,\n",
    "            eval_metric='logloss',\n",
    "            random_state=random_state),\n",
    "\n",
    "        'CatBoostClassifier': CatBoostClassifier(\n",
    "            depth=6,\n",
    "            learning_rate=0.03,\n",
    "            l2_leaf_reg=3.0,        # Régularisation\n",
    "            subsample=0.8,\n",
    "            verbose=False,\n",
    "            random_state=random_state)\n",
    "    }\n",
    "\n",
    "    all_results = []\n",
    "    detailed_reports = []\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"COMPARAISON DES MODÈLES DE CLASSIFICATION\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n>>> Évaluation de {model_name}...\")\n",
    "\n",
    "        # Train/Test\n",
    "        print(f\"  - Train/Test split...\")\n",
    "        tt_results, report, cm = train_test_evaluation(\n",
    "            model, X_train, X_test, y_train, y_test, model_name\n",
    "        )\n",
    "        all_results.append(tt_results)\n",
    "\n",
    "        # Sauvegarde du rapport détaillé\n",
    "        detailed_reports.append({\n",
    "            'model': model_name,\n",
    "            'method': 'train_test',\n",
    "            'report': report,\n",
    "            'confusion_matrix': cm\n",
    "        })\n",
    "\n",
    "        # Cross-validation\n",
    "        print(f\"  - Validation croisée ({cv_folds} folds)...\")\n",
    "        cv_results = cross_validation_evaluation(\n",
    "            model, X, y, model_name, cv=cv_folds\n",
    "        )\n",
    "        all_results.append(cv_results)\n",
    "\n",
    "        print(f\"  ✓ {model_name} terminé\")\n",
    "\n",
    "    # Conversion en DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "\n",
    "    # Réorganisation des colonnes pour la lisibilité\n",
    "    cols_order = ['model', 'method',\n",
    "                  'train_accuracy', 'test_accuracy', 'accuracy_gap',\n",
    "                  'train_f1_macro', 'test_f1_macro', 'f1_gap',\n",
    "                  'overfitting',\n",
    "                  'train_precision_macro', 'test_precision_macro',\n",
    "                  'train_recall_macro', 'test_recall_macro']\n",
    "\n",
    "    # Ajout des colonnes std si elles existent\n",
    "    std_cols = [col for col in results_df.columns if '_std' in col]\n",
    "    cols_order.extend(std_cols)\n",
    "    cols_order.append('confusion_matrix')\n",
    "\n",
    "    # Colonnes présentes dans le DataFrame\n",
    "    cols_order = [col for col in cols_order if col in results_df.columns]\n",
    "    results_df = results_df[cols_order]\n",
    "\n",
    "    return results_df, detailed_reports"
   ],
   "id": "42b712baf50c2528",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sauvegarde des résultats\n",
    "- classification_results_by_class.csv => Rapport de classification par classes\n",
    "- classification_results_confusion_matrices.csv => Matrices de confusion\n",
    "- classification_results_summary.csv => Tous les scores\n",
    "- classification_results_overfitting_analysis.csv => Analyse spécifique de l'overfitting"
   ],
   "id": "731e5a3a58b0b674"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T13:09:43.408874Z",
     "start_time": "2025-10-02T13:09:43.404915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_results(results_df, detailed_reports, output_prefix='classification_results'):\n",
    "    \"\"\"\n",
    "    Sauvegarde les résultats dans des fichiers CSV.\n",
    "\n",
    "    Args:\n",
    "        results_df: DataFrame avec tous les résultats\n",
    "        detailed_reports: Liste des rapports détaillés\n",
    "        output_prefix: Préfixe pour les fichiers de sortie\n",
    "    \"\"\"\n",
    "    # 1. Fichier principal avec toutes les métriques\n",
    "    results_df.to_csv(f'{output_prefix}_summary.csv', index=False)\n",
    "    print(f\"\\n✓ Résumé sauvegardé: {output_prefix}_summary.csv\")\n",
    "\n",
    "    # 2. Fichier avec les rapports détaillés de classification\n",
    "    detailed_data = []\n",
    "    for item in detailed_reports:\n",
    "        model = item['model']\n",
    "        report = item['report']\n",
    "\n",
    "        # Extraction des métriques par classe\n",
    "        for class_label, metrics in report.items():\n",
    "            if class_label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "                detailed_data.append({\n",
    "                    'model': model,\n",
    "                    'class': class_label,\n",
    "                    'precision': metrics['precision'],\n",
    "                    'recall': metrics['recall'],\n",
    "                    'f1-score': metrics['f1-score'],\n",
    "                    'support': metrics['support']\n",
    "                })\n",
    "\n",
    "    detailed_df = pd.DataFrame(detailed_data)\n",
    "    detailed_df.to_csv(f'{output_prefix}_by_class.csv', index=False)\n",
    "    print(f\"✓ Résultats par classe sauvegardés: {output_prefix}_by_class.csv\")\n",
    "\n",
    "    # 3. Fichier avec les matrices de confusion\n",
    "    cm_data = []\n",
    "    for item in detailed_reports:\n",
    "        cm_data.append({\n",
    "            'model': item['model'],\n",
    "            'confusion_matrix': str(item['confusion_matrix'].tolist())\n",
    "        })\n",
    "\n",
    "    cm_df = pd.DataFrame(cm_data)\n",
    "    cm_df.to_csv(f'{output_prefix}_confusion_matrices.csv', index=False)\n",
    "    print(f\"✓ Matrices de confusion sauvegardées: {output_prefix}_confusion_matrices.csv\")\n",
    "\n",
    "    # 4. Fichier spécifique pour l'analyse d'overfitting\n",
    "    overfitting_data = results_df[['model', 'method', 'train_accuracy', 'test_accuracy',\n",
    "                                     'accuracy_gap', 'train_f1_macro', 'test_f1_macro',\n",
    "                                     'f1_gap', 'overfitting']].copy()\n",
    "    overfitting_data.to_csv(f'{output_prefix}_overfitting_analysis.csv', index=False)\n",
    "    print(f\"✓ Analyse d'overfitting sauvegardée: {output_prefix}_overfitting_analysis.csv\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TOUS LES RÉSULTATS ONT ÉTÉ SAUVEGARDÉS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Affichage d'un résumé de l'overfitting\n",
    "    print(\"\\n📊 RÉSUMÉ DE L'OVERFITTING:\")\n",
    "    print(\"-\" * 70)\n",
    "    for _, row in overfitting_data.iterrows():\n",
    "        status = \"⚠️  OVERFITTING DÉTECTÉ\" if row['overfitting'] == 'OUI' else \"✓  Pas d'overfitting\"\n",
    "        print(f\"{row['model']:25s} ({row['method']:17s}): {status}\")\n",
    "        print(f\"  → Écart accuracy: {row['accuracy_gap']:+.4f} | Écart F1: {row['f1_gap']:+.4f}\")\n",
    "    print(\"-\" * 70)"
   ],
   "id": "48f53b78c9c4a2f2",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Comparaison des modèles",
   "id": "48c7fbcb6d31f141"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T13:09:47.786303Z",
     "start_time": "2025-10-02T13:09:43.412622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Comparaison des modèles\n",
    "results_df, detailed_reports = compare_models(X, y, test_size=0.15, cv_folds=5)\n",
    "\n",
    "# Sauvegarde des résultats\n",
    "save_results(results_df, detailed_reports, output_prefix='classification_results')\n"
   ],
   "id": "61685b8de3eb957f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPARAISON DES MODÈLES DE CLASSIFICATION\n",
      "======================================================================\n",
      "\n",
      ">>> Évaluation de DummyClassifier...\n",
      "  - Train/Test split...\n",
      "  - Validation croisée (5 folds)...\n",
      "  ✓ DummyClassifier terminé\n",
      "\n",
      ">>> Évaluation de RandomForestClassifier...\n",
      "  - Train/Test split...\n",
      "  - Validation croisée (5 folds)...\n",
      "  ✓ RandomForestClassifier terminé\n",
      "\n",
      ">>> Évaluation de XGBClassifier...\n",
      "  - Train/Test split...\n",
      "  - Validation croisée (5 folds)...\n",
      "  ✓ XGBClassifier terminé\n",
      "\n",
      ">>> Évaluation de CatBoostClassifier...\n",
      "  - Train/Test split...\n",
      "  - Validation croisée (5 folds)...\n",
      "  ✓ CatBoostClassifier terminé\n",
      "\n",
      "✓ Résumé sauvegardé: classification_results_summary.csv\n",
      "✓ Résultats par classe sauvegardés: classification_results_by_class.csv\n",
      "✓ Matrices de confusion sauvegardées: classification_results_confusion_matrices.csv\n",
      "✓ Analyse d'overfitting sauvegardée: classification_results_overfitting_analysis.csv\n",
      "\n",
      "======================================================================\n",
      "TOUS LES RÉSULTATS ONT ÉTÉ SAUVEGARDÉS\n",
      "======================================================================\n",
      "\n",
      "📊 RÉSUMÉ DE L'OVERFITTING:\n",
      "----------------------------------------------------------------------\n",
      "DummyClassifier           (train_test       ): ✓  Pas d'overfitting\n",
      "  → Écart accuracy: +0.0020 | Écart F1: +0.0006\n",
      "DummyClassifier           (cross_validation ): ✓  Pas d'overfitting\n",
      "  → Écart accuracy: +0.0000 | Écart F1: +0.0000\n",
      "RandomForestClassifier    (train_test       ): ⚠️  OVERFITTING DÉTECTÉ\n",
      "  → Écart accuracy: +0.0543 | Écart F1: +0.1999\n",
      "RandomForestClassifier    (cross_validation ): ⚠️  OVERFITTING DÉTECTÉ\n",
      "  → Écart accuracy: +0.0656 | Écart F1: +0.2231\n",
      "XGBClassifier             (train_test       ): ✓  Pas d'overfitting\n",
      "  → Écart accuracy: +0.0028 | Écart F1: +0.0057\n",
      "XGBClassifier             (cross_validation ): ✓  Pas d'overfitting\n",
      "  → Écart accuracy: +0.0002 | Écart F1: +0.0022\n",
      "CatBoostClassifier        (train_test       ): ⚠️  OVERFITTING DÉTECTÉ\n",
      "  → Écart accuracy: +0.1437 | Écart F1: +0.3552\n",
      "CatBoostClassifier        (cross_validation ): ⚠️  OVERFITTING DÉTECTÉ\n",
      "  → Écart accuracy: +0.1585 | Écart F1: +0.3852\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Modele DUMMY\n",
    "- DummyClassifier"
   ],
   "id": "650ed83dcec2372b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " # Modele LINEAIRE",
   "id": "f70bdcb11e6d8d8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T13:09:47.790205Z",
     "start_time": "2025-10-02T13:09:47.788825Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "29a0efd29823ea6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Modele NON LINEAIRE\n",
    "- RandomForest, XGBoost ou CatBoost\n",
    "- Métriques d’évaluation en classification : matrice de confusion, rappel et précision.\n",
    "- Scores (présence d’overfit ou non, capacité d’éviter les faux positifs ou faux négatifs)"
   ],
   "id": "cdbcdda4b8fc0779"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Amélioration de la classification\n",
    "- demandez-vous si éviter des faux positifs est plus important qu’éviter des faux négatifs."
   ],
   "id": "fcd52f7d66ce3752"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T13:09:47.794498Z",
     "start_time": "2025-10-02T13:09:47.793379Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6a0ef8be5672fd39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# OPTIMISATION DES HYPER-PARAMETRES",
   "id": "c7724358cbe70863"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T13:09:51.365622Z",
     "start_time": "2025-10-02T13:09:47.798204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datetime import time\n",
    "\n",
    "\n",
    "def get_param_grids():\n",
    "    \"\"\"\n",
    "    Définit les grilles de paramètres pour chaque modèle.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionnaire avec les grilles de paramètres pour chaque modèle\n",
    "    \"\"\"\n",
    "    param_grids = {\n",
    "        'DummyClassifier': {\n",
    "            'strategy': ['most_frequent', 'stratified', 'uniform']\n",
    "        },\n",
    "\n",
    "        'RandomForestClassifier': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 15, None],\n",
    "            'min_samples_split': [2, 10, 20],\n",
    "            'min_samples_leaf': [1, 5, 10],\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        },\n",
    "\n",
    "        'XGBClassifier': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.3],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0],\n",
    "            'reg_alpha': [0, 0.1, 1],\n",
    "            'reg_lambda': [1, 2, 5]\n",
    "        },\n",
    "\n",
    "        'CatBoostClassifier': {\n",
    "            'iterations': [50, 100, 200],\n",
    "            'depth': [4, 6, 8],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'l2_leaf_reg': [1, 3, 5],\n",
    "            'subsample': [0.8, 1.0]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return param_grids\n",
    "\n",
    "\n",
    "def get_small_param_grids():\n",
    "    \"\"\"\n",
    "    Grilles réduites pour des tests rapides.\n",
    "\n",
    "    Returns:\n",
    "        dict: Grilles de paramètres réduites\n",
    "    \"\"\"\n",
    "    param_grids = {\n",
    "        'DummyClassifier': {\n",
    "            'strategy': ['most_frequent', 'stratified']\n",
    "        },\n",
    "\n",
    "        'RandomForestClassifier': {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [5, 10, None],\n",
    "            'min_samples_split': [2, 10],\n",
    "            'max_features': ['sqrt']\n",
    "        },\n",
    "\n",
    "        'XGBClassifier': {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [3, 5],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'reg_lambda': [1, 2]\n",
    "        },\n",
    "\n",
    "        'CatBoostClassifier': {\n",
    "            'iterations': [50, 100],\n",
    "            'depth': [4, 6],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'l2_leaf_reg': [1, 3]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return param_grids\n",
    "\n",
    "\n",
    "def perform_grid_search(model, param_grid, X_train, y_train, model_name,\n",
    "                       cv=5, scoring='f1_macro', n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Effectue un GridSearchCV pour un modèle donné.\n",
    "\n",
    "    Args:\n",
    "        model: Le modèle à optimiser\n",
    "        param_grid: Grille de paramètres\n",
    "        X_train, y_train: Données d'entraînement\n",
    "        model_name: Nom du modèle\n",
    "        cv: Nombre de folds pour la validation croisée\n",
    "        scoring: Métrique d'optimisation\n",
    "        n_jobs: Nombre de processus parallèles\n",
    "\n",
    "    Returns:\n",
    "        tuple: (GridSearchCV object, résultats dict)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"🔍 Grid Search pour {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Nombre de combinaisons à tester: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "    print(f\"Métrique d'optimisation: {scoring}\")\n",
    "    print(f\"Validation croisée: {cv} folds\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Configuration de la validation croisée stratifiée\n",
    "    cv_strategy = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "    # GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv_strategy,\n",
    "        scoring=scoring,\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=0,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Entraînement\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Résultats\n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_params': str(grid_search.best_params_),\n",
    "        'n_combinations': len(grid_search.cv_results_['params']),\n",
    "        'time_seconds': elapsed_time\n",
    "    }\n",
    "\n",
    "    print(f\"✓ Terminé en {elapsed_time:.2f} secondes\")\n",
    "    print(f\"📊 Meilleur score ({scoring}): {grid_search.best_score_:.4f}\")\n",
    "    print(f\"🏆 Meilleurs paramètres:\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"   - {param}: {value}\")\n",
    "\n",
    "    return grid_search, results\n",
    "\n",
    "\n",
    "def evaluate_best_model(grid_search, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Évalue le meilleur modèle trouvé par GridSearch sur le jeu de test.\n",
    "\n",
    "    Args:\n",
    "        grid_search: Objet GridSearchCV entraîné\n",
    "        X_train, X_test: Features\n",
    "        y_train, y_test: Labels\n",
    "        model_name: Nom du modèle\n",
    "\n",
    "    Returns:\n",
    "        dict: Résultats d'évaluation\n",
    "    \"\"\"\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Prédictions\n",
    "    y_pred_train = best_model.predict(X_train)\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "    # Rapports\n",
    "    report_train = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "    report_test = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "\n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "    # Calcul des écarts (overfitting)\n",
    "    accuracy_gap = report_train['accuracy'] - report_test['accuracy']\n",
    "    f1_gap = report_train['macro avg']['f1-score'] - report_test['macro avg']['f1-score']\n",
    "    overfitting = 'OUI' if (accuracy_gap > 0.05 or f1_gap > 0.05) else 'NON'\n",
    "\n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'train_accuracy': report_train['accuracy'],\n",
    "        'test_accuracy': report_test['accuracy'],\n",
    "        'train_f1_macro': report_train['macro avg']['f1-score'],\n",
    "        'test_f1_macro': report_test['macro avg']['f1-score'],\n",
    "        'train_precision': report_train['macro avg']['precision'],\n",
    "        'test_precision': report_test['macro avg']['precision'],\n",
    "        'train_recall': report_train['macro avg']['recall'],\n",
    "        'test_recall': report_test['macro avg']['recall'],\n",
    "        'accuracy_gap': accuracy_gap,\n",
    "        'f1_gap': f1_gap,\n",
    "        'overfitting': overfitting,\n",
    "        'confusion_matrix': str(cm.tolist())\n",
    "    }\n",
    "\n",
    "    print(f\"\\n📈 Évaluation sur le jeu de test:\")\n",
    "    print(f\"   Train accuracy: {report_train['accuracy']:.4f}\")\n",
    "    print(f\"   Test accuracy:  {report_test['accuracy']:.4f}\")\n",
    "    print(f\"   Écart accuracy: {accuracy_gap:+.4f}\")\n",
    "    print(f\"   Overfitting:    {overfitting}\")\n",
    "\n",
    "    return results, report_test, cm\n",
    "\n",
    "\n",
    "def compare_models_gridsearch(X, y, param_grids='full', test_size=0.2,\n",
    "                              cv=5, scoring='f1_macro', random_state=42):\n",
    "    \"\"\"\n",
    "    Compare tous les modèles avec GridSearchCV.\n",
    "\n",
    "    Args:\n",
    "        X: Features\n",
    "        y: Labels\n",
    "        param_grids: 'full', 'small', ou dict personnalisé\n",
    "        test_size: Proportion du jeu de test\n",
    "        cv: Nombre de folds\n",
    "        scoring: Métrique d'optimisation\n",
    "        random_state: Seed\n",
    "\n",
    "    Returns:\n",
    "        tuple: (résultats_grid, résultats_eval, grid_objects)\n",
    "    \"\"\"\n",
    "    # Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"GRIDSEARCH - COMPARAISON DES MODÈLES\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Taille du dataset: {len(X)} échantillons\")\n",
    "    print(f\"Train: {len(X_train)} | Test: {len(X_test)}\")\n",
    "    print(f\"Distribution des classes: {dict(pd.Series(y).value_counts())}\")\n",
    "\n",
    "    # Sélection de la grille\n",
    "    if param_grids == 'full':\n",
    "        grids = get_param_grids()\n",
    "    elif param_grids == 'small':\n",
    "        grids = get_small_param_grids()\n",
    "    else:\n",
    "        grids = param_grids\n",
    "\n",
    "    # Modèles de base\n",
    "    base_models = {\n",
    "        'DummyClassifier': DummyClassifier(random_state=random_state),\n",
    "        'RandomForestClassifier': RandomForestClassifier(random_state=random_state),\n",
    "        'XGBClassifier': XGBClassifier(random_state=random_state, eval_metric='logloss'),\n",
    "        'CatBoostClassifier': CatBoostClassifier(random_state=random_state, verbose=0)\n",
    "    }\n",
    "\n",
    "    grid_results = []\n",
    "    eval_results = []\n",
    "    grid_objects = {}\n",
    "    detailed_reports = []\n",
    "\n",
    "    # GridSearch pour chaque modèle\n",
    "    for model_name, base_model in base_models.items():\n",
    "        param_grid = grids[model_name]\n",
    "\n",
    "        # GridSearch\n",
    "        grid_search, grid_res = perform_grid_search(\n",
    "            base_model, param_grid, X_train, y_train,\n",
    "            model_name, cv=cv, scoring=scoring\n",
    "        )\n",
    "        grid_results.append(grid_res)\n",
    "        grid_objects[model_name] = grid_search\n",
    "\n",
    "        # Évaluation du meilleur modèle\n",
    "        eval_res, report, cm = evaluate_best_model(\n",
    "            grid_search, X_train, X_test, y_train, y_test, model_name\n",
    "        )\n",
    "        eval_results.append(eval_res)\n",
    "\n",
    "        detailed_reports.append({\n",
    "            'model': model_name,\n",
    "            'report': report,\n",
    "            'confusion_matrix': cm\n",
    "        })\n",
    "\n",
    "    # Conversion en DataFrames\n",
    "    grid_df = pd.DataFrame(grid_results)\n",
    "    eval_df = pd.DataFrame(eval_results)\n",
    "\n",
    "    return grid_df, eval_df, grid_objects, detailed_reports\n",
    "\n",
    "\n",
    "def save_gridsearch_results(grid_df, eval_df, grid_objects, detailed_reports,\n",
    "                            output_prefix='gridsearch_results'):\n",
    "    \"\"\"\n",
    "    Sauvegarde tous les résultats du GridSearch.\n",
    "\n",
    "    Args:\n",
    "        grid_df: DataFrame avec résultats du GridSearch\n",
    "        eval_df: DataFrame avec évaluation finale\n",
    "        grid_objects: Dict des objets GridSearchCV\n",
    "        detailed_reports: Rapports détaillés\n",
    "        output_prefix: Préfixe des fichiers\n",
    "    \"\"\"\n",
    "    # 1. Résumé du GridSearch\n",
    "    grid_df.to_csv(f'{output_prefix}_grid_summary.csv', index=False)\n",
    "    print(f\"\\n✓ Résumé GridSearch: {output_prefix}_grid_summary.csv\")\n",
    "\n",
    "    # 2. Évaluation finale\n",
    "    eval_df.to_csv(f'{output_prefix}_evaluation.csv', index=False)\n",
    "    print(f\"✓ Évaluation finale: {output_prefix}_evaluation.csv\")\n",
    "\n",
    "    # 3. Détails complets du GridSearch (tous les résultats)\n",
    "    all_cv_results = []\n",
    "    for model_name, grid_search in grid_objects.items():\n",
    "        cv_res = pd.DataFrame(grid_search.cv_results_)\n",
    "        cv_res.insert(0, 'model', model_name)\n",
    "        all_cv_results.append(cv_res)\n",
    "\n",
    "    full_cv_df = pd.concat(all_cv_results, ignore_index=True)\n",
    "    full_cv_df.to_csv(f'{output_prefix}_full_cv_results.csv', index=False)\n",
    "    print(f\"✓ Résultats CV complets: {output_prefix}_full_cv_results.csv\")\n",
    "\n",
    "    # 4. Meilleurs paramètres\n",
    "    best_params_data = []\n",
    "    for model_name, grid_search in grid_objects.items():\n",
    "        for param, value in grid_search.best_params_.items():\n",
    "            best_params_data.append({\n",
    "                'model': model_name,\n",
    "                'parameter': param,\n",
    "                'value': str(value)\n",
    "            })\n",
    "\n",
    "    best_params_df = pd.DataFrame(best_params_data)\n",
    "    best_params_df.to_csv(f'{output_prefix}_best_params.csv', index=False)\n",
    "    print(f\"✓ Meilleurs paramètres: {output_prefix}_best_params.csv\")\n",
    "\n",
    "    # 5. Matrices de confusion\n",
    "    cm_data = []\n",
    "    for item in detailed_reports:\n",
    "        cm_data.append({\n",
    "            'model': item['model'],\n",
    "            'confusion_matrix': str(item['confusion_matrix'].tolist())\n",
    "        })\n",
    "    cm_df = pd.DataFrame(cm_data)\n",
    "    cm_df.to_csv(f'{output_prefix}_confusion_matrices.csv', index=False)\n",
    "    print(f\"✓ Matrices de confusion: {output_prefix}_confusion_matrices.csv\")\n",
    "\n",
    "    # 6. Analyse d'overfitting\n",
    "    overfitting_cols = ['model', 'train_accuracy', 'test_accuracy', 'accuracy_gap',\n",
    "                       'train_f1_macro', 'test_f1_macro', 'f1_gap', 'overfitting']\n",
    "    overfitting_df = eval_df[overfitting_cols]\n",
    "    overfitting_df.to_csv(f'{output_prefix}_overfitting.csv', index=False)\n",
    "    print(f\"✓ Analyse overfitting: {output_prefix}_overfitting.csv\")\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TOUS LES RÉSULTATS ONT ÉTÉ SAUVEGARDÉS\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Affichage du classement\n",
    "    print(f\"\\n🏆 CLASSEMENT DES MODÈLES (par {grid_objects[list(grid_objects.keys())[0]].scoring}):\")\n",
    "    print(\"-\" * 70)\n",
    "    ranking = grid_df.sort_values('best_score', ascending=False)\n",
    "    for i, row in ranking.iterrows():\n",
    "        print(f\"{i+1}. {row['model']:25s} - Score: {row['best_score']:.4f} - Temps: {row['time_seconds']:.1f}s\")\n",
    "\n",
    "    print(\"\\n📊 DÉTECTION D'OVERFITTING:\")\n",
    "    print(\"-\" * 70)\n",
    "    for _, row in eval_df.iterrows():\n",
    "        status = \"⚠️  OVERFITTING\" if row['overfitting'] == 'OUI' else \"✓  Pas d'overfitting\"\n",
    "        print(f\"{row['model']:25s}: {status} (écart: {row['accuracy_gap']:+.4f})\")\n",
    "\n",
    "\n",
    "# GridSearch avec grille réduite (rapide pour test)\n",
    "# Utilisez param_grids='full' pour une recherche complète\n",
    "grid_df, eval_df, grid_objects, detailed_reports = compare_models_gridsearch(\n",
    "    X, y,\n",
    "    param_grids='small',  # 'small', 'full', ou dict personnalisé\n",
    "    test_size=0.2,\n",
    "    cv=3,  # 3 pour test rapide, 5 recommandé\n",
    "    scoring='f1_macro',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Sauvegarde\n",
    "save_gridsearch_results(\n",
    "    grid_df, eval_df, grid_objects, detailed_reports,\n",
    "    output_prefix='gridsearch_results'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"APERÇU DES MEILLEURS RÉSULTATS\")\n",
    "print(\"=\"*70)\n",
    "print(grid_df.round(4).to_string(index=False))"
   ],
   "id": "5b89ecdc8742d28a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GRIDSEARCH - COMPARAISON DES MODÈLES\n",
      "======================================================================\n",
      "Taille du dataset: 1470 échantillons\n",
      "Train: 1176 | Test: 294\n",
      "Distribution des classes: {False: np.int64(1233), True: np.int64(237)}\n",
      "\n",
      "======================================================================\n",
      "🔍 Grid Search pour DummyClassifier\n",
      "======================================================================\n",
      "Nombre de combinaisons à tester: 2\n",
      "Métrique d'optimisation: f1_macro\n",
      "Validation croisée: 3 folds\n",
      "✓ Terminé en 1.07 secondes\n",
      "📊 Meilleur score (f1_macro): 0.4853\n",
      "🏆 Meilleurs paramètres:\n",
      "   - strategy: stratified\n",
      "\n",
      "📈 Évaluation sur le jeu de test:\n",
      "   Train accuracy: 0.7168\n",
      "   Test accuracy:  0.7143\n",
      "   Écart accuracy: +0.0026\n",
      "   Overfitting:    NON\n",
      "\n",
      "======================================================================\n",
      "🔍 Grid Search pour RandomForestClassifier\n",
      "======================================================================\n",
      "Nombre de combinaisons à tester: 12\n",
      "Métrique d'optimisation: f1_macro\n",
      "Validation croisée: 3 folds\n",
      "✓ Terminé en 1.16 secondes\n",
      "📊 Meilleur score (f1_macro): 0.6145\n",
      "🏆 Meilleurs paramètres:\n",
      "   - max_depth: 10\n",
      "   - max_features: sqrt\n",
      "   - min_samples_split: 2\n",
      "   - n_estimators: 100\n",
      "\n",
      "📈 Évaluation sur le jeu de test:\n",
      "   Train accuracy: 0.9379\n",
      "   Test accuracy:  0.8367\n",
      "   Écart accuracy: +0.1012\n",
      "   Overfitting:    OUI\n",
      "\n",
      "======================================================================\n",
      "🔍 Grid Search pour XGBClassifier\n",
      "======================================================================\n",
      "Nombre de combinaisons à tester: 32\n",
      "Métrique d'optimisation: f1_macro\n",
      "Validation croisée: 3 folds\n",
      "✓ Terminé en 0.42 secondes\n",
      "📊 Meilleur score (f1_macro): 0.6608\n",
      "🏆 Meilleurs paramètres:\n",
      "   - learning_rate: 0.1\n",
      "   - max_depth: 3\n",
      "   - n_estimators: 100\n",
      "   - reg_lambda: 2\n",
      "   - subsample: 0.8\n",
      "\n",
      "📈 Évaluation sur le jeu de test:\n",
      "   Train accuracy: 0.9014\n",
      "   Test accuracy:  0.8333\n",
      "   Écart accuracy: +0.0680\n",
      "   Overfitting:    OUI\n",
      "\n",
      "======================================================================\n",
      "🔍 Grid Search pour CatBoostClassifier\n",
      "======================================================================\n",
      "Nombre de combinaisons à tester: 16\n",
      "Métrique d'optimisation: f1_macro\n",
      "Validation croisée: 3 folds\n",
      "✓ Terminé en 0.79 secondes\n",
      "📊 Meilleur score (f1_macro): 0.6479\n",
      "🏆 Meilleurs paramètres:\n",
      "   - depth: 6\n",
      "   - iterations: 100\n",
      "   - l2_leaf_reg: 1\n",
      "   - learning_rate: 0.1\n",
      "\n",
      "📈 Évaluation sur le jeu de test:\n",
      "   Train accuracy: 0.9566\n",
      "   Test accuracy:  0.8435\n",
      "   Écart accuracy: +0.1131\n",
      "   Overfitting:    OUI\n",
      "\n",
      "✓ Résumé GridSearch: gridsearch_results_grid_summary.csv\n",
      "✓ Évaluation finale: gridsearch_results_evaluation.csv\n",
      "✓ Résultats CV complets: gridsearch_results_full_cv_results.csv\n",
      "✓ Meilleurs paramètres: gridsearch_results_best_params.csv\n",
      "✓ Matrices de confusion: gridsearch_results_confusion_matrices.csv\n",
      "✓ Analyse overfitting: gridsearch_results_overfitting.csv\n",
      "\n",
      "======================================================================\n",
      "TOUS LES RÉSULTATS ONT ÉTÉ SAUVEGARDÉS\n",
      "======================================================================\n",
      "\n",
      "🏆 CLASSEMENT DES MODÈLES (par f1_macro):\n",
      "----------------------------------------------------------------------\n",
      "3. XGBClassifier             - Score: 0.6608 - Temps: 0.4s\n",
      "4. CatBoostClassifier        - Score: 0.6479 - Temps: 0.8s\n",
      "2. RandomForestClassifier    - Score: 0.6145 - Temps: 1.2s\n",
      "1. DummyClassifier           - Score: 0.4853 - Temps: 1.1s\n",
      "\n",
      "📊 DÉTECTION D'OVERFITTING:\n",
      "----------------------------------------------------------------------\n",
      "DummyClassifier          : ✓  Pas d'overfitting (écart: +0.0026)\n",
      "RandomForestClassifier   : ⚠️  OVERFITTING (écart: +0.1012)\n",
      "XGBClassifier            : ⚠️  OVERFITTING (écart: +0.0680)\n",
      "CatBoostClassifier       : ⚠️  OVERFITTING (écart: +0.1131)\n",
      "\n",
      "======================================================================\n",
      "APERÇU DES MEILLEURS RÉSULTATS\n",
      "======================================================================\n",
      "                 model  best_score                                                                                    best_params  n_combinations  time_seconds\n",
      "       DummyClassifier      0.4853                                                                     {'strategy': 'stratified'}               2        1.0683\n",
      "RandomForestClassifier      0.6145         {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100}              12        1.1649\n",
      "         XGBClassifier      0.6608 {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_lambda': 2, 'subsample': 0.8}              32        0.4232\n",
      "    CatBoostClassifier      0.6479                        {'depth': 6, 'iterations': 100, 'l2_leaf_reg': 1, 'learning_rate': 0.1}              16        0.7909\n"
     ]
    }
   ],
   "execution_count": 107
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
